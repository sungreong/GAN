{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_Keras_gpu(o)_mnist.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sungreong/GAN/blob/master/WGAN_Keras_gpu(o)_mnist.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "lttMwI5tfKu8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function ,division"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "djczM-ikfPAw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape , Flatten , Dropout\n",
        "from keras.layers import BatchNormalization, Activation ,ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import keras.backend.tensorflow_backend as K\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJisIBFyoXM-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "3384ea5c-fc8a-4ef6-f59e-1e4526706621"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3513284834210694460\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11287966516\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 17363116924762274192\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WPum15xRfzF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        \n",
        "        self.clip_value = 0.01\n",
        "        \n",
        "        optimizer = RMSprop(lr=0.00005)  ## 논문에서 추천?했던 걸로 알고 있음 \n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=self.wasserstein_loss, \n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the generator\n",
        "        self.generator = self.build_generator()\n",
        "        self.generator.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(100,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator) takes\n",
        "        # noise as input => generates images => determines validity \n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss=self.wasserstein_loss, \n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        noise_shape = (100,)\n",
        "        with K.tf.device('/gpu:0'):\n",
        "            model = Sequential()\n",
        "\n",
        "            model.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=noise_shape))\n",
        "            model.add(Reshape((7, 7, 128)))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "            model.add(UpSampling2D())\n",
        "            model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "            model.add(Activation(\"relu\"))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "            model.add(UpSampling2D())\n",
        "            model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "            model.add(Activation(\"relu\"))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "            model.add(Conv2D(1, kernel_size=4, padding=\"same\"))\n",
        "            model.add(Activation(\"tanh\"))\n",
        "\n",
        "            model.summary()\n",
        "\n",
        "            noise = Input(shape=noise_shape)\n",
        "            img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        with K.tf.device('/gpu:0'):\n",
        "            model = Sequential()\n",
        "\n",
        "            model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(Dropout(0.25))\n",
        "            model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "            model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(Dropout(0.25))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "            model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(Dropout(0.25))\n",
        "            model.add(BatchNormalization(momentum=0.8))\n",
        "            model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(Dropout(0.25))\n",
        "\n",
        "            model.add(Flatten())\n",
        "\n",
        "            model.summary()\n",
        "\n",
        "            img = Input(shape=img_shape)\n",
        "            features = model(img)\n",
        "            valid = Dense(1, activation=\"linear\")(features)\n",
        "\n",
        "        return Model(img, valid)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        half_batch = int(batch_size / 2)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                #  Train Discriminator\n",
        "\n",
        "\n",
        "                # Select a random half batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "                imgs = X_train[idx]\n",
        "\n",
        "                noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "\n",
        "                # Generate a half batch of new images\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the discriminator\n",
        "                with K.tf.device('/gpu:0'):\n",
        "                    d_loss_real = self.discriminator.train_on_batch(imgs, -np.ones((half_batch, 1)))\n",
        "                    d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.ones((half_batch, 1)))\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "                # Clip discriminator weights\n",
        "                for l in self.discriminator.layers:\n",
        "                    weights = l.get_weights()\n",
        "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                    l.set_weights(weights)\n",
        "\n",
        "\n",
        "   \n",
        "            #  Train Generator\n",
        "  \n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "            # Train the generator\n",
        "            with K.tf.device('/gpu:0'):\n",
        "                for _ in range(3) : # 하드하게 generator 훈련시키기 \n",
        "                    g_loss = self.combined.train_on_batch(noise, -np.ones((batch_size, 1)))\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, 100))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 1\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        if not os.path.exists(\"./images\"):\n",
        "            os.makedirs(\"./images\")\n",
        "        fig.savefig(\"./images/mnist_%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5euAmgcjSXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72478
        },
        "outputId": "ced1e7e6-28d3-45e9-c7b1-a4030d11002e"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if __name__ == '__main__':\n",
        "    wgan = WGAN()\n",
        "    wgan.train(epochs=4000, batch_size=32, save_interval=50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 14, 14, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 7, 7, 32)          4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPaddin (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 4, 4, 64)          18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 4, 4, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "=================================================================\n",
            "Total params: 97,536\n",
            "Trainable params: 97,344\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 14, 14, 128)       262272    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 28, 28, 64)        131136    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 28, 28, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,029,185\n",
            "Trainable params: 1,028,545\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.999919] [G loss: 1.000163]\n",
            "1 [D loss: 0.999918] [G loss: 1.000166]\n",
            "2 [D loss: 0.999925] [G loss: 1.000152]\n",
            "3 [D loss: 0.999934] [G loss: 1.000140]\n",
            "4 [D loss: 0.999932] [G loss: 1.000132]\n",
            "5 [D loss: 0.999934] [G loss: 1.000129]\n",
            "6 [D loss: 0.999937] [G loss: 1.000134]\n",
            "7 [D loss: 0.999924] [G loss: 1.000123]\n",
            "8 [D loss: 0.999923] [G loss: 1.000131]\n",
            "9 [D loss: 0.999933] [G loss: 1.000124]\n",
            "10 [D loss: 0.999929] [G loss: 1.000133]\n",
            "11 [D loss: 0.999927] [G loss: 1.000121]\n",
            "12 [D loss: 0.999929] [G loss: 1.000126]\n",
            "13 [D loss: 0.999937] [G loss: 1.000133]\n",
            "14 [D loss: 0.999930] [G loss: 1.000118]\n",
            "15 [D loss: 0.999935] [G loss: 1.000129]\n",
            "16 [D loss: 0.999933] [G loss: 1.000116]\n",
            "17 [D loss: 0.999930] [G loss: 1.000128]\n",
            "18 [D loss: 0.999933] [G loss: 1.000112]\n",
            "19 [D loss: 0.999933] [G loss: 1.000117]\n",
            "20 [D loss: 0.999938] [G loss: 1.000129]\n",
            "21 [D loss: 0.999935] [G loss: 1.000115]\n",
            "22 [D loss: 0.999948] [G loss: 1.000112]\n",
            "23 [D loss: 0.999949] [G loss: 1.000102]\n",
            "24 [D loss: 0.999965] [G loss: 1.000113]\n",
            "25 [D loss: 0.999947] [G loss: 1.000106]\n",
            "26 [D loss: 0.999950] [G loss: 1.000116]\n",
            "27 [D loss: 0.999955] [G loss: 1.000106]\n",
            "28 [D loss: 0.999967] [G loss: 1.000088]\n",
            "29 [D loss: 0.999962] [G loss: 1.000085]\n",
            "30 [D loss: 0.999963] [G loss: 1.000090]\n",
            "31 [D loss: 0.999965] [G loss: 1.000091]\n",
            "32 [D loss: 0.999962] [G loss: 1.000087]\n",
            "33 [D loss: 0.999980] [G loss: 1.000096]\n",
            "34 [D loss: 0.999989] [G loss: 1.000099]\n",
            "35 [D loss: 0.999986] [G loss: 1.000098]\n",
            "36 [D loss: 0.999989] [G loss: 1.000088]\n",
            "37 [D loss: 0.999999] [G loss: 1.000100]\n",
            "38 [D loss: 0.999991] [G loss: 1.000108]\n",
            "39 [D loss: 0.999991] [G loss: 1.000107]\n",
            "40 [D loss: 1.000031] [G loss: 1.000116]\n",
            "41 [D loss: 0.999993] [G loss: 1.000156]\n",
            "42 [D loss: 1.000026] [G loss: 1.000168]\n",
            "43 [D loss: 1.000015] [G loss: 1.000143]\n",
            "44 [D loss: 1.000023] [G loss: 1.000163]\n",
            "45 [D loss: 1.000043] [G loss: 1.000165]\n",
            "46 [D loss: 1.000037] [G loss: 1.000154]\n",
            "47 [D loss: 1.000054] [G loss: 1.000190]\n",
            "48 [D loss: 1.000102] [G loss: 1.000238]\n",
            "49 [D loss: 1.000081] [G loss: 1.000239]\n",
            "50 [D loss: 1.000074] [G loss: 1.000228]\n",
            "51 [D loss: 1.000101] [G loss: 1.000245]\n",
            "52 [D loss: 1.000092] [G loss: 1.000266]\n",
            "53 [D loss: 1.000068] [G loss: 1.000264]\n",
            "54 [D loss: 1.000139] [G loss: 1.000311]\n",
            "55 [D loss: 1.000115] [G loss: 1.000290]\n",
            "56 [D loss: 1.000155] [G loss: 1.000317]\n",
            "57 [D loss: 1.000105] [G loss: 1.000349]\n",
            "58 [D loss: 1.000123] [G loss: 1.000353]\n",
            "59 [D loss: 1.000118] [G loss: 1.000296]\n",
            "60 [D loss: 1.000211] [G loss: 1.000351]\n",
            "61 [D loss: 1.000278] [G loss: 1.000324]\n",
            "62 [D loss: 1.000170] [G loss: 1.000352]\n",
            "63 [D loss: 1.000337] [G loss: 1.000366]\n",
            "64 [D loss: 1.000241] [G loss: 1.000414]\n",
            "65 [D loss: 1.000253] [G loss: 1.000418]\n",
            "66 [D loss: 1.000313] [G loss: 1.000428]\n",
            "67 [D loss: 1.000252] [G loss: 1.000444]\n",
            "68 [D loss: 1.000330] [G loss: 1.000419]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "69 [D loss: 1.000137] [G loss: 1.000499]\n",
            "70 [D loss: 1.000376] [G loss: 1.000525]\n",
            "71 [D loss: 1.000309] [G loss: 1.000474]\n",
            "72 [D loss: 1.000403] [G loss: 1.000541]\n",
            "73 [D loss: 1.000296] [G loss: 1.000508]\n",
            "74 [D loss: 1.000480] [G loss: 1.000691]\n",
            "75 [D loss: 1.000330] [G loss: 1.000579]\n",
            "76 [D loss: 1.000373] [G loss: 1.000656]\n",
            "77 [D loss: 1.000410] [G loss: 1.000526]\n",
            "78 [D loss: 1.000166] [G loss: 1.000670]\n",
            "79 [D loss: 1.000340] [G loss: 1.000801]\n",
            "80 [D loss: 1.000365] [G loss: 1.000734]\n",
            "81 [D loss: 1.000365] [G loss: 1.000845]\n",
            "82 [D loss: 1.000621] [G loss: 1.000794]\n",
            "83 [D loss: 1.000436] [G loss: 1.000768]\n",
            "84 [D loss: 1.000295] [G loss: 1.000744]\n",
            "85 [D loss: 1.000503] [G loss: 1.000784]\n",
            "86 [D loss: 1.000389] [G loss: 1.000898]\n",
            "87 [D loss: 1.000365] [G loss: 1.000784]\n",
            "88 [D loss: 1.000431] [G loss: 1.000820]\n",
            "89 [D loss: 1.000557] [G loss: 1.000946]\n",
            "90 [D loss: 1.000131] [G loss: 1.000837]\n",
            "91 [D loss: 1.000415] [G loss: 1.000903]\n",
            "92 [D loss: 1.000346] [G loss: 1.001012]\n",
            "93 [D loss: 1.000378] [G loss: 1.000974]\n",
            "94 [D loss: 1.000279] [G loss: 1.000990]\n",
            "95 [D loss: 1.000445] [G loss: 1.001097]\n",
            "96 [D loss: 1.000551] [G loss: 1.001140]\n",
            "97 [D loss: 1.000272] [G loss: 1.001018]\n",
            "98 [D loss: 1.000428] [G loss: 1.001092]\n",
            "99 [D loss: 1.000288] [G loss: 1.001167]\n",
            "100 [D loss: 1.000437] [G loss: 1.001160]\n",
            "101 [D loss: 1.000560] [G loss: 1.001412]\n",
            "102 [D loss: 1.000568] [G loss: 1.001341]\n",
            "103 [D loss: 1.000468] [G loss: 1.001240]\n",
            "104 [D loss: 1.000605] [G loss: 1.001309]\n",
            "105 [D loss: 1.000645] [G loss: 1.001481]\n",
            "106 [D loss: 1.000737] [G loss: 1.001506]\n",
            "107 [D loss: 1.000602] [G loss: 1.001435]\n",
            "108 [D loss: 1.000559] [G loss: 1.001557]\n",
            "109 [D loss: 1.000386] [G loss: 1.001575]\n",
            "110 [D loss: 1.000529] [G loss: 1.001442]\n",
            "111 [D loss: 1.000406] [G loss: 1.001489]\n",
            "112 [D loss: 1.000420] [G loss: 1.001724]\n",
            "113 [D loss: 1.000472] [G loss: 1.001545]\n",
            "114 [D loss: 1.000532] [G loss: 1.001740]\n",
            "115 [D loss: 1.000342] [G loss: 1.001576]\n",
            "116 [D loss: 1.000536] [G loss: 1.001623]\n",
            "117 [D loss: 1.000519] [G loss: 1.001720]\n",
            "118 [D loss: 1.000707] [G loss: 1.001794]\n",
            "119 [D loss: 1.000474] [G loss: 1.001918]\n",
            "120 [D loss: 1.000581] [G loss: 1.001785]\n",
            "121 [D loss: 1.000387] [G loss: 1.001801]\n",
            "122 [D loss: 1.000539] [G loss: 1.002043]\n",
            "123 [D loss: 1.000322] [G loss: 1.001973]\n",
            "124 [D loss: 1.000421] [G loss: 1.002079]\n",
            "125 [D loss: 1.000500] [G loss: 1.001935]\n",
            "126 [D loss: 1.000588] [G loss: 1.001884]\n",
            "127 [D loss: 1.000335] [G loss: 1.002080]\n",
            "128 [D loss: 1.000387] [G loss: 1.002087]\n",
            "129 [D loss: 1.000375] [G loss: 1.002138]\n",
            "130 [D loss: 1.000322] [G loss: 1.002183]\n",
            "131 [D loss: 1.000408] [G loss: 1.002173]\n",
            "132 [D loss: 1.000424] [G loss: 1.002025]\n",
            "133 [D loss: 1.000277] [G loss: 1.002250]\n",
            "134 [D loss: 1.000211] [G loss: 1.002087]\n",
            "135 [D loss: 1.000394] [G loss: 1.002390]\n",
            "136 [D loss: 1.000220] [G loss: 1.002145]\n",
            "137 [D loss: 1.000176] [G loss: 1.002216]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "138 [D loss: 1.000443] [G loss: 1.002291]\n",
            "139 [D loss: 1.000370] [G loss: 1.002359]\n",
            "140 [D loss: 1.000188] [G loss: 1.002394]\n",
            "141 [D loss: 0.999926] [G loss: 1.002515]\n",
            "142 [D loss: 1.000235] [G loss: 1.002327]\n",
            "143 [D loss: 1.000364] [G loss: 1.002456]\n",
            "144 [D loss: 1.000222] [G loss: 1.002446]\n",
            "145 [D loss: 1.000345] [G loss: 1.002474]\n",
            "146 [D loss: 1.000162] [G loss: 1.002491]\n",
            "147 [D loss: 1.000364] [G loss: 1.002525]\n",
            "148 [D loss: 1.000181] [G loss: 1.002511]\n",
            "149 [D loss: 1.000097] [G loss: 1.002227]\n",
            "150 [D loss: 1.000280] [G loss: 1.002484]\n",
            "151 [D loss: 1.000316] [G loss: 1.002492]\n",
            "152 [D loss: 1.000169] [G loss: 1.002519]\n",
            "153 [D loss: 1.000199] [G loss: 1.002602]\n",
            "154 [D loss: 1.000081] [G loss: 1.002575]\n",
            "155 [D loss: 1.000061] [G loss: 1.002499]\n",
            "156 [D loss: 1.000357] [G loss: 1.002452]\n",
            "157 [D loss: 1.000193] [G loss: 1.002476]\n",
            "158 [D loss: 1.000401] [G loss: 1.002587]\n",
            "159 [D loss: 1.000198] [G loss: 1.002461]\n",
            "160 [D loss: 1.000249] [G loss: 1.002556]\n",
            "161 [D loss: 1.000218] [G loss: 1.002615]\n",
            "162 [D loss: 1.000165] [G loss: 1.002572]\n",
            "163 [D loss: 1.000286] [G loss: 1.002577]\n",
            "164 [D loss: 1.000200] [G loss: 1.002545]\n",
            "165 [D loss: 1.000179] [G loss: 1.002731]\n",
            "166 [D loss: 1.000300] [G loss: 1.002611]\n",
            "167 [D loss: 1.000275] [G loss: 1.002706]\n",
            "168 [D loss: 1.000276] [G loss: 1.002571]\n",
            "169 [D loss: 1.000047] [G loss: 1.002591]\n",
            "170 [D loss: 1.000167] [G loss: 1.002524]\n",
            "171 [D loss: 1.000105] [G loss: 1.002562]\n",
            "172 [D loss: 1.000059] [G loss: 1.002555]\n",
            "173 [D loss: 1.000211] [G loss: 1.002681]\n",
            "174 [D loss: 1.000103] [G loss: 1.002677]\n",
            "175 [D loss: 1.000246] [G loss: 1.002591]\n",
            "176 [D loss: 1.000076] [G loss: 1.002481]\n",
            "177 [D loss: 0.999969] [G loss: 1.002732]\n",
            "178 [D loss: 1.000232] [G loss: 1.002730]\n",
            "179 [D loss: 0.999886] [G loss: 1.002716]\n",
            "180 [D loss: 1.000095] [G loss: 1.002504]\n",
            "181 [D loss: 1.000029] [G loss: 1.002576]\n",
            "182 [D loss: 1.000307] [G loss: 1.002583]\n",
            "183 [D loss: 1.000262] [G loss: 1.002585]\n",
            "184 [D loss: 1.000015] [G loss: 1.002621]\n",
            "185 [D loss: 1.000035] [G loss: 1.002539]\n",
            "186 [D loss: 0.999973] [G loss: 1.002619]\n",
            "187 [D loss: 1.000139] [G loss: 1.002557]\n",
            "188 [D loss: 1.000131] [G loss: 1.002639]\n",
            "189 [D loss: 1.000161] [G loss: 1.002657]\n",
            "190 [D loss: 1.000036] [G loss: 1.002533]\n",
            "191 [D loss: 1.000010] [G loss: 1.002499]\n",
            "192 [D loss: 1.000077] [G loss: 1.002571]\n",
            "193 [D loss: 1.000051] [G loss: 1.002522]\n",
            "194 [D loss: 0.999964] [G loss: 1.002600]\n",
            "195 [D loss: 0.999968] [G loss: 1.002614]\n",
            "196 [D loss: 1.000146] [G loss: 1.002734]\n",
            "197 [D loss: 1.000040] [G loss: 1.002630]\n",
            "198 [D loss: 1.000004] [G loss: 1.002470]\n",
            "199 [D loss: 0.999976] [G loss: 1.002671]\n",
            "200 [D loss: 1.000073] [G loss: 1.002485]\n",
            "201 [D loss: 0.999945] [G loss: 1.002442]\n",
            "202 [D loss: 1.000111] [G loss: 1.002442]\n",
            "203 [D loss: 1.000175] [G loss: 1.002474]\n",
            "204 [D loss: 0.999732] [G loss: 1.002405]\n",
            "205 [D loss: 1.000000] [G loss: 1.002565]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "206 [D loss: 0.999859] [G loss: 1.002480]\n",
            "207 [D loss: 1.000012] [G loss: 1.002368]\n",
            "208 [D loss: 1.000023] [G loss: 1.002605]\n",
            "209 [D loss: 0.999981] [G loss: 1.002346]\n",
            "210 [D loss: 1.000079] [G loss: 1.002416]\n",
            "211 [D loss: 0.999971] [G loss: 1.002290]\n",
            "212 [D loss: 0.999937] [G loss: 1.002361]\n",
            "213 [D loss: 0.999876] [G loss: 1.002295]\n",
            "214 [D loss: 0.999926] [G loss: 1.002271]\n",
            "215 [D loss: 0.999991] [G loss: 1.002337]\n",
            "216 [D loss: 0.999845] [G loss: 1.002231]\n",
            "217 [D loss: 0.999804] [G loss: 1.002287]\n",
            "218 [D loss: 0.999848] [G loss: 1.002187]\n",
            "219 [D loss: 0.999804] [G loss: 1.002187]\n",
            "220 [D loss: 1.000018] [G loss: 1.002188]\n",
            "221 [D loss: 0.999925] [G loss: 1.002270]\n",
            "222 [D loss: 0.999835] [G loss: 1.002104]\n",
            "223 [D loss: 0.999925] [G loss: 1.002018]\n",
            "224 [D loss: 1.000010] [G loss: 1.002050]\n",
            "225 [D loss: 0.999930] [G loss: 1.002139]\n",
            "226 [D loss: 0.999964] [G loss: 1.002196]\n",
            "227 [D loss: 0.999941] [G loss: 1.002064]\n",
            "228 [D loss: 0.999805] [G loss: 1.002019]\n",
            "229 [D loss: 0.999873] [G loss: 1.001954]\n",
            "230 [D loss: 0.999904] [G loss: 1.002052]\n",
            "231 [D loss: 0.999892] [G loss: 1.001948]\n",
            "232 [D loss: 0.999896] [G loss: 1.001939]\n",
            "233 [D loss: 0.999965] [G loss: 1.001876]\n",
            "234 [D loss: 0.999881] [G loss: 1.001774]\n",
            "235 [D loss: 0.999915] [G loss: 1.001828]\n",
            "236 [D loss: 0.999869] [G loss: 1.001757]\n",
            "237 [D loss: 0.999884] [G loss: 1.001726]\n",
            "238 [D loss: 0.999816] [G loss: 1.001688]\n",
            "239 [D loss: 0.999945] [G loss: 1.001780]\n",
            "240 [D loss: 0.999900] [G loss: 1.001686]\n",
            "241 [D loss: 0.999949] [G loss: 1.001657]\n",
            "242 [D loss: 0.999858] [G loss: 1.001640]\n",
            "243 [D loss: 0.999966] [G loss: 1.001640]\n",
            "244 [D loss: 0.999984] [G loss: 1.001505]\n",
            "245 [D loss: 0.999852] [G loss: 1.001567]\n",
            "246 [D loss: 0.999845] [G loss: 1.001640]\n",
            "247 [D loss: 0.999904] [G loss: 1.001517]\n",
            "248 [D loss: 1.000003] [G loss: 1.001500]\n",
            "249 [D loss: 0.999981] [G loss: 1.001474]\n",
            "250 [D loss: 0.999966] [G loss: 1.001502]\n",
            "251 [D loss: 0.999868] [G loss: 1.001393]\n",
            "252 [D loss: 0.999980] [G loss: 1.001405]\n",
            "253 [D loss: 0.999903] [G loss: 1.001296]\n",
            "254 [D loss: 0.999875] [G loss: 1.001348]\n",
            "255 [D loss: 0.999966] [G loss: 1.001375]\n",
            "256 [D loss: 0.999949] [G loss: 1.001283]\n",
            "257 [D loss: 0.999977] [G loss: 1.001276]\n",
            "258 [D loss: 0.999919] [G loss: 1.001190]\n",
            "259 [D loss: 0.999953] [G loss: 1.001189]\n",
            "260 [D loss: 0.999945] [G loss: 1.001249]\n",
            "261 [D loss: 0.999885] [G loss: 1.001186]\n",
            "262 [D loss: 0.999881] [G loss: 1.001145]\n",
            "263 [D loss: 0.999937] [G loss: 1.001165]\n",
            "264 [D loss: 0.999889] [G loss: 1.001084]\n",
            "265 [D loss: 0.999885] [G loss: 1.001137]\n",
            "266 [D loss: 0.999913] [G loss: 1.001019]\n",
            "267 [D loss: 0.999913] [G loss: 1.001011]\n",
            "268 [D loss: 0.999994] [G loss: 1.000982]\n",
            "269 [D loss: 0.999919] [G loss: 1.000999]\n",
            "270 [D loss: 0.999932] [G loss: 1.000949]\n",
            "271 [D loss: 0.999978] [G loss: 1.000911]\n",
            "272 [D loss: 0.999944] [G loss: 1.000934]\n",
            "273 [D loss: 0.999915] [G loss: 1.000924]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "274 [D loss: 0.999948] [G loss: 1.000896]\n",
            "275 [D loss: 0.999934] [G loss: 1.000847]\n",
            "276 [D loss: 0.999895] [G loss: 1.000861]\n",
            "277 [D loss: 0.999967] [G loss: 1.000751]\n",
            "278 [D loss: 1.000005] [G loss: 1.000813]\n",
            "279 [D loss: 0.999933] [G loss: 1.000799]\n",
            "280 [D loss: 0.999939] [G loss: 1.000742]\n",
            "281 [D loss: 0.999879] [G loss: 1.000783]\n",
            "282 [D loss: 0.999918] [G loss: 1.000715]\n",
            "283 [D loss: 0.999962] [G loss: 1.000673]\n",
            "284 [D loss: 0.999974] [G loss: 1.000708]\n",
            "285 [D loss: 0.999942] [G loss: 1.000638]\n",
            "286 [D loss: 0.999985] [G loss: 1.000692]\n",
            "287 [D loss: 0.999929] [G loss: 1.000664]\n",
            "288 [D loss: 0.999960] [G loss: 1.000595]\n",
            "289 [D loss: 0.999927] [G loss: 1.000694]\n",
            "290 [D loss: 0.999981] [G loss: 1.000668]\n",
            "291 [D loss: 0.999887] [G loss: 1.000601]\n",
            "292 [D loss: 0.999948] [G loss: 1.000593]\n",
            "293 [D loss: 0.999957] [G loss: 1.000644]\n",
            "294 [D loss: 0.999934] [G loss: 1.000588]\n",
            "295 [D loss: 0.999943] [G loss: 1.000525]\n",
            "296 [D loss: 0.999923] [G loss: 1.000563]\n",
            "297 [D loss: 0.999967] [G loss: 1.000545]\n",
            "298 [D loss: 0.999961] [G loss: 1.000512]\n",
            "299 [D loss: 0.999940] [G loss: 1.000475]\n",
            "300 [D loss: 1.000011] [G loss: 1.000442]\n",
            "301 [D loss: 0.999898] [G loss: 1.000474]\n",
            "302 [D loss: 0.999948] [G loss: 1.000478]\n",
            "303 [D loss: 0.999893] [G loss: 1.000484]\n",
            "304 [D loss: 0.999969] [G loss: 1.000427]\n",
            "305 [D loss: 0.999929] [G loss: 1.000416]\n",
            "306 [D loss: 1.000003] [G loss: 1.000475]\n",
            "307 [D loss: 0.999940] [G loss: 1.000418]\n",
            "308 [D loss: 0.999932] [G loss: 1.000356]\n",
            "309 [D loss: 0.999954] [G loss: 1.000358]\n",
            "310 [D loss: 0.999981] [G loss: 1.000415]\n",
            "311 [D loss: 0.999955] [G loss: 1.000372]\n",
            "312 [D loss: 0.999951] [G loss: 1.000322]\n",
            "313 [D loss: 0.999973] [G loss: 1.000339]\n",
            "314 [D loss: 0.999971] [G loss: 1.000364]\n",
            "315 [D loss: 0.999960] [G loss: 1.000324]\n",
            "316 [D loss: 0.999974] [G loss: 1.000359]\n",
            "317 [D loss: 0.999970] [G loss: 1.000284]\n",
            "318 [D loss: 0.999951] [G loss: 1.000330]\n",
            "319 [D loss: 0.999931] [G loss: 1.000314]\n",
            "320 [D loss: 0.999941] [G loss: 1.000315]\n",
            "321 [D loss: 0.999924] [G loss: 1.000324]\n",
            "322 [D loss: 0.999959] [G loss: 1.000317]\n",
            "323 [D loss: 0.999955] [G loss: 1.000281]\n",
            "324 [D loss: 0.999893] [G loss: 1.000283]\n",
            "325 [D loss: 0.999943] [G loss: 1.000291]\n",
            "326 [D loss: 0.999934] [G loss: 1.000279]\n",
            "327 [D loss: 0.999949] [G loss: 1.000236]\n",
            "328 [D loss: 0.999957] [G loss: 1.000243]\n",
            "329 [D loss: 0.999947] [G loss: 1.000245]\n",
            "330 [D loss: 0.999976] [G loss: 1.000240]\n",
            "331 [D loss: 0.999986] [G loss: 1.000211]\n",
            "332 [D loss: 0.999950] [G loss: 1.000269]\n",
            "333 [D loss: 0.999965] [G loss: 1.000224]\n",
            "334 [D loss: 0.999947] [G loss: 1.000257]\n",
            "335 [D loss: 0.999972] [G loss: 1.000225]\n",
            "336 [D loss: 0.999973] [G loss: 1.000227]\n",
            "337 [D loss: 0.999954] [G loss: 1.000228]\n",
            "338 [D loss: 0.999952] [G loss: 1.000215]\n",
            "339 [D loss: 0.999959] [G loss: 1.000225]\n",
            "340 [D loss: 0.999991] [G loss: 1.000202]\n",
            "341 [D loss: 0.999926] [G loss: 1.000233]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "342 [D loss: 0.999960] [G loss: 1.000197]\n",
            "343 [D loss: 0.999938] [G loss: 1.000218]\n",
            "344 [D loss: 0.999936] [G loss: 1.000206]\n",
            "345 [D loss: 0.999941] [G loss: 1.000182]\n",
            "346 [D loss: 0.999941] [G loss: 1.000173]\n",
            "347 [D loss: 0.999957] [G loss: 1.000185]\n",
            "348 [D loss: 0.999977] [G loss: 1.000202]\n",
            "349 [D loss: 0.999961] [G loss: 1.000186]\n",
            "350 [D loss: 0.999964] [G loss: 1.000168]\n",
            "351 [D loss: 0.999946] [G loss: 1.000163]\n",
            "352 [D loss: 0.999955] [G loss: 1.000160]\n",
            "353 [D loss: 0.999950] [G loss: 1.000191]\n",
            "354 [D loss: 0.999963] [G loss: 1.000173]\n",
            "355 [D loss: 0.999937] [G loss: 1.000172]\n",
            "356 [D loss: 0.999972] [G loss: 1.000164]\n",
            "357 [D loss: 0.999965] [G loss: 1.000171]\n",
            "358 [D loss: 0.999977] [G loss: 1.000132]\n",
            "359 [D loss: 0.999960] [G loss: 1.000142]\n",
            "360 [D loss: 0.999976] [G loss: 1.000160]\n",
            "361 [D loss: 0.999941] [G loss: 1.000150]\n",
            "362 [D loss: 0.999968] [G loss: 1.000152]\n",
            "363 [D loss: 0.999957] [G loss: 1.000142]\n",
            "364 [D loss: 0.999951] [G loss: 1.000165]\n",
            "365 [D loss: 0.999996] [G loss: 1.000152]\n",
            "366 [D loss: 0.999980] [G loss: 1.000157]\n",
            "367 [D loss: 0.999981] [G loss: 1.000138]\n",
            "368 [D loss: 0.999978] [G loss: 1.000154]\n",
            "369 [D loss: 0.999966] [G loss: 1.000154]\n",
            "370 [D loss: 0.999932] [G loss: 1.000153]\n",
            "371 [D loss: 0.999967] [G loss: 1.000149]\n",
            "372 [D loss: 0.999962] [G loss: 1.000141]\n",
            "373 [D loss: 0.999972] [G loss: 1.000149]\n",
            "374 [D loss: 0.999984] [G loss: 1.000130]\n",
            "375 [D loss: 0.999924] [G loss: 1.000135]\n",
            "376 [D loss: 0.999985] [G loss: 1.000139]\n",
            "377 [D loss: 0.999964] [G loss: 1.000105]\n",
            "378 [D loss: 0.999980] [G loss: 1.000138]\n",
            "379 [D loss: 0.999979] [G loss: 1.000145]\n",
            "380 [D loss: 0.999977] [G loss: 1.000111]\n",
            "381 [D loss: 0.999974] [G loss: 1.000098]\n",
            "382 [D loss: 0.999970] [G loss: 1.000144]\n",
            "383 [D loss: 0.999971] [G loss: 1.000150]\n",
            "384 [D loss: 0.999958] [G loss: 1.000138]\n",
            "385 [D loss: 0.999968] [G loss: 1.000125]\n",
            "386 [D loss: 0.999968] [G loss: 1.000118]\n",
            "387 [D loss: 0.999964] [G loss: 1.000120]\n",
            "388 [D loss: 0.999944] [G loss: 1.000137]\n",
            "389 [D loss: 0.999998] [G loss: 1.000125]\n",
            "390 [D loss: 0.999949] [G loss: 1.000106]\n",
            "391 [D loss: 0.999985] [G loss: 1.000069]\n",
            "392 [D loss: 0.999959] [G loss: 1.000098]\n",
            "393 [D loss: 0.999930] [G loss: 1.000088]\n",
            "394 [D loss: 0.999953] [G loss: 1.000084]\n",
            "395 [D loss: 0.999940] [G loss: 1.000103]\n",
            "396 [D loss: 0.999940] [G loss: 1.000092]\n",
            "397 [D loss: 0.999995] [G loss: 1.000087]\n",
            "398 [D loss: 0.999992] [G loss: 1.000118]\n",
            "399 [D loss: 0.999952] [G loss: 1.000106]\n",
            "400 [D loss: 0.999983] [G loss: 1.000102]\n",
            "401 [D loss: 0.999933] [G loss: 1.000098]\n",
            "402 [D loss: 0.999986] [G loss: 1.000106]\n",
            "403 [D loss: 0.999952] [G loss: 1.000076]\n",
            "404 [D loss: 0.999970] [G loss: 1.000128]\n",
            "405 [D loss: 0.999977] [G loss: 1.000094]\n",
            "406 [D loss: 0.999971] [G loss: 1.000099]\n",
            "407 [D loss: 0.999941] [G loss: 1.000091]\n",
            "408 [D loss: 0.999968] [G loss: 1.000098]\n",
            "409 [D loss: 0.999987] [G loss: 1.000087]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "410 [D loss: 0.999959] [G loss: 1.000090]\n",
            "411 [D loss: 0.999974] [G loss: 1.000080]\n",
            "412 [D loss: 0.999980] [G loss: 1.000101]\n",
            "413 [D loss: 0.999997] [G loss: 1.000119]\n",
            "414 [D loss: 0.999975] [G loss: 1.000084]\n",
            "415 [D loss: 0.999970] [G loss: 1.000089]\n",
            "416 [D loss: 0.999958] [G loss: 1.000038]\n",
            "417 [D loss: 0.999957] [G loss: 1.000074]\n",
            "418 [D loss: 0.999973] [G loss: 1.000119]\n",
            "419 [D loss: 0.999947] [G loss: 1.000110]\n",
            "420 [D loss: 0.999961] [G loss: 1.000091]\n",
            "421 [D loss: 0.999942] [G loss: 1.000118]\n",
            "422 [D loss: 0.999952] [G loss: 1.000106]\n",
            "423 [D loss: 0.999968] [G loss: 1.000141]\n",
            "424 [D loss: 0.999970] [G loss: 1.000100]\n",
            "425 [D loss: 0.999994] [G loss: 1.000096]\n",
            "426 [D loss: 0.999978] [G loss: 1.000100]\n",
            "427 [D loss: 0.999963] [G loss: 1.000061]\n",
            "428 [D loss: 0.999967] [G loss: 1.000104]\n",
            "429 [D loss: 0.999969] [G loss: 1.000128]\n",
            "430 [D loss: 0.999951] [G loss: 1.000084]\n",
            "431 [D loss: 0.999956] [G loss: 1.000081]\n",
            "432 [D loss: 0.999985] [G loss: 1.000079]\n",
            "433 [D loss: 0.999980] [G loss: 1.000108]\n",
            "434 [D loss: 0.999953] [G loss: 1.000042]\n",
            "435 [D loss: 0.999947] [G loss: 1.000106]\n",
            "436 [D loss: 0.999976] [G loss: 1.000097]\n",
            "437 [D loss: 0.999984] [G loss: 1.000087]\n",
            "438 [D loss: 0.999996] [G loss: 1.000088]\n",
            "439 [D loss: 0.999948] [G loss: 1.000092]\n",
            "440 [D loss: 0.999969] [G loss: 1.000116]\n",
            "441 [D loss: 0.999963] [G loss: 1.000086]\n",
            "442 [D loss: 0.999998] [G loss: 1.000104]\n",
            "443 [D loss: 0.999943] [G loss: 1.000077]\n",
            "444 [D loss: 0.999968] [G loss: 1.000098]\n",
            "445 [D loss: 0.999981] [G loss: 1.000073]\n",
            "446 [D loss: 0.999948] [G loss: 1.000038]\n",
            "447 [D loss: 0.999955] [G loss: 1.000060]\n",
            "448 [D loss: 0.999998] [G loss: 1.000092]\n",
            "449 [D loss: 0.999975] [G loss: 1.000061]\n",
            "450 [D loss: 0.999962] [G loss: 1.000069]\n",
            "451 [D loss: 0.999979] [G loss: 1.000064]\n",
            "452 [D loss: 0.999971] [G loss: 1.000073]\n",
            "453 [D loss: 0.999945] [G loss: 1.000046]\n",
            "454 [D loss: 0.999950] [G loss: 1.000080]\n",
            "455 [D loss: 0.999983] [G loss: 1.000070]\n",
            "456 [D loss: 0.999961] [G loss: 1.000063]\n",
            "457 [D loss: 0.999955] [G loss: 1.000079]\n",
            "458 [D loss: 0.999982] [G loss: 1.000109]\n",
            "459 [D loss: 0.999953] [G loss: 1.000082]\n",
            "460 [D loss: 0.999964] [G loss: 1.000059]\n",
            "461 [D loss: 0.999967] [G loss: 1.000105]\n",
            "462 [D loss: 0.999986] [G loss: 1.000116]\n",
            "463 [D loss: 0.999995] [G loss: 1.000083]\n",
            "464 [D loss: 0.999968] [G loss: 1.000065]\n",
            "465 [D loss: 0.999953] [G loss: 1.000081]\n",
            "466 [D loss: 0.999970] [G loss: 1.000083]\n",
            "467 [D loss: 0.999976] [G loss: 1.000045]\n",
            "468 [D loss: 0.999983] [G loss: 1.000089]\n",
            "469 [D loss: 0.999951] [G loss: 1.000067]\n",
            "470 [D loss: 0.999934] [G loss: 1.000052]\n",
            "471 [D loss: 0.999994] [G loss: 1.000079]\n",
            "472 [D loss: 0.999965] [G loss: 1.000078]\n",
            "473 [D loss: 0.999971] [G loss: 1.000111]\n",
            "474 [D loss: 0.999950] [G loss: 1.000070]\n",
            "475 [D loss: 0.999939] [G loss: 1.000059]\n",
            "476 [D loss: 0.999980] [G loss: 1.000059]\n",
            "477 [D loss: 0.999966] [G loss: 1.000064]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "478 [D loss: 0.999968] [G loss: 1.000085]\n",
            "479 [D loss: 0.999968] [G loss: 1.000046]\n",
            "480 [D loss: 0.999979] [G loss: 1.000083]\n",
            "481 [D loss: 0.999949] [G loss: 1.000045]\n",
            "482 [D loss: 0.999949] [G loss: 1.000081]\n",
            "483 [D loss: 0.999951] [G loss: 1.000071]\n",
            "484 [D loss: 0.999955] [G loss: 1.000094]\n",
            "485 [D loss: 0.999976] [G loss: 1.000056]\n",
            "486 [D loss: 0.999976] [G loss: 1.000079]\n",
            "487 [D loss: 0.999985] [G loss: 1.000045]\n",
            "488 [D loss: 0.999977] [G loss: 1.000080]\n",
            "489 [D loss: 0.999958] [G loss: 1.000079]\n",
            "490 [D loss: 0.999979] [G loss: 1.000066]\n",
            "491 [D loss: 0.999947] [G loss: 1.000033]\n",
            "492 [D loss: 0.999944] [G loss: 1.000067]\n",
            "493 [D loss: 0.999954] [G loss: 1.000082]\n",
            "494 [D loss: 0.999966] [G loss: 1.000058]\n",
            "495 [D loss: 0.999981] [G loss: 1.000013]\n",
            "496 [D loss: 0.999947] [G loss: 1.000086]\n",
            "497 [D loss: 0.999956] [G loss: 1.000083]\n",
            "498 [D loss: 0.999975] [G loss: 1.000070]\n",
            "499 [D loss: 0.999964] [G loss: 1.000070]\n",
            "500 [D loss: 0.999985] [G loss: 1.000068]\n",
            "501 [D loss: 0.999936] [G loss: 1.000077]\n",
            "502 [D loss: 0.999962] [G loss: 1.000083]\n",
            "503 [D loss: 0.999946] [G loss: 1.000095]\n",
            "504 [D loss: 0.999964] [G loss: 1.000067]\n",
            "505 [D loss: 0.999945] [G loss: 1.000064]\n",
            "506 [D loss: 0.999958] [G loss: 1.000064]\n",
            "507 [D loss: 0.999953] [G loss: 1.000054]\n",
            "508 [D loss: 0.999988] [G loss: 1.000063]\n",
            "509 [D loss: 0.999988] [G loss: 1.000086]\n",
            "510 [D loss: 1.000004] [G loss: 1.000048]\n",
            "511 [D loss: 0.999978] [G loss: 1.000056]\n",
            "512 [D loss: 0.999961] [G loss: 1.000077]\n",
            "513 [D loss: 0.999976] [G loss: 1.000063]\n",
            "514 [D loss: 0.999971] [G loss: 1.000068]\n",
            "515 [D loss: 0.999973] [G loss: 1.000058]\n",
            "516 [D loss: 0.999970] [G loss: 1.000069]\n",
            "517 [D loss: 0.999962] [G loss: 1.000056]\n",
            "518 [D loss: 0.999955] [G loss: 1.000099]\n",
            "519 [D loss: 0.999975] [G loss: 1.000060]\n",
            "520 [D loss: 0.999970] [G loss: 1.000072]\n",
            "521 [D loss: 0.999982] [G loss: 1.000058]\n",
            "522 [D loss: 0.999920] [G loss: 1.000062]\n",
            "523 [D loss: 0.999974] [G loss: 1.000047]\n",
            "524 [D loss: 0.999987] [G loss: 1.000086]\n",
            "525 [D loss: 0.999945] [G loss: 1.000069]\n",
            "526 [D loss: 1.000000] [G loss: 1.000098]\n",
            "527 [D loss: 0.999967] [G loss: 1.000067]\n",
            "528 [D loss: 0.999996] [G loss: 1.000042]\n",
            "529 [D loss: 0.999943] [G loss: 1.000080]\n",
            "530 [D loss: 0.999983] [G loss: 1.000053]\n",
            "531 [D loss: 0.999959] [G loss: 1.000059]\n",
            "532 [D loss: 0.999938] [G loss: 1.000040]\n",
            "533 [D loss: 0.999957] [G loss: 1.000055]\n",
            "534 [D loss: 0.999934] [G loss: 1.000061]\n",
            "535 [D loss: 0.999976] [G loss: 1.000052]\n",
            "536 [D loss: 0.999988] [G loss: 1.000066]\n",
            "537 [D loss: 0.999943] [G loss: 1.000067]\n",
            "538 [D loss: 0.999965] [G loss: 1.000069]\n",
            "539 [D loss: 0.999942] [G loss: 1.000072]\n",
            "540 [D loss: 0.999980] [G loss: 1.000058]\n",
            "541 [D loss: 0.999944] [G loss: 1.000063]\n",
            "542 [D loss: 0.999950] [G loss: 1.000075]\n",
            "543 [D loss: 0.999947] [G loss: 1.000085]\n",
            "544 [D loss: 0.999960] [G loss: 1.000041]\n",
            "545 [D loss: 0.999983] [G loss: 1.000073]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "546 [D loss: 0.999960] [G loss: 1.000044]\n",
            "547 [D loss: 0.999963] [G loss: 1.000047]\n",
            "548 [D loss: 0.999937] [G loss: 1.000069]\n",
            "549 [D loss: 0.999955] [G loss: 1.000082]\n",
            "550 [D loss: 0.999964] [G loss: 1.000069]\n",
            "551 [D loss: 0.999965] [G loss: 1.000088]\n",
            "552 [D loss: 0.999963] [G loss: 1.000034]\n",
            "553 [D loss: 0.999983] [G loss: 1.000053]\n",
            "554 [D loss: 0.999945] [G loss: 1.000075]\n",
            "555 [D loss: 0.999970] [G loss: 1.000067]\n",
            "556 [D loss: 0.999946] [G loss: 1.000085]\n",
            "557 [D loss: 0.999967] [G loss: 1.000073]\n",
            "558 [D loss: 0.999966] [G loss: 1.000055]\n",
            "559 [D loss: 0.999962] [G loss: 1.000070]\n",
            "560 [D loss: 0.999981] [G loss: 1.000068]\n",
            "561 [D loss: 0.999987] [G loss: 1.000093]\n",
            "562 [D loss: 0.999965] [G loss: 1.000060]\n",
            "563 [D loss: 0.999952] [G loss: 1.000061]\n",
            "564 [D loss: 0.999956] [G loss: 1.000070]\n",
            "565 [D loss: 0.999965] [G loss: 1.000040]\n",
            "566 [D loss: 0.999975] [G loss: 1.000075]\n",
            "567 [D loss: 0.999999] [G loss: 1.000081]\n",
            "568 [D loss: 0.999966] [G loss: 1.000045]\n",
            "569 [D loss: 0.999932] [G loss: 1.000066]\n",
            "570 [D loss: 0.999970] [G loss: 1.000080]\n",
            "571 [D loss: 0.999962] [G loss: 1.000028]\n",
            "572 [D loss: 0.999987] [G loss: 1.000071]\n",
            "573 [D loss: 0.999979] [G loss: 1.000053]\n",
            "574 [D loss: 0.999984] [G loss: 1.000073]\n",
            "575 [D loss: 0.999966] [G loss: 1.000033]\n",
            "576 [D loss: 0.999961] [G loss: 1.000076]\n",
            "577 [D loss: 0.999977] [G loss: 1.000068]\n",
            "578 [D loss: 0.999966] [G loss: 1.000055]\n",
            "579 [D loss: 0.999953] [G loss: 1.000067]\n",
            "580 [D loss: 0.999954] [G loss: 1.000045]\n",
            "581 [D loss: 0.999968] [G loss: 1.000060]\n",
            "582 [D loss: 0.999964] [G loss: 1.000079]\n",
            "583 [D loss: 0.999967] [G loss: 1.000074]\n",
            "584 [D loss: 0.999967] [G loss: 1.000085]\n",
            "585 [D loss: 0.999959] [G loss: 1.000060]\n",
            "586 [D loss: 0.999952] [G loss: 1.000075]\n",
            "587 [D loss: 0.999967] [G loss: 1.000054]\n",
            "588 [D loss: 0.999969] [G loss: 1.000076]\n",
            "589 [D loss: 0.999957] [G loss: 1.000083]\n",
            "590 [D loss: 0.999956] [G loss: 1.000061]\n",
            "591 [D loss: 0.999961] [G loss: 1.000080]\n",
            "592 [D loss: 0.999965] [G loss: 1.000063]\n",
            "593 [D loss: 0.999953] [G loss: 1.000054]\n",
            "594 [D loss: 0.999946] [G loss: 1.000066]\n",
            "595 [D loss: 0.999986] [G loss: 1.000082]\n",
            "596 [D loss: 0.999988] [G loss: 1.000064]\n",
            "597 [D loss: 0.999948] [G loss: 1.000084]\n",
            "598 [D loss: 0.999955] [G loss: 1.000031]\n",
            "599 [D loss: 0.999971] [G loss: 1.000074]\n",
            "600 [D loss: 0.999973] [G loss: 1.000076]\n",
            "601 [D loss: 0.999969] [G loss: 1.000049]\n",
            "602 [D loss: 0.999954] [G loss: 1.000092]\n",
            "603 [D loss: 0.999981] [G loss: 1.000063]\n",
            "604 [D loss: 0.999984] [G loss: 1.000070]\n",
            "605 [D loss: 0.999936] [G loss: 1.000062]\n",
            "606 [D loss: 0.999954] [G loss: 1.000071]\n",
            "607 [D loss: 0.999992] [G loss: 1.000086]\n",
            "608 [D loss: 0.999965] [G loss: 1.000092]\n",
            "609 [D loss: 0.999989] [G loss: 1.000078]\n",
            "610 [D loss: 0.999969] [G loss: 1.000069]\n",
            "611 [D loss: 0.999959] [G loss: 1.000082]\n",
            "612 [D loss: 0.999950] [G loss: 1.000083]\n",
            "613 [D loss: 0.999944] [G loss: 1.000053]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "614 [D loss: 0.999970] [G loss: 1.000071]\n",
            "615 [D loss: 0.999936] [G loss: 1.000085]\n",
            "616 [D loss: 0.999953] [G loss: 1.000076]\n",
            "617 [D loss: 0.999949] [G loss: 1.000080]\n",
            "618 [D loss: 0.999991] [G loss: 1.000103]\n",
            "619 [D loss: 0.999964] [G loss: 1.000080]\n",
            "620 [D loss: 0.999987] [G loss: 1.000054]\n",
            "621 [D loss: 0.999952] [G loss: 1.000059]\n",
            "622 [D loss: 0.999971] [G loss: 1.000092]\n",
            "623 [D loss: 0.999962] [G loss: 1.000065]\n",
            "624 [D loss: 0.999947] [G loss: 1.000084]\n",
            "625 [D loss: 0.999984] [G loss: 1.000079]\n",
            "626 [D loss: 0.999973] [G loss: 1.000030]\n",
            "627 [D loss: 0.999941] [G loss: 1.000078]\n",
            "628 [D loss: 0.999945] [G loss: 1.000090]\n",
            "629 [D loss: 0.999964] [G loss: 1.000072]\n",
            "630 [D loss: 0.999980] [G loss: 1.000050]\n",
            "631 [D loss: 0.999964] [G loss: 1.000064]\n",
            "632 [D loss: 0.999966] [G loss: 1.000045]\n",
            "633 [D loss: 0.999968] [G loss: 1.000062]\n",
            "634 [D loss: 0.999953] [G loss: 1.000074]\n",
            "635 [D loss: 0.999969] [G loss: 1.000079]\n",
            "636 [D loss: 0.999940] [G loss: 1.000065]\n",
            "637 [D loss: 0.999968] [G loss: 1.000095]\n",
            "638 [D loss: 0.999962] [G loss: 1.000086]\n",
            "639 [D loss: 0.999952] [G loss: 1.000045]\n",
            "640 [D loss: 0.999989] [G loss: 1.000063]\n",
            "641 [D loss: 0.999957] [G loss: 1.000048]\n",
            "642 [D loss: 0.999970] [G loss: 1.000056]\n",
            "643 [D loss: 0.999973] [G loss: 1.000058]\n",
            "644 [D loss: 0.999988] [G loss: 1.000074]\n",
            "645 [D loss: 0.999975] [G loss: 1.000063]\n",
            "646 [D loss: 0.999971] [G loss: 1.000033]\n",
            "647 [D loss: 0.999991] [G loss: 1.000065]\n",
            "648 [D loss: 0.999998] [G loss: 1.000093]\n",
            "649 [D loss: 0.999959] [G loss: 1.000056]\n",
            "650 [D loss: 0.999987] [G loss: 1.000096]\n",
            "651 [D loss: 0.999970] [G loss: 1.000053]\n",
            "652 [D loss: 0.999988] [G loss: 1.000072]\n",
            "653 [D loss: 0.999977] [G loss: 1.000034]\n",
            "654 [D loss: 0.999949] [G loss: 1.000066]\n",
            "655 [D loss: 0.999990] [G loss: 1.000086]\n",
            "656 [D loss: 0.999957] [G loss: 1.000067]\n",
            "657 [D loss: 0.999962] [G loss: 1.000065]\n",
            "658 [D loss: 0.999954] [G loss: 1.000075]\n",
            "659 [D loss: 0.999954] [G loss: 1.000083]\n",
            "660 [D loss: 0.999944] [G loss: 1.000069]\n",
            "661 [D loss: 0.999972] [G loss: 1.000051]\n",
            "662 [D loss: 0.999941] [G loss: 1.000064]\n",
            "663 [D loss: 0.999948] [G loss: 1.000082]\n",
            "664 [D loss: 0.999968] [G loss: 1.000022]\n",
            "665 [D loss: 0.999955] [G loss: 1.000075]\n",
            "666 [D loss: 0.999958] [G loss: 1.000086]\n",
            "667 [D loss: 0.999981] [G loss: 1.000069]\n",
            "668 [D loss: 0.999934] [G loss: 1.000094]\n",
            "669 [D loss: 0.999946] [G loss: 1.000100]\n",
            "670 [D loss: 0.999979] [G loss: 1.000074]\n",
            "671 [D loss: 0.999980] [G loss: 1.000069]\n",
            "672 [D loss: 0.999967] [G loss: 1.000071]\n",
            "673 [D loss: 0.999990] [G loss: 1.000060]\n",
            "674 [D loss: 0.999953] [G loss: 1.000111]\n",
            "675 [D loss: 0.999961] [G loss: 1.000078]\n",
            "676 [D loss: 1.000004] [G loss: 1.000054]\n",
            "677 [D loss: 0.999996] [G loss: 1.000052]\n",
            "678 [D loss: 0.999944] [G loss: 1.000068]\n",
            "679 [D loss: 1.000008] [G loss: 1.000071]\n",
            "680 [D loss: 1.000000] [G loss: 1.000051]\n",
            "681 [D loss: 0.999963] [G loss: 1.000079]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "682 [D loss: 0.999958] [G loss: 1.000045]\n",
            "683 [D loss: 0.999961] [G loss: 1.000065]\n",
            "684 [D loss: 0.999971] [G loss: 1.000061]\n",
            "685 [D loss: 0.999975] [G loss: 1.000088]\n",
            "686 [D loss: 0.999951] [G loss: 1.000049]\n",
            "687 [D loss: 0.999955] [G loss: 1.000040]\n",
            "688 [D loss: 0.999956] [G loss: 1.000060]\n",
            "689 [D loss: 0.999959] [G loss: 1.000047]\n",
            "690 [D loss: 0.999980] [G loss: 1.000053]\n",
            "691 [D loss: 0.999985] [G loss: 1.000064]\n",
            "692 [D loss: 0.999967] [G loss: 1.000062]\n",
            "693 [D loss: 0.999969] [G loss: 1.000047]\n",
            "694 [D loss: 0.999987] [G loss: 1.000056]\n",
            "695 [D loss: 0.999986] [G loss: 1.000057]\n",
            "696 [D loss: 0.999973] [G loss: 1.000063]\n",
            "697 [D loss: 0.999966] [G loss: 1.000042]\n",
            "698 [D loss: 0.999950] [G loss: 1.000058]\n",
            "699 [D loss: 0.999990] [G loss: 1.000062]\n",
            "700 [D loss: 1.000004] [G loss: 1.000097]\n",
            "701 [D loss: 0.999928] [G loss: 1.000033]\n",
            "702 [D loss: 0.999997] [G loss: 1.000034]\n",
            "703 [D loss: 0.999981] [G loss: 1.000074]\n",
            "704 [D loss: 0.999926] [G loss: 1.000057]\n",
            "705 [D loss: 1.000004] [G loss: 1.000055]\n",
            "706 [D loss: 0.999968] [G loss: 1.000065]\n",
            "707 [D loss: 0.999955] [G loss: 1.000088]\n",
            "708 [D loss: 0.999961] [G loss: 1.000032]\n",
            "709 [D loss: 0.999963] [G loss: 1.000074]\n",
            "710 [D loss: 0.999936] [G loss: 1.000043]\n",
            "711 [D loss: 0.999953] [G loss: 1.000067]\n",
            "712 [D loss: 0.999961] [G loss: 1.000051]\n",
            "713 [D loss: 0.999984] [G loss: 1.000073]\n",
            "714 [D loss: 0.999994] [G loss: 1.000071]\n",
            "715 [D loss: 0.999966] [G loss: 1.000070]\n",
            "716 [D loss: 0.999949] [G loss: 1.000064]\n",
            "717 [D loss: 0.999976] [G loss: 1.000029]\n",
            "718 [D loss: 0.999981] [G loss: 1.000075]\n",
            "719 [D loss: 0.999984] [G loss: 1.000078]\n",
            "720 [D loss: 0.999984] [G loss: 1.000062]\n",
            "721 [D loss: 0.999962] [G loss: 1.000052]\n",
            "722 [D loss: 0.999946] [G loss: 1.000086]\n",
            "723 [D loss: 0.999937] [G loss: 1.000123]\n",
            "724 [D loss: 0.999995] [G loss: 1.000048]\n",
            "725 [D loss: 0.999952] [G loss: 1.000093]\n",
            "726 [D loss: 0.999966] [G loss: 1.000075]\n",
            "727 [D loss: 1.000009] [G loss: 1.000038]\n",
            "728 [D loss: 0.999966] [G loss: 1.000062]\n",
            "729 [D loss: 0.999961] [G loss: 1.000093]\n",
            "730 [D loss: 0.999979] [G loss: 1.000050]\n",
            "731 [D loss: 0.999994] [G loss: 1.000072]\n",
            "732 [D loss: 0.999957] [G loss: 1.000079]\n",
            "733 [D loss: 0.999968] [G loss: 1.000068]\n",
            "734 [D loss: 0.999967] [G loss: 1.000074]\n",
            "735 [D loss: 0.999946] [G loss: 1.000050]\n",
            "736 [D loss: 0.999974] [G loss: 1.000047]\n",
            "737 [D loss: 0.999939] [G loss: 1.000081]\n",
            "738 [D loss: 0.999994] [G loss: 1.000074]\n",
            "739 [D loss: 0.999963] [G loss: 1.000085]\n",
            "740 [D loss: 0.999939] [G loss: 1.000048]\n",
            "741 [D loss: 0.999983] [G loss: 1.000021]\n",
            "742 [D loss: 0.999990] [G loss: 1.000077]\n",
            "743 [D loss: 0.999972] [G loss: 1.000061]\n",
            "744 [D loss: 0.999949] [G loss: 1.000062]\n",
            "745 [D loss: 0.999942] [G loss: 1.000061]\n",
            "746 [D loss: 0.999980] [G loss: 1.000076]\n",
            "747 [D loss: 0.999940] [G loss: 1.000072]\n",
            "748 [D loss: 0.999953] [G loss: 1.000066]\n",
            "749 [D loss: 0.999986] [G loss: 1.000062]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "750 [D loss: 0.999990] [G loss: 1.000091]\n",
            "751 [D loss: 0.999963] [G loss: 1.000095]\n",
            "752 [D loss: 0.999985] [G loss: 1.000052]\n",
            "753 [D loss: 0.999963] [G loss: 1.000077]\n",
            "754 [D loss: 0.999993] [G loss: 1.000098]\n",
            "755 [D loss: 0.999952] [G loss: 1.000063]\n",
            "756 [D loss: 0.999996] [G loss: 1.000058]\n",
            "757 [D loss: 0.999947] [G loss: 1.000054]\n",
            "758 [D loss: 0.999937] [G loss: 1.000085]\n",
            "759 [D loss: 0.999972] [G loss: 1.000097]\n",
            "760 [D loss: 0.999966] [G loss: 1.000079]\n",
            "761 [D loss: 1.000019] [G loss: 1.000085]\n",
            "762 [D loss: 0.999957] [G loss: 1.000091]\n",
            "763 [D loss: 0.999989] [G loss: 1.000092]\n",
            "764 [D loss: 0.999946] [G loss: 1.000072]\n",
            "765 [D loss: 0.999970] [G loss: 1.000080]\n",
            "766 [D loss: 0.999958] [G loss: 1.000044]\n",
            "767 [D loss: 0.999955] [G loss: 1.000079]\n",
            "768 [D loss: 1.000013] [G loss: 1.000075]\n",
            "769 [D loss: 0.999971] [G loss: 1.000127]\n",
            "770 [D loss: 0.999934] [G loss: 1.000062]\n",
            "771 [D loss: 0.999964] [G loss: 1.000044]\n",
            "772 [D loss: 0.999957] [G loss: 1.000060]\n",
            "773 [D loss: 0.999945] [G loss: 1.000065]\n",
            "774 [D loss: 0.999955] [G loss: 1.000021]\n",
            "775 [D loss: 0.999993] [G loss: 1.000053]\n",
            "776 [D loss: 0.999965] [G loss: 1.000043]\n",
            "777 [D loss: 0.999926] [G loss: 1.000078]\n",
            "778 [D loss: 0.999983] [G loss: 1.000067]\n",
            "779 [D loss: 0.999965] [G loss: 1.000064]\n",
            "780 [D loss: 0.999976] [G loss: 1.000088]\n",
            "781 [D loss: 1.000002] [G loss: 1.000102]\n",
            "782 [D loss: 0.999974] [G loss: 1.000089]\n",
            "783 [D loss: 1.000013] [G loss: 1.000086]\n",
            "784 [D loss: 0.999974] [G loss: 1.000081]\n",
            "785 [D loss: 0.999970] [G loss: 1.000046]\n",
            "786 [D loss: 0.999988] [G loss: 1.000046]\n",
            "787 [D loss: 0.999957] [G loss: 1.000071]\n",
            "788 [D loss: 0.999955] [G loss: 1.000052]\n",
            "789 [D loss: 0.999993] [G loss: 1.000064]\n",
            "790 [D loss: 0.999999] [G loss: 1.000075]\n",
            "791 [D loss: 0.999949] [G loss: 1.000075]\n",
            "792 [D loss: 0.999967] [G loss: 1.000059]\n",
            "793 [D loss: 0.999966] [G loss: 1.000045]\n",
            "794 [D loss: 0.999972] [G loss: 1.000060]\n",
            "795 [D loss: 0.999946] [G loss: 1.000083]\n",
            "796 [D loss: 0.999976] [G loss: 1.000083]\n",
            "797 [D loss: 0.999947] [G loss: 1.000046]\n",
            "798 [D loss: 0.999992] [G loss: 1.000087]\n",
            "799 [D loss: 0.999978] [G loss: 1.000057]\n",
            "800 [D loss: 0.999967] [G loss: 1.000072]\n",
            "801 [D loss: 0.999953] [G loss: 1.000060]\n",
            "802 [D loss: 0.999949] [G loss: 1.000052]\n",
            "803 [D loss: 0.999938] [G loss: 1.000110]\n",
            "804 [D loss: 1.000005] [G loss: 1.000073]\n",
            "805 [D loss: 0.999965] [G loss: 1.000064]\n",
            "806 [D loss: 0.999992] [G loss: 1.000087]\n",
            "807 [D loss: 0.999960] [G loss: 1.000055]\n",
            "808 [D loss: 0.999941] [G loss: 1.000033]\n",
            "809 [D loss: 0.999966] [G loss: 1.000064]\n",
            "810 [D loss: 0.999980] [G loss: 1.000076]\n",
            "811 [D loss: 0.999981] [G loss: 1.000037]\n",
            "812 [D loss: 1.000006] [G loss: 1.000108]\n",
            "813 [D loss: 0.999940] [G loss: 1.000082]\n",
            "814 [D loss: 0.999965] [G loss: 1.000033]\n",
            "815 [D loss: 0.999961] [G loss: 1.000060]\n",
            "816 [D loss: 0.999969] [G loss: 1.000059]\n",
            "817 [D loss: 0.999976] [G loss: 1.000084]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "818 [D loss: 0.999934] [G loss: 1.000093]\n",
            "819 [D loss: 0.999966] [G loss: 1.000078]\n",
            "820 [D loss: 0.999921] [G loss: 1.000076]\n",
            "821 [D loss: 0.999976] [G loss: 1.000080]\n",
            "822 [D loss: 0.999979] [G loss: 1.000096]\n",
            "823 [D loss: 0.999984] [G loss: 1.000055]\n",
            "824 [D loss: 0.999956] [G loss: 1.000074]\n",
            "825 [D loss: 0.999952] [G loss: 1.000051]\n",
            "826 [D loss: 0.999981] [G loss: 1.000037]\n",
            "827 [D loss: 0.999959] [G loss: 1.000066]\n",
            "828 [D loss: 0.999995] [G loss: 1.000118]\n",
            "829 [D loss: 0.999971] [G loss: 1.000067]\n",
            "830 [D loss: 0.999975] [G loss: 1.000051]\n",
            "831 [D loss: 0.999942] [G loss: 1.000097]\n",
            "832 [D loss: 0.999943] [G loss: 1.000020]\n",
            "833 [D loss: 0.999953] [G loss: 1.000054]\n",
            "834 [D loss: 0.999974] [G loss: 1.000071]\n",
            "835 [D loss: 0.999943] [G loss: 1.000083]\n",
            "836 [D loss: 0.999983] [G loss: 1.000083]\n",
            "837 [D loss: 0.999992] [G loss: 1.000040]\n",
            "838 [D loss: 0.999954] [G loss: 1.000031]\n",
            "839 [D loss: 0.999970] [G loss: 1.000088]\n",
            "840 [D loss: 0.999974] [G loss: 1.000106]\n",
            "841 [D loss: 0.999936] [G loss: 1.000076]\n",
            "842 [D loss: 0.999931] [G loss: 1.000073]\n",
            "843 [D loss: 0.999951] [G loss: 1.000071]\n",
            "844 [D loss: 0.999959] [G loss: 1.000103]\n",
            "845 [D loss: 0.999942] [G loss: 1.000081]\n",
            "846 [D loss: 0.999961] [G loss: 1.000067]\n",
            "847 [D loss: 0.999955] [G loss: 1.000071]\n",
            "848 [D loss: 0.999963] [G loss: 1.000104]\n",
            "849 [D loss: 0.999954] [G loss: 1.000060]\n",
            "850 [D loss: 0.999950] [G loss: 1.000060]\n",
            "851 [D loss: 0.999949] [G loss: 1.000071]\n",
            "852 [D loss: 0.999952] [G loss: 1.000072]\n",
            "853 [D loss: 0.999988] [G loss: 1.000084]\n",
            "854 [D loss: 0.999979] [G loss: 1.000093]\n",
            "855 [D loss: 0.999931] [G loss: 1.000074]\n",
            "856 [D loss: 0.999989] [G loss: 1.000093]\n",
            "857 [D loss: 0.999936] [G loss: 1.000051]\n",
            "858 [D loss: 0.999955] [G loss: 1.000108]\n",
            "859 [D loss: 0.999991] [G loss: 1.000075]\n",
            "860 [D loss: 0.999982] [G loss: 1.000052]\n",
            "861 [D loss: 0.999949] [G loss: 1.000086]\n",
            "862 [D loss: 0.999969] [G loss: 1.000091]\n",
            "863 [D loss: 0.999994] [G loss: 1.000073]\n",
            "864 [D loss: 0.999947] [G loss: 1.000079]\n",
            "865 [D loss: 0.999958] [G loss: 1.000083]\n",
            "866 [D loss: 0.999993] [G loss: 1.000056]\n",
            "867 [D loss: 0.999942] [G loss: 1.000070]\n",
            "868 [D loss: 0.999968] [G loss: 1.000065]\n",
            "869 [D loss: 0.999949] [G loss: 1.000109]\n",
            "870 [D loss: 0.999946] [G loss: 1.000085]\n",
            "871 [D loss: 0.999992] [G loss: 1.000047]\n",
            "872 [D loss: 0.999963] [G loss: 1.000058]\n",
            "873 [D loss: 0.999948] [G loss: 1.000039]\n",
            "874 [D loss: 1.000004] [G loss: 1.000077]\n",
            "875 [D loss: 0.999986] [G loss: 1.000066]\n",
            "876 [D loss: 0.999969] [G loss: 1.000063]\n",
            "877 [D loss: 0.999952] [G loss: 1.000057]\n",
            "878 [D loss: 0.999939] [G loss: 1.000093]\n",
            "879 [D loss: 1.000001] [G loss: 1.000066]\n",
            "880 [D loss: 0.999960] [G loss: 1.000072]\n",
            "881 [D loss: 1.000000] [G loss: 1.000023]\n",
            "882 [D loss: 0.999989] [G loss: 1.000031]\n",
            "883 [D loss: 0.999982] [G loss: 1.000074]\n",
            "884 [D loss: 0.999994] [G loss: 1.000087]\n",
            "885 [D loss: 0.999943] [G loss: 1.000072]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "886 [D loss: 0.999987] [G loss: 1.000037]\n",
            "887 [D loss: 0.999968] [G loss: 1.000043]\n",
            "888 [D loss: 0.999960] [G loss: 1.000088]\n",
            "889 [D loss: 0.999969] [G loss: 1.000067]\n",
            "890 [D loss: 0.999970] [G loss: 1.000080]\n",
            "891 [D loss: 0.999982] [G loss: 1.000038]\n",
            "892 [D loss: 0.999974] [G loss: 1.000104]\n",
            "893 [D loss: 0.999950] [G loss: 1.000093]\n",
            "894 [D loss: 0.999980] [G loss: 1.000065]\n",
            "895 [D loss: 0.999967] [G loss: 1.000063]\n",
            "896 [D loss: 0.999948] [G loss: 1.000064]\n",
            "897 [D loss: 0.999939] [G loss: 1.000060]\n",
            "898 [D loss: 0.999968] [G loss: 1.000089]\n",
            "899 [D loss: 0.999959] [G loss: 1.000074]\n",
            "900 [D loss: 0.999969] [G loss: 1.000082]\n",
            "901 [D loss: 0.999989] [G loss: 1.000009]\n",
            "902 [D loss: 0.999970] [G loss: 1.000061]\n",
            "903 [D loss: 0.999948] [G loss: 1.000048]\n",
            "904 [D loss: 0.999953] [G loss: 1.000090]\n",
            "905 [D loss: 0.999959] [G loss: 1.000080]\n",
            "906 [D loss: 0.999948] [G loss: 1.000091]\n",
            "907 [D loss: 0.999960] [G loss: 1.000043]\n",
            "908 [D loss: 0.999959] [G loss: 1.000058]\n",
            "909 [D loss: 0.999942] [G loss: 1.000081]\n",
            "910 [D loss: 0.999963] [G loss: 1.000066]\n",
            "911 [D loss: 1.000015] [G loss: 1.000080]\n",
            "912 [D loss: 0.999944] [G loss: 1.000049]\n",
            "913 [D loss: 0.999937] [G loss: 1.000065]\n",
            "914 [D loss: 0.999950] [G loss: 1.000095]\n",
            "915 [D loss: 0.999970] [G loss: 1.000057]\n",
            "916 [D loss: 0.999975] [G loss: 1.000061]\n",
            "917 [D loss: 0.999969] [G loss: 1.000083]\n",
            "918 [D loss: 0.999971] [G loss: 1.000061]\n",
            "919 [D loss: 0.999969] [G loss: 1.000091]\n",
            "920 [D loss: 0.999940] [G loss: 1.000040]\n",
            "921 [D loss: 0.999936] [G loss: 1.000097]\n",
            "922 [D loss: 0.999932] [G loss: 1.000057]\n",
            "923 [D loss: 0.999943] [G loss: 1.000054]\n",
            "924 [D loss: 0.999985] [G loss: 1.000080]\n",
            "925 [D loss: 0.999976] [G loss: 1.000031]\n",
            "926 [D loss: 0.999949] [G loss: 1.000054]\n",
            "927 [D loss: 0.999957] [G loss: 1.000077]\n",
            "928 [D loss: 0.999999] [G loss: 1.000076]\n",
            "929 [D loss: 0.999912] [G loss: 1.000094]\n",
            "930 [D loss: 0.999991] [G loss: 1.000075]\n",
            "931 [D loss: 0.999964] [G loss: 1.000055]\n",
            "932 [D loss: 0.999951] [G loss: 1.000090]\n",
            "933 [D loss: 0.999944] [G loss: 1.000062]\n",
            "934 [D loss: 0.999993] [G loss: 1.000109]\n",
            "935 [D loss: 0.999928] [G loss: 1.000031]\n",
            "936 [D loss: 0.999990] [G loss: 1.000053]\n",
            "937 [D loss: 0.999982] [G loss: 1.000031]\n",
            "938 [D loss: 0.999965] [G loss: 1.000069]\n",
            "939 [D loss: 0.999969] [G loss: 1.000063]\n",
            "940 [D loss: 0.999977] [G loss: 1.000045]\n",
            "941 [D loss: 1.000037] [G loss: 1.000117]\n",
            "942 [D loss: 0.999973] [G loss: 1.000072]\n",
            "943 [D loss: 0.999994] [G loss: 1.000029]\n",
            "944 [D loss: 0.999987] [G loss: 1.000071]\n",
            "945 [D loss: 0.999931] [G loss: 1.000043]\n",
            "946 [D loss: 0.999947] [G loss: 1.000071]\n",
            "947 [D loss: 1.000001] [G loss: 1.000084]\n",
            "948 [D loss: 0.999959] [G loss: 1.000070]\n",
            "949 [D loss: 0.999993] [G loss: 1.000053]\n",
            "950 [D loss: 0.999940] [G loss: 1.000082]\n",
            "951 [D loss: 1.000001] [G loss: 1.000050]\n",
            "952 [D loss: 1.000027] [G loss: 1.000058]\n",
            "953 [D loss: 0.999967] [G loss: 1.000062]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "954 [D loss: 0.999911] [G loss: 1.000068]\n",
            "955 [D loss: 0.999936] [G loss: 1.000060]\n",
            "956 [D loss: 0.999974] [G loss: 1.000032]\n",
            "957 [D loss: 1.000003] [G loss: 1.000046]\n",
            "958 [D loss: 0.999993] [G loss: 1.000067]\n",
            "959 [D loss: 0.999972] [G loss: 1.000073]\n",
            "960 [D loss: 0.999978] [G loss: 1.000076]\n",
            "961 [D loss: 0.999954] [G loss: 1.000091]\n",
            "962 [D loss: 0.999982] [G loss: 1.000082]\n",
            "963 [D loss: 0.999984] [G loss: 1.000103]\n",
            "964 [D loss: 0.999971] [G loss: 1.000097]\n",
            "965 [D loss: 0.999957] [G loss: 1.000075]\n",
            "966 [D loss: 0.999939] [G loss: 1.000066]\n",
            "967 [D loss: 0.999945] [G loss: 1.000085]\n",
            "968 [D loss: 0.999949] [G loss: 1.000075]\n",
            "969 [D loss: 0.999963] [G loss: 1.000042]\n",
            "970 [D loss: 0.999999] [G loss: 1.000034]\n",
            "971 [D loss: 1.000026] [G loss: 1.000072]\n",
            "972 [D loss: 1.000008] [G loss: 1.000080]\n",
            "973 [D loss: 0.999969] [G loss: 1.000086]\n",
            "974 [D loss: 0.999998] [G loss: 1.000102]\n",
            "975 [D loss: 0.999993] [G loss: 1.000061]\n",
            "976 [D loss: 0.999949] [G loss: 1.000059]\n",
            "977 [D loss: 0.999988] [G loss: 1.000101]\n",
            "978 [D loss: 0.999957] [G loss: 1.000080]\n",
            "979 [D loss: 0.999970] [G loss: 1.000115]\n",
            "980 [D loss: 0.999961] [G loss: 1.000070]\n",
            "981 [D loss: 0.999948] [G loss: 1.000068]\n",
            "982 [D loss: 0.999990] [G loss: 1.000083]\n",
            "983 [D loss: 0.999980] [G loss: 1.000092]\n",
            "984 [D loss: 0.999939] [G loss: 1.000098]\n",
            "985 [D loss: 0.999969] [G loss: 1.000028]\n",
            "986 [D loss: 0.999963] [G loss: 1.000032]\n",
            "987 [D loss: 0.999940] [G loss: 1.000082]\n",
            "988 [D loss: 0.999904] [G loss: 1.000092]\n",
            "989 [D loss: 0.999999] [G loss: 1.000069]\n",
            "990 [D loss: 0.999965] [G loss: 1.000070]\n",
            "991 [D loss: 0.999946] [G loss: 1.000108]\n",
            "992 [D loss: 0.999972] [G loss: 1.000046]\n",
            "993 [D loss: 0.999948] [G loss: 1.000062]\n",
            "994 [D loss: 0.999978] [G loss: 1.000036]\n",
            "995 [D loss: 0.999974] [G loss: 1.000028]\n",
            "996 [D loss: 0.999977] [G loss: 1.000094]\n",
            "997 [D loss: 1.000015] [G loss: 1.000123]\n",
            "998 [D loss: 0.999915] [G loss: 1.000080]\n",
            "999 [D loss: 0.999995] [G loss: 1.000024]\n",
            "1000 [D loss: 0.999927] [G loss: 1.000089]\n",
            "1001 [D loss: 0.999940] [G loss: 1.000072]\n",
            "1002 [D loss: 0.999983] [G loss: 1.000053]\n",
            "1003 [D loss: 0.999960] [G loss: 1.000031]\n",
            "1004 [D loss: 0.999962] [G loss: 1.000086]\n",
            "1005 [D loss: 0.999985] [G loss: 1.000048]\n",
            "1006 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1007 [D loss: 0.999957] [G loss: 1.000099]\n",
            "1008 [D loss: 0.999947] [G loss: 1.000089]\n",
            "1009 [D loss: 0.999992] [G loss: 1.000082]\n",
            "1010 [D loss: 0.999968] [G loss: 1.000038]\n",
            "1011 [D loss: 0.999983] [G loss: 1.000065]\n",
            "1012 [D loss: 0.999965] [G loss: 1.000053]\n",
            "1013 [D loss: 0.999965] [G loss: 1.000060]\n",
            "1014 [D loss: 0.999942] [G loss: 1.000093]\n",
            "1015 [D loss: 0.999969] [G loss: 1.000091]\n",
            "1016 [D loss: 0.999964] [G loss: 1.000091]\n",
            "1017 [D loss: 0.999966] [G loss: 1.000076]\n",
            "1018 [D loss: 0.999947] [G loss: 1.000121]\n",
            "1019 [D loss: 0.999971] [G loss: 1.000097]\n",
            "1020 [D loss: 0.999923] [G loss: 1.000035]\n",
            "1021 [D loss: 0.999962] [G loss: 1.000058]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1022 [D loss: 0.999978] [G loss: 1.000060]\n",
            "1023 [D loss: 0.999970] [G loss: 1.000050]\n",
            "1024 [D loss: 0.999999] [G loss: 1.000059]\n",
            "1025 [D loss: 0.999965] [G loss: 1.000058]\n",
            "1026 [D loss: 0.999983] [G loss: 1.000045]\n",
            "1027 [D loss: 0.999945] [G loss: 1.000074]\n",
            "1028 [D loss: 1.000002] [G loss: 1.000025]\n",
            "1029 [D loss: 0.999945] [G loss: 1.000085]\n",
            "1030 [D loss: 0.999970] [G loss: 1.000070]\n",
            "1031 [D loss: 0.999987] [G loss: 1.000037]\n",
            "1032 [D loss: 0.999968] [G loss: 1.000070]\n",
            "1033 [D loss: 0.999944] [G loss: 1.000042]\n",
            "1034 [D loss: 0.999989] [G loss: 1.000084]\n",
            "1035 [D loss: 0.999964] [G loss: 1.000057]\n",
            "1036 [D loss: 0.999986] [G loss: 1.000089]\n",
            "1037 [D loss: 0.999964] [G loss: 1.000078]\n",
            "1038 [D loss: 0.999946] [G loss: 1.000073]\n",
            "1039 [D loss: 0.999954] [G loss: 1.000072]\n",
            "1040 [D loss: 0.999986] [G loss: 1.000079]\n",
            "1041 [D loss: 0.999944] [G loss: 1.000091]\n",
            "1042 [D loss: 0.999994] [G loss: 1.000071]\n",
            "1043 [D loss: 0.999936] [G loss: 1.000058]\n",
            "1044 [D loss: 0.999991] [G loss: 1.000029]\n",
            "1045 [D loss: 0.999990] [G loss: 1.000037]\n",
            "1046 [D loss: 0.999962] [G loss: 1.000092]\n",
            "1047 [D loss: 0.999982] [G loss: 1.000073]\n",
            "1048 [D loss: 0.999972] [G loss: 1.000085]\n",
            "1049 [D loss: 0.999955] [G loss: 1.000089]\n",
            "1050 [D loss: 0.999968] [G loss: 1.000073]\n",
            "1051 [D loss: 0.999965] [G loss: 1.000049]\n",
            "1052 [D loss: 0.999960] [G loss: 1.000064]\n",
            "1053 [D loss: 0.999941] [G loss: 1.000097]\n",
            "1054 [D loss: 0.999980] [G loss: 1.000090]\n",
            "1055 [D loss: 0.999971] [G loss: 1.000085]\n",
            "1056 [D loss: 0.999977] [G loss: 1.000073]\n",
            "1057 [D loss: 0.999972] [G loss: 1.000068]\n",
            "1058 [D loss: 0.999965] [G loss: 1.000079]\n",
            "1059 [D loss: 0.999945] [G loss: 1.000069]\n",
            "1060 [D loss: 0.999978] [G loss: 1.000041]\n",
            "1061 [D loss: 0.999978] [G loss: 1.000079]\n",
            "1062 [D loss: 0.999985] [G loss: 1.000077]\n",
            "1063 [D loss: 0.999949] [G loss: 1.000107]\n",
            "1064 [D loss: 0.999965] [G loss: 1.000074]\n",
            "1065 [D loss: 1.000028] [G loss: 1.000097]\n",
            "1066 [D loss: 0.999982] [G loss: 1.000093]\n",
            "1067 [D loss: 0.999986] [G loss: 1.000058]\n",
            "1068 [D loss: 0.999963] [G loss: 1.000057]\n",
            "1069 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1070 [D loss: 1.000009] [G loss: 1.000064]\n",
            "1071 [D loss: 0.999997] [G loss: 1.000024]\n",
            "1072 [D loss: 0.999969] [G loss: 1.000087]\n",
            "1073 [D loss: 0.999961] [G loss: 1.000089]\n",
            "1074 [D loss: 0.999984] [G loss: 1.000063]\n",
            "1075 [D loss: 0.999963] [G loss: 1.000077]\n",
            "1076 [D loss: 0.999956] [G loss: 1.000078]\n",
            "1077 [D loss: 1.000013] [G loss: 1.000055]\n",
            "1078 [D loss: 1.000004] [G loss: 1.000061]\n",
            "1079 [D loss: 0.999938] [G loss: 1.000099]\n",
            "1080 [D loss: 1.000018] [G loss: 1.000097]\n",
            "1081 [D loss: 0.999933] [G loss: 1.000061]\n",
            "1082 [D loss: 1.000006] [G loss: 1.000014]\n",
            "1083 [D loss: 0.999948] [G loss: 1.000043]\n",
            "1084 [D loss: 0.999981] [G loss: 1.000079]\n",
            "1085 [D loss: 0.999957] [G loss: 1.000085]\n",
            "1086 [D loss: 0.999903] [G loss: 1.000079]\n",
            "1087 [D loss: 0.999984] [G loss: 1.000075]\n",
            "1088 [D loss: 0.999991] [G loss: 1.000085]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1089 [D loss: 0.999930] [G loss: 1.000057]\n",
            "1090 [D loss: 0.999961] [G loss: 1.000075]\n",
            "1091 [D loss: 0.999948] [G loss: 1.000055]\n",
            "1092 [D loss: 0.999978] [G loss: 1.000034]\n",
            "1093 [D loss: 0.999998] [G loss: 1.000071]\n",
            "1094 [D loss: 0.999982] [G loss: 1.000087]\n",
            "1095 [D loss: 0.999936] [G loss: 1.000055]\n",
            "1096 [D loss: 0.999954] [G loss: 1.000044]\n",
            "1097 [D loss: 0.999965] [G loss: 1.000040]\n",
            "1098 [D loss: 0.999941] [G loss: 1.000075]\n",
            "1099 [D loss: 0.999976] [G loss: 1.000069]\n",
            "1100 [D loss: 0.999937] [G loss: 1.000099]\n",
            "1101 [D loss: 0.999938] [G loss: 1.000062]\n",
            "1102 [D loss: 0.999997] [G loss: 1.000080]\n",
            "1103 [D loss: 0.999940] [G loss: 1.000090]\n",
            "1104 [D loss: 0.999988] [G loss: 1.000100]\n",
            "1105 [D loss: 0.999957] [G loss: 1.000080]\n",
            "1106 [D loss: 0.999933] [G loss: 1.000109]\n",
            "1107 [D loss: 0.999952] [G loss: 1.000053]\n",
            "1108 [D loss: 0.999958] [G loss: 1.000077]\n",
            "1109 [D loss: 0.999991] [G loss: 1.000044]\n",
            "1110 [D loss: 0.999954] [G loss: 1.000057]\n",
            "1111 [D loss: 0.999973] [G loss: 1.000081]\n",
            "1112 [D loss: 0.999966] [G loss: 1.000056]\n",
            "1113 [D loss: 0.999955] [G loss: 1.000043]\n",
            "1114 [D loss: 0.999983] [G loss: 1.000067]\n",
            "1115 [D loss: 0.999953] [G loss: 1.000095]\n",
            "1116 [D loss: 0.999950] [G loss: 1.000072]\n",
            "1117 [D loss: 0.999943] [G loss: 1.000083]\n",
            "1118 [D loss: 0.999964] [G loss: 1.000074]\n",
            "1119 [D loss: 0.999973] [G loss: 1.000072]\n",
            "1120 [D loss: 0.999939] [G loss: 1.000067]\n",
            "1121 [D loss: 0.999966] [G loss: 1.000063]\n",
            "1122 [D loss: 0.999968] [G loss: 1.000084]\n",
            "1123 [D loss: 0.999958] [G loss: 1.000059]\n",
            "1124 [D loss: 1.000000] [G loss: 1.000041]\n",
            "1125 [D loss: 0.999944] [G loss: 1.000064]\n",
            "1126 [D loss: 0.999968] [G loss: 1.000080]\n",
            "1127 [D loss: 0.999987] [G loss: 1.000082]\n",
            "1128 [D loss: 0.999953] [G loss: 1.000095]\n",
            "1129 [D loss: 0.999957] [G loss: 1.000033]\n",
            "1130 [D loss: 0.999947] [G loss: 1.000090]\n",
            "1131 [D loss: 0.999977] [G loss: 1.000087]\n",
            "1132 [D loss: 0.999990] [G loss: 1.000085]\n",
            "1133 [D loss: 0.999961] [G loss: 1.000058]\n",
            "1134 [D loss: 0.999950] [G loss: 1.000067]\n",
            "1135 [D loss: 0.999972] [G loss: 1.000041]\n",
            "1136 [D loss: 0.999944] [G loss: 1.000089]\n",
            "1137 [D loss: 0.999974] [G loss: 1.000078]\n",
            "1138 [D loss: 0.999944] [G loss: 1.000113]\n",
            "1139 [D loss: 0.999971] [G loss: 1.000097]\n",
            "1140 [D loss: 0.999980] [G loss: 1.000054]\n",
            "1141 [D loss: 0.999955] [G loss: 1.000082]\n",
            "1142 [D loss: 0.999963] [G loss: 1.000076]\n",
            "1143 [D loss: 0.999980] [G loss: 1.000088]\n",
            "1144 [D loss: 0.999963] [G loss: 1.000082]\n",
            "1145 [D loss: 0.999983] [G loss: 1.000094]\n",
            "1146 [D loss: 0.999943] [G loss: 1.000061]\n",
            "1147 [D loss: 0.999977] [G loss: 1.000054]\n",
            "1148 [D loss: 0.999996] [G loss: 1.000048]\n",
            "1149 [D loss: 0.999986] [G loss: 1.000074]\n",
            "1150 [D loss: 0.999966] [G loss: 1.000080]\n",
            "1151 [D loss: 0.999962] [G loss: 1.000056]\n",
            "1152 [D loss: 0.999959] [G loss: 1.000039]\n",
            "1153 [D loss: 0.999963] [G loss: 1.000092]\n",
            "1154 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1155 [D loss: 0.999979] [G loss: 1.000042]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1156 [D loss: 0.999957] [G loss: 1.000058]\n",
            "1157 [D loss: 0.999964] [G loss: 1.000074]\n",
            "1158 [D loss: 0.999990] [G loss: 1.000047]\n",
            "1159 [D loss: 0.999961] [G loss: 1.000099]\n",
            "1160 [D loss: 0.999979] [G loss: 1.000084]\n",
            "1161 [D loss: 0.999952] [G loss: 1.000057]\n",
            "1162 [D loss: 0.999957] [G loss: 1.000068]\n",
            "1163 [D loss: 0.999958] [G loss: 1.000057]\n",
            "1164 [D loss: 0.999969] [G loss: 1.000107]\n",
            "1165 [D loss: 0.999949] [G loss: 1.000039]\n",
            "1166 [D loss: 0.999974] [G loss: 1.000051]\n",
            "1167 [D loss: 0.999981] [G loss: 1.000089]\n",
            "1168 [D loss: 0.999990] [G loss: 1.000057]\n",
            "1169 [D loss: 0.999965] [G loss: 1.000083]\n",
            "1170 [D loss: 0.999974] [G loss: 1.000049]\n",
            "1171 [D loss: 0.999963] [G loss: 1.000092]\n",
            "1172 [D loss: 0.999940] [G loss: 1.000108]\n",
            "1173 [D loss: 0.999951] [G loss: 1.000082]\n",
            "1174 [D loss: 0.999963] [G loss: 1.000122]\n",
            "1175 [D loss: 1.000000] [G loss: 1.000094]\n",
            "1176 [D loss: 0.999984] [G loss: 1.000024]\n",
            "1177 [D loss: 0.999991] [G loss: 1.000075]\n",
            "1178 [D loss: 0.999982] [G loss: 1.000068]\n",
            "1179 [D loss: 0.999943] [G loss: 1.000052]\n",
            "1180 [D loss: 0.999982] [G loss: 1.000056]\n",
            "1181 [D loss: 0.999961] [G loss: 1.000057]\n",
            "1182 [D loss: 0.999991] [G loss: 1.000057]\n",
            "1183 [D loss: 0.999950] [G loss: 1.000066]\n",
            "1184 [D loss: 0.999985] [G loss: 1.000078]\n",
            "1185 [D loss: 0.999982] [G loss: 1.000053]\n",
            "1186 [D loss: 0.999985] [G loss: 1.000063]\n",
            "1187 [D loss: 0.999978] [G loss: 1.000045]\n",
            "1188 [D loss: 0.999959] [G loss: 1.000095]\n",
            "1189 [D loss: 0.999910] [G loss: 1.000081]\n",
            "1190 [D loss: 0.999949] [G loss: 1.000061]\n",
            "1191 [D loss: 0.999928] [G loss: 1.000090]\n",
            "1192 [D loss: 0.999951] [G loss: 1.000086]\n",
            "1193 [D loss: 0.999951] [G loss: 1.000075]\n",
            "1194 [D loss: 0.999984] [G loss: 1.000054]\n",
            "1195 [D loss: 0.999991] [G loss: 1.000061]\n",
            "1196 [D loss: 0.999963] [G loss: 1.000066]\n",
            "1197 [D loss: 0.999962] [G loss: 1.000047]\n",
            "1198 [D loss: 0.999993] [G loss: 1.000074]\n",
            "1199 [D loss: 0.999947] [G loss: 1.000103]\n",
            "1200 [D loss: 1.000013] [G loss: 1.000054]\n",
            "1201 [D loss: 0.999965] [G loss: 1.000046]\n",
            "1202 [D loss: 0.999974] [G loss: 1.000051]\n",
            "1203 [D loss: 0.999956] [G loss: 1.000040]\n",
            "1204 [D loss: 0.999939] [G loss: 1.000050]\n",
            "1205 [D loss: 0.999963] [G loss: 1.000032]\n",
            "1206 [D loss: 0.999977] [G loss: 1.000042]\n",
            "1207 [D loss: 0.999963] [G loss: 1.000064]\n",
            "1208 [D loss: 1.000004] [G loss: 1.000080]\n",
            "1209 [D loss: 0.999998] [G loss: 1.000088]\n",
            "1210 [D loss: 0.999958] [G loss: 1.000053]\n",
            "1211 [D loss: 0.999922] [G loss: 1.000066]\n",
            "1212 [D loss: 0.999989] [G loss: 1.000072]\n",
            "1213 [D loss: 1.000001] [G loss: 1.000026]\n",
            "1214 [D loss: 0.999992] [G loss: 1.000094]\n",
            "1215 [D loss: 0.999973] [G loss: 1.000069]\n",
            "1216 [D loss: 0.999953] [G loss: 1.000040]\n",
            "1217 [D loss: 0.999994] [G loss: 1.000074]\n",
            "1218 [D loss: 0.999963] [G loss: 1.000077]\n",
            "1219 [D loss: 0.999967] [G loss: 1.000070]\n",
            "1220 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1221 [D loss: 0.999958] [G loss: 1.000091]\n",
            "1222 [D loss: 0.999992] [G loss: 1.000068]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1223 [D loss: 0.999974] [G loss: 1.000078]\n",
            "1224 [D loss: 0.999946] [G loss: 1.000099]\n",
            "1225 [D loss: 0.999997] [G loss: 1.000060]\n",
            "1226 [D loss: 0.999954] [G loss: 1.000103]\n",
            "1227 [D loss: 0.999972] [G loss: 1.000087]\n",
            "1228 [D loss: 0.999950] [G loss: 1.000057]\n",
            "1229 [D loss: 0.999972] [G loss: 1.000108]\n",
            "1230 [D loss: 0.999963] [G loss: 1.000089]\n",
            "1231 [D loss: 0.999950] [G loss: 1.000079]\n",
            "1232 [D loss: 0.999974] [G loss: 1.000052]\n",
            "1233 [D loss: 0.999972] [G loss: 1.000091]\n",
            "1234 [D loss: 0.999981] [G loss: 1.000084]\n",
            "1235 [D loss: 0.999930] [G loss: 1.000082]\n",
            "1236 [D loss: 0.999967] [G loss: 1.000051]\n",
            "1237 [D loss: 0.999959] [G loss: 1.000100]\n",
            "1238 [D loss: 0.999944] [G loss: 1.000039]\n",
            "1239 [D loss: 0.999963] [G loss: 1.000056]\n",
            "1240 [D loss: 0.999997] [G loss: 1.000079]\n",
            "1241 [D loss: 0.999973] [G loss: 1.000076]\n",
            "1242 [D loss: 0.999970] [G loss: 1.000088]\n",
            "1243 [D loss: 0.999937] [G loss: 1.000071]\n",
            "1244 [D loss: 0.999976] [G loss: 1.000046]\n",
            "1245 [D loss: 0.999968] [G loss: 1.000052]\n",
            "1246 [D loss: 0.999988] [G loss: 1.000066]\n",
            "1247 [D loss: 0.999969] [G loss: 1.000101]\n",
            "1248 [D loss: 0.999982] [G loss: 1.000035]\n",
            "1249 [D loss: 0.999981] [G loss: 1.000102]\n",
            "1250 [D loss: 0.999967] [G loss: 1.000097]\n",
            "1251 [D loss: 0.999917] [G loss: 1.000080]\n",
            "1252 [D loss: 0.999939] [G loss: 1.000055]\n",
            "1253 [D loss: 0.999946] [G loss: 1.000089]\n",
            "1254 [D loss: 0.999956] [G loss: 1.000071]\n",
            "1255 [D loss: 0.999974] [G loss: 1.000071]\n",
            "1256 [D loss: 0.999990] [G loss: 1.000027]\n",
            "1257 [D loss: 0.999963] [G loss: 1.000071]\n",
            "1258 [D loss: 0.999981] [G loss: 1.000083]\n",
            "1259 [D loss: 0.999970] [G loss: 1.000078]\n",
            "1260 [D loss: 0.999942] [G loss: 1.000051]\n",
            "1261 [D loss: 0.999965] [G loss: 1.000071]\n",
            "1262 [D loss: 0.999960] [G loss: 1.000038]\n",
            "1263 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1264 [D loss: 0.999971] [G loss: 1.000072]\n",
            "1265 [D loss: 0.999990] [G loss: 1.000060]\n",
            "1266 [D loss: 0.999960] [G loss: 1.000088]\n",
            "1267 [D loss: 0.999979] [G loss: 1.000052]\n",
            "1268 [D loss: 0.999942] [G loss: 1.000039]\n",
            "1269 [D loss: 0.999952] [G loss: 1.000084]\n",
            "1270 [D loss: 0.999967] [G loss: 1.000070]\n",
            "1271 [D loss: 0.999997] [G loss: 1.000067]\n",
            "1272 [D loss: 0.999974] [G loss: 1.000024]\n",
            "1273 [D loss: 0.999971] [G loss: 1.000076]\n",
            "1274 [D loss: 1.000012] [G loss: 1.000055]\n",
            "1275 [D loss: 0.999942] [G loss: 1.000063]\n",
            "1276 [D loss: 0.999981] [G loss: 1.000074]\n",
            "1277 [D loss: 0.999969] [G loss: 1.000050]\n",
            "1278 [D loss: 0.999994] [G loss: 1.000079]\n",
            "1279 [D loss: 0.999971] [G loss: 1.000044]\n",
            "1280 [D loss: 0.999985] [G loss: 1.000054]\n",
            "1281 [D loss: 0.999983] [G loss: 1.000064]\n",
            "1282 [D loss: 0.999975] [G loss: 1.000040]\n",
            "1283 [D loss: 0.999965] [G loss: 1.000078]\n",
            "1284 [D loss: 0.999951] [G loss: 1.000046]\n",
            "1285 [D loss: 0.999938] [G loss: 1.000078]\n",
            "1286 [D loss: 0.999977] [G loss: 1.000073]\n",
            "1287 [D loss: 0.999996] [G loss: 1.000082]\n",
            "1288 [D loss: 0.999964] [G loss: 1.000073]\n",
            "1289 [D loss: 0.999995] [G loss: 1.000056]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1290 [D loss: 0.999965] [G loss: 1.000042]\n",
            "1291 [D loss: 0.999972] [G loss: 1.000096]\n",
            "1292 [D loss: 0.999987] [G loss: 1.000096]\n",
            "1293 [D loss: 0.999937] [G loss: 1.000065]\n",
            "1294 [D loss: 0.999961] [G loss: 1.000047]\n",
            "1295 [D loss: 0.999964] [G loss: 1.000080]\n",
            "1296 [D loss: 0.999952] [G loss: 1.000068]\n",
            "1297 [D loss: 1.000017] [G loss: 1.000050]\n",
            "1298 [D loss: 0.999946] [G loss: 1.000079]\n",
            "1299 [D loss: 0.999939] [G loss: 1.000041]\n",
            "1300 [D loss: 0.999980] [G loss: 1.000065]\n",
            "1301 [D loss: 0.999952] [G loss: 1.000047]\n",
            "1302 [D loss: 0.999957] [G loss: 1.000087]\n",
            "1303 [D loss: 0.999951] [G loss: 1.000081]\n",
            "1304 [D loss: 0.999953] [G loss: 1.000066]\n",
            "1305 [D loss: 0.999973] [G loss: 1.000068]\n",
            "1306 [D loss: 0.999937] [G loss: 1.000065]\n",
            "1307 [D loss: 0.999995] [G loss: 1.000046]\n",
            "1308 [D loss: 0.999974] [G loss: 1.000071]\n",
            "1309 [D loss: 0.999953] [G loss: 1.000061]\n",
            "1310 [D loss: 1.000008] [G loss: 1.000107]\n",
            "1311 [D loss: 0.999975] [G loss: 1.000077]\n",
            "1312 [D loss: 0.999973] [G loss: 1.000041]\n",
            "1313 [D loss: 0.999991] [G loss: 1.000084]\n",
            "1314 [D loss: 0.999954] [G loss: 1.000049]\n",
            "1315 [D loss: 0.999992] [G loss: 1.000076]\n",
            "1316 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1317 [D loss: 0.999981] [G loss: 1.000034]\n",
            "1318 [D loss: 0.999950] [G loss: 1.000074]\n",
            "1319 [D loss: 0.999975] [G loss: 1.000047]\n",
            "1320 [D loss: 0.999972] [G loss: 1.000037]\n",
            "1321 [D loss: 0.999993] [G loss: 1.000054]\n",
            "1322 [D loss: 0.999974] [G loss: 1.000044]\n",
            "1323 [D loss: 0.999970] [G loss: 1.000050]\n",
            "1324 [D loss: 0.999971] [G loss: 1.000076]\n",
            "1325 [D loss: 1.000013] [G loss: 1.000080]\n",
            "1326 [D loss: 0.999931] [G loss: 1.000060]\n",
            "1327 [D loss: 0.999963] [G loss: 1.000081]\n",
            "1328 [D loss: 0.999932] [G loss: 1.000066]\n",
            "1329 [D loss: 0.999979] [G loss: 1.000063]\n",
            "1330 [D loss: 0.999953] [G loss: 1.000058]\n",
            "1331 [D loss: 0.999927] [G loss: 1.000071]\n",
            "1332 [D loss: 0.999986] [G loss: 1.000079]\n",
            "1333 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1334 [D loss: 0.999961] [G loss: 1.000060]\n",
            "1335 [D loss: 0.999950] [G loss: 1.000098]\n",
            "1336 [D loss: 0.999971] [G loss: 1.000074]\n",
            "1337 [D loss: 0.999984] [G loss: 1.000045]\n",
            "1338 [D loss: 0.999944] [G loss: 1.000089]\n",
            "1339 [D loss: 0.999956] [G loss: 1.000068]\n",
            "1340 [D loss: 0.999982] [G loss: 1.000088]\n",
            "1341 [D loss: 0.999953] [G loss: 1.000101]\n",
            "1342 [D loss: 0.999994] [G loss: 1.000070]\n",
            "1343 [D loss: 0.999945] [G loss: 1.000124]\n",
            "1344 [D loss: 0.999988] [G loss: 1.000020]\n",
            "1345 [D loss: 0.999983] [G loss: 1.000031]\n",
            "1346 [D loss: 0.999971] [G loss: 1.000127]\n",
            "1347 [D loss: 0.999984] [G loss: 1.000075]\n",
            "1348 [D loss: 0.999961] [G loss: 1.000079]\n",
            "1349 [D loss: 0.999993] [G loss: 1.000079]\n",
            "1350 [D loss: 0.999985] [G loss: 1.000087]\n",
            "1351 [D loss: 0.999973] [G loss: 1.000107]\n",
            "1352 [D loss: 0.999927] [G loss: 1.000083]\n",
            "1353 [D loss: 0.999960] [G loss: 1.000093]\n",
            "1354 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1355 [D loss: 0.999911] [G loss: 1.000027]\n",
            "1356 [D loss: 0.999974] [G loss: 1.000099]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1357 [D loss: 0.999948] [G loss: 1.000064]\n",
            "1358 [D loss: 0.999991] [G loss: 1.000092]\n",
            "1359 [D loss: 0.999987] [G loss: 1.000059]\n",
            "1360 [D loss: 0.999987] [G loss: 1.000084]\n",
            "1361 [D loss: 0.999935] [G loss: 1.000067]\n",
            "1362 [D loss: 0.999988] [G loss: 1.000040]\n",
            "1363 [D loss: 0.999954] [G loss: 1.000075]\n",
            "1364 [D loss: 0.999986] [G loss: 1.000085]\n",
            "1365 [D loss: 0.999976] [G loss: 1.000084]\n",
            "1366 [D loss: 0.999975] [G loss: 1.000068]\n",
            "1367 [D loss: 0.999951] [G loss: 1.000058]\n",
            "1368 [D loss: 0.999963] [G loss: 1.000060]\n",
            "1369 [D loss: 0.999981] [G loss: 1.000060]\n",
            "1370 [D loss: 0.999956] [G loss: 1.000088]\n",
            "1371 [D loss: 0.999953] [G loss: 1.000096]\n",
            "1372 [D loss: 0.999967] [G loss: 1.000068]\n",
            "1373 [D loss: 0.999964] [G loss: 1.000071]\n",
            "1374 [D loss: 0.999983] [G loss: 1.000086]\n",
            "1375 [D loss: 0.999972] [G loss: 1.000077]\n",
            "1376 [D loss: 0.999986] [G loss: 1.000061]\n",
            "1377 [D loss: 0.999932] [G loss: 1.000087]\n",
            "1378 [D loss: 0.999957] [G loss: 1.000095]\n",
            "1379 [D loss: 0.999984] [G loss: 1.000090]\n",
            "1380 [D loss: 0.999949] [G loss: 1.000044]\n",
            "1381 [D loss: 0.999958] [G loss: 1.000069]\n",
            "1382 [D loss: 0.999944] [G loss: 1.000057]\n",
            "1383 [D loss: 0.999949] [G loss: 1.000059]\n",
            "1384 [D loss: 0.999946] [G loss: 1.000079]\n",
            "1385 [D loss: 0.999980] [G loss: 1.000049]\n",
            "1386 [D loss: 0.999934] [G loss: 1.000067]\n",
            "1387 [D loss: 0.999971] [G loss: 1.000032]\n",
            "1388 [D loss: 0.999960] [G loss: 1.000074]\n",
            "1389 [D loss: 0.999981] [G loss: 1.000100]\n",
            "1390 [D loss: 0.999989] [G loss: 1.000048]\n",
            "1391 [D loss: 0.999970] [G loss: 1.000086]\n",
            "1392 [D loss: 0.999935] [G loss: 1.000035]\n",
            "1393 [D loss: 0.999954] [G loss: 1.000068]\n",
            "1394 [D loss: 0.999952] [G loss: 1.000054]\n",
            "1395 [D loss: 0.999950] [G loss: 1.000062]\n",
            "1396 [D loss: 0.999977] [G loss: 1.000033]\n",
            "1397 [D loss: 0.999986] [G loss: 1.000087]\n",
            "1398 [D loss: 0.999986] [G loss: 1.000040]\n",
            "1399 [D loss: 0.999987] [G loss: 1.000081]\n",
            "1400 [D loss: 0.999925] [G loss: 1.000057]\n",
            "1401 [D loss: 0.999960] [G loss: 1.000046]\n",
            "1402 [D loss: 0.999995] [G loss: 1.000081]\n",
            "1403 [D loss: 0.999972] [G loss: 1.000011]\n",
            "1404 [D loss: 0.999931] [G loss: 1.000061]\n",
            "1405 [D loss: 0.999975] [G loss: 1.000058]\n",
            "1406 [D loss: 0.999993] [G loss: 1.000095]\n",
            "1407 [D loss: 0.999977] [G loss: 1.000081]\n",
            "1408 [D loss: 0.999959] [G loss: 1.000074]\n",
            "1409 [D loss: 0.999982] [G loss: 1.000039]\n",
            "1410 [D loss: 0.999966] [G loss: 1.000058]\n",
            "1411 [D loss: 0.999962] [G loss: 1.000079]\n",
            "1412 [D loss: 0.999991] [G loss: 1.000030]\n",
            "1413 [D loss: 0.999959] [G loss: 1.000054]\n",
            "1414 [D loss: 0.999943] [G loss: 1.000104]\n",
            "1415 [D loss: 0.999995] [G loss: 1.000089]\n",
            "1416 [D loss: 0.999965] [G loss: 1.000069]\n",
            "1417 [D loss: 0.999997] [G loss: 1.000072]\n",
            "1418 [D loss: 1.000008] [G loss: 1.000100]\n",
            "1419 [D loss: 0.999977] [G loss: 1.000074]\n",
            "1420 [D loss: 0.999970] [G loss: 1.000083]\n",
            "1421 [D loss: 0.999929] [G loss: 1.000042]\n",
            "1422 [D loss: 0.999964] [G loss: 1.000067]\n",
            "1423 [D loss: 0.999958] [G loss: 1.000099]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1424 [D loss: 0.999979] [G loss: 1.000083]\n",
            "1425 [D loss: 0.999955] [G loss: 1.000109]\n",
            "1426 [D loss: 0.999963] [G loss: 1.000077]\n",
            "1427 [D loss: 0.999912] [G loss: 1.000067]\n",
            "1428 [D loss: 0.999988] [G loss: 1.000080]\n",
            "1429 [D loss: 0.999985] [G loss: 1.000094]\n",
            "1430 [D loss: 0.999975] [G loss: 1.000092]\n",
            "1431 [D loss: 0.999977] [G loss: 1.000046]\n",
            "1432 [D loss: 0.999946] [G loss: 1.000064]\n",
            "1433 [D loss: 0.999930] [G loss: 1.000077]\n",
            "1434 [D loss: 0.999936] [G loss: 1.000075]\n",
            "1435 [D loss: 0.999952] [G loss: 1.000060]\n",
            "1436 [D loss: 0.999984] [G loss: 1.000087]\n",
            "1437 [D loss: 0.999927] [G loss: 1.000077]\n",
            "1438 [D loss: 0.999943] [G loss: 1.000068]\n",
            "1439 [D loss: 0.999966] [G loss: 1.000087]\n",
            "1440 [D loss: 0.999953] [G loss: 1.000046]\n",
            "1441 [D loss: 0.999988] [G loss: 1.000127]\n",
            "1442 [D loss: 0.999966] [G loss: 1.000090]\n",
            "1443 [D loss: 0.999963] [G loss: 1.000086]\n",
            "1444 [D loss: 0.999947] [G loss: 1.000083]\n",
            "1445 [D loss: 0.999914] [G loss: 1.000101]\n",
            "1446 [D loss: 0.999956] [G loss: 1.000088]\n",
            "1447 [D loss: 0.999970] [G loss: 1.000052]\n",
            "1448 [D loss: 0.999957] [G loss: 1.000081]\n",
            "1449 [D loss: 0.999978] [G loss: 1.000083]\n",
            "1450 [D loss: 0.999945] [G loss: 1.000052]\n",
            "1451 [D loss: 0.999963] [G loss: 1.000078]\n",
            "1452 [D loss: 0.999967] [G loss: 1.000040]\n",
            "1453 [D loss: 0.999997] [G loss: 1.000090]\n",
            "1454 [D loss: 0.999939] [G loss: 1.000079]\n",
            "1455 [D loss: 0.999950] [G loss: 1.000053]\n",
            "1456 [D loss: 0.999954] [G loss: 1.000082]\n",
            "1457 [D loss: 0.999978] [G loss: 1.000063]\n",
            "1458 [D loss: 0.999993] [G loss: 1.000057]\n",
            "1459 [D loss: 0.999975] [G loss: 1.000062]\n",
            "1460 [D loss: 0.999999] [G loss: 1.000045]\n",
            "1461 [D loss: 0.999952] [G loss: 1.000112]\n",
            "1462 [D loss: 1.000000] [G loss: 1.000095]\n",
            "1463 [D loss: 0.999963] [G loss: 1.000105]\n",
            "1464 [D loss: 0.999956] [G loss: 1.000104]\n",
            "1465 [D loss: 0.999949] [G loss: 1.000077]\n",
            "1466 [D loss: 0.999944] [G loss: 1.000058]\n",
            "1467 [D loss: 0.999950] [G loss: 1.000075]\n",
            "1468 [D loss: 0.999946] [G loss: 1.000108]\n",
            "1469 [D loss: 0.999927] [G loss: 1.000076]\n",
            "1470 [D loss: 0.999934] [G loss: 1.000074]\n",
            "1471 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1472 [D loss: 0.999966] [G loss: 1.000054]\n",
            "1473 [D loss: 0.999951] [G loss: 1.000071]\n",
            "1474 [D loss: 0.999991] [G loss: 1.000074]\n",
            "1475 [D loss: 0.999991] [G loss: 1.000096]\n",
            "1476 [D loss: 0.999962] [G loss: 1.000078]\n",
            "1477 [D loss: 1.000003] [G loss: 1.000088]\n",
            "1478 [D loss: 1.000009] [G loss: 1.000103]\n",
            "1479 [D loss: 0.999957] [G loss: 1.000066]\n",
            "1480 [D loss: 0.999981] [G loss: 1.000084]\n",
            "1481 [D loss: 0.999964] [G loss: 1.000066]\n",
            "1482 [D loss: 1.000004] [G loss: 1.000075]\n",
            "1483 [D loss: 0.999912] [G loss: 1.000070]\n",
            "1484 [D loss: 0.999942] [G loss: 1.000075]\n",
            "1485 [D loss: 0.999977] [G loss: 1.000053]\n",
            "1486 [D loss: 0.999934] [G loss: 1.000052]\n",
            "1487 [D loss: 0.999966] [G loss: 1.000072]\n",
            "1488 [D loss: 0.999936] [G loss: 1.000076]\n",
            "1489 [D loss: 0.999973] [G loss: 1.000066]\n",
            "1490 [D loss: 0.999976] [G loss: 1.000078]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1491 [D loss: 0.999966] [G loss: 1.000063]\n",
            "1492 [D loss: 0.999987] [G loss: 1.000081]\n",
            "1493 [D loss: 0.999954] [G loss: 1.000089]\n",
            "1494 [D loss: 0.999959] [G loss: 1.000057]\n",
            "1495 [D loss: 0.999963] [G loss: 1.000047]\n",
            "1496 [D loss: 0.999973] [G loss: 1.000082]\n",
            "1497 [D loss: 0.999956] [G loss: 1.000061]\n",
            "1498 [D loss: 0.999991] [G loss: 1.000101]\n",
            "1499 [D loss: 0.999956] [G loss: 1.000084]\n",
            "1500 [D loss: 0.999983] [G loss: 1.000094]\n",
            "1501 [D loss: 0.999974] [G loss: 1.000067]\n",
            "1502 [D loss: 0.999960] [G loss: 1.000076]\n",
            "1503 [D loss: 0.999936] [G loss: 1.000048]\n",
            "1504 [D loss: 0.999981] [G loss: 1.000086]\n",
            "1505 [D loss: 0.999963] [G loss: 1.000052]\n",
            "1506 [D loss: 1.000001] [G loss: 1.000075]\n",
            "1507 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1508 [D loss: 0.999962] [G loss: 1.000073]\n",
            "1509 [D loss: 0.999953] [G loss: 1.000070]\n",
            "1510 [D loss: 0.999985] [G loss: 1.000060]\n",
            "1511 [D loss: 0.999977] [G loss: 1.000061]\n",
            "1512 [D loss: 0.999970] [G loss: 1.000042]\n",
            "1513 [D loss: 0.999985] [G loss: 1.000064]\n",
            "1514 [D loss: 0.999946] [G loss: 1.000061]\n",
            "1515 [D loss: 0.999942] [G loss: 1.000066]\n",
            "1516 [D loss: 0.999964] [G loss: 1.000064]\n",
            "1517 [D loss: 0.999956] [G loss: 1.000066]\n",
            "1518 [D loss: 0.999991] [G loss: 1.000055]\n",
            "1519 [D loss: 0.999989] [G loss: 1.000059]\n",
            "1520 [D loss: 1.000006] [G loss: 1.000059]\n",
            "1521 [D loss: 0.999982] [G loss: 1.000046]\n",
            "1522 [D loss: 0.999967] [G loss: 1.000120]\n",
            "1523 [D loss: 0.999928] [G loss: 1.000090]\n",
            "1524 [D loss: 0.999917] [G loss: 1.000073]\n",
            "1525 [D loss: 0.999978] [G loss: 1.000074]\n",
            "1526 [D loss: 0.999937] [G loss: 1.000078]\n",
            "1527 [D loss: 0.999975] [G loss: 1.000094]\n",
            "1528 [D loss: 0.999931] [G loss: 1.000071]\n",
            "1529 [D loss: 0.999953] [G loss: 1.000099]\n",
            "1530 [D loss: 0.999989] [G loss: 1.000075]\n",
            "1531 [D loss: 0.999978] [G loss: 1.000076]\n",
            "1532 [D loss: 0.999980] [G loss: 1.000058]\n",
            "1533 [D loss: 0.999949] [G loss: 1.000096]\n",
            "1534 [D loss: 0.999952] [G loss: 1.000056]\n",
            "1535 [D loss: 0.999959] [G loss: 1.000084]\n",
            "1536 [D loss: 0.999960] [G loss: 1.000083]\n",
            "1537 [D loss: 0.999990] [G loss: 1.000074]\n",
            "1538 [D loss: 0.999976] [G loss: 1.000064]\n",
            "1539 [D loss: 0.999994] [G loss: 1.000078]\n",
            "1540 [D loss: 0.999942] [G loss: 1.000100]\n",
            "1541 [D loss: 1.000013] [G loss: 1.000035]\n",
            "1542 [D loss: 0.999967] [G loss: 1.000073]\n",
            "1543 [D loss: 0.999944] [G loss: 1.000119]\n",
            "1544 [D loss: 0.999980] [G loss: 1.000012]\n",
            "1545 [D loss: 1.000003] [G loss: 1.000087]\n",
            "1546 [D loss: 0.999961] [G loss: 1.000094]\n",
            "1547 [D loss: 0.999996] [G loss: 1.000050]\n",
            "1548 [D loss: 0.999973] [G loss: 1.000090]\n",
            "1549 [D loss: 0.999981] [G loss: 1.000071]\n",
            "1550 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1551 [D loss: 0.999960] [G loss: 1.000062]\n",
            "1552 [D loss: 0.999957] [G loss: 1.000090]\n",
            "1553 [D loss: 0.999954] [G loss: 1.000043]\n",
            "1554 [D loss: 0.999974] [G loss: 1.000063]\n",
            "1555 [D loss: 0.999980] [G loss: 1.000060]\n",
            "1556 [D loss: 0.999951] [G loss: 1.000075]\n",
            "1557 [D loss: 0.999941] [G loss: 1.000069]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1558 [D loss: 0.999901] [G loss: 1.000044]\n",
            "1559 [D loss: 0.999933] [G loss: 1.000052]\n",
            "1560 [D loss: 0.999986] [G loss: 1.000051]\n",
            "1561 [D loss: 0.999975] [G loss: 1.000116]\n",
            "1562 [D loss: 0.999967] [G loss: 1.000102]\n",
            "1563 [D loss: 0.999977] [G loss: 1.000081]\n",
            "1564 [D loss: 0.999952] [G loss: 1.000091]\n",
            "1565 [D loss: 0.999963] [G loss: 1.000049]\n",
            "1566 [D loss: 0.999926] [G loss: 1.000076]\n",
            "1567 [D loss: 0.999999] [G loss: 1.000065]\n",
            "1568 [D loss: 0.999947] [G loss: 1.000041]\n",
            "1569 [D loss: 0.999967] [G loss: 1.000026]\n",
            "1570 [D loss: 0.999964] [G loss: 1.000058]\n",
            "1571 [D loss: 0.999964] [G loss: 1.000067]\n",
            "1572 [D loss: 0.999947] [G loss: 1.000078]\n",
            "1573 [D loss: 0.999965] [G loss: 1.000057]\n",
            "1574 [D loss: 0.999979] [G loss: 1.000081]\n",
            "1575 [D loss: 0.999949] [G loss: 1.000067]\n",
            "1576 [D loss: 0.999992] [G loss: 1.000068]\n",
            "1577 [D loss: 0.999983] [G loss: 1.000088]\n",
            "1578 [D loss: 0.999955] [G loss: 1.000067]\n",
            "1579 [D loss: 0.999934] [G loss: 1.000080]\n",
            "1580 [D loss: 0.999999] [G loss: 1.000032]\n",
            "1581 [D loss: 0.999943] [G loss: 1.000081]\n",
            "1582 [D loss: 0.999953] [G loss: 1.000081]\n",
            "1583 [D loss: 0.999966] [G loss: 1.000101]\n",
            "1584 [D loss: 0.999971] [G loss: 1.000073]\n",
            "1585 [D loss: 0.999976] [G loss: 1.000048]\n",
            "1586 [D loss: 0.999947] [G loss: 1.000067]\n",
            "1587 [D loss: 0.999978] [G loss: 1.000096]\n",
            "1588 [D loss: 0.999962] [G loss: 1.000067]\n",
            "1589 [D loss: 0.999961] [G loss: 1.000069]\n",
            "1590 [D loss: 1.000000] [G loss: 1.000048]\n",
            "1591 [D loss: 0.999982] [G loss: 1.000101]\n",
            "1592 [D loss: 0.999952] [G loss: 1.000089]\n",
            "1593 [D loss: 0.999974] [G loss: 1.000077]\n",
            "1594 [D loss: 0.999942] [G loss: 1.000085]\n",
            "1595 [D loss: 0.999943] [G loss: 1.000073]\n",
            "1596 [D loss: 0.999967] [G loss: 1.000089]\n",
            "1597 [D loss: 0.999961] [G loss: 1.000099]\n",
            "1598 [D loss: 0.999958] [G loss: 1.000088]\n",
            "1599 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1600 [D loss: 0.999975] [G loss: 1.000067]\n",
            "1601 [D loss: 0.999949] [G loss: 1.000062]\n",
            "1602 [D loss: 0.999969] [G loss: 1.000112]\n",
            "1603 [D loss: 0.999980] [G loss: 1.000120]\n",
            "1604 [D loss: 0.999952] [G loss: 1.000074]\n",
            "1605 [D loss: 0.999964] [G loss: 1.000086]\n",
            "1606 [D loss: 0.999928] [G loss: 1.000047]\n",
            "1607 [D loss: 1.000007] [G loss: 1.000065]\n",
            "1608 [D loss: 0.999986] [G loss: 1.000009]\n",
            "1609 [D loss: 0.999946] [G loss: 1.000067]\n",
            "1610 [D loss: 1.000012] [G loss: 1.000089]\n",
            "1611 [D loss: 0.999970] [G loss: 1.000073]\n",
            "1612 [D loss: 0.999911] [G loss: 1.000079]\n",
            "1613 [D loss: 0.999960] [G loss: 1.000060]\n",
            "1614 [D loss: 0.999939] [G loss: 1.000065]\n",
            "1615 [D loss: 1.000009] [G loss: 1.000088]\n",
            "1616 [D loss: 0.999938] [G loss: 1.000081]\n",
            "1617 [D loss: 0.999955] [G loss: 1.000050]\n",
            "1618 [D loss: 1.000000] [G loss: 1.000078]\n",
            "1619 [D loss: 0.999933] [G loss: 1.000065]\n",
            "1620 [D loss: 0.999975] [G loss: 1.000091]\n",
            "1621 [D loss: 0.999947] [G loss: 1.000068]\n",
            "1622 [D loss: 0.999973] [G loss: 1.000068]\n",
            "1623 [D loss: 0.999941] [G loss: 1.000040]\n",
            "1624 [D loss: 0.999945] [G loss: 1.000082]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1625 [D loss: 0.999955] [G loss: 1.000070]\n",
            "1626 [D loss: 0.999979] [G loss: 1.000078]\n",
            "1627 [D loss: 0.999950] [G loss: 1.000063]\n",
            "1628 [D loss: 0.999979] [G loss: 1.000064]\n",
            "1629 [D loss: 0.999988] [G loss: 1.000063]\n",
            "1630 [D loss: 0.999984] [G loss: 1.000074]\n",
            "1631 [D loss: 0.999951] [G loss: 1.000095]\n",
            "1632 [D loss: 0.999977] [G loss: 1.000094]\n",
            "1633 [D loss: 0.999983] [G loss: 1.000064]\n",
            "1634 [D loss: 0.999946] [G loss: 1.000086]\n",
            "1635 [D loss: 0.999971] [G loss: 1.000094]\n",
            "1636 [D loss: 0.999987] [G loss: 1.000052]\n",
            "1637 [D loss: 1.000004] [G loss: 1.000018]\n",
            "1638 [D loss: 0.999943] [G loss: 1.000067]\n",
            "1639 [D loss: 0.999997] [G loss: 1.000028]\n",
            "1640 [D loss: 0.999960] [G loss: 1.000047]\n",
            "1641 [D loss: 0.999956] [G loss: 1.000085]\n",
            "1642 [D loss: 0.999953] [G loss: 1.000079]\n",
            "1643 [D loss: 0.999967] [G loss: 1.000084]\n",
            "1644 [D loss: 0.999956] [G loss: 1.000057]\n",
            "1645 [D loss: 0.999957] [G loss: 1.000102]\n",
            "1646 [D loss: 0.999946] [G loss: 1.000054]\n",
            "1647 [D loss: 0.999975] [G loss: 1.000087]\n",
            "1648 [D loss: 0.999992] [G loss: 1.000068]\n",
            "1649 [D loss: 0.999990] [G loss: 1.000056]\n",
            "1650 [D loss: 0.999925] [G loss: 1.000046]\n",
            "1651 [D loss: 0.999980] [G loss: 1.000101]\n",
            "1652 [D loss: 0.999960] [G loss: 1.000100]\n",
            "1653 [D loss: 0.999963] [G loss: 1.000073]\n",
            "1654 [D loss: 0.999967] [G loss: 1.000030]\n",
            "1655 [D loss: 0.999971] [G loss: 1.000072]\n",
            "1656 [D loss: 0.999979] [G loss: 1.000084]\n",
            "1657 [D loss: 1.000000] [G loss: 1.000089]\n",
            "1658 [D loss: 0.999994] [G loss: 1.000020]\n",
            "1659 [D loss: 0.999969] [G loss: 1.000092]\n",
            "1660 [D loss: 0.999948] [G loss: 1.000086]\n",
            "1661 [D loss: 0.999943] [G loss: 1.000089]\n",
            "1662 [D loss: 0.999974] [G loss: 1.000069]\n",
            "1663 [D loss: 0.999965] [G loss: 1.000070]\n",
            "1664 [D loss: 0.999970] [G loss: 1.000083]\n",
            "1665 [D loss: 0.999960] [G loss: 1.000049]\n",
            "1666 [D loss: 1.000002] [G loss: 1.000037]\n",
            "1667 [D loss: 0.999945] [G loss: 1.000051]\n",
            "1668 [D loss: 0.999983] [G loss: 1.000078]\n",
            "1669 [D loss: 0.999952] [G loss: 1.000075]\n",
            "1670 [D loss: 0.999939] [G loss: 1.000059]\n",
            "1671 [D loss: 0.999961] [G loss: 1.000090]\n",
            "1672 [D loss: 0.999976] [G loss: 1.000046]\n",
            "1673 [D loss: 0.999991] [G loss: 1.000054]\n",
            "1674 [D loss: 0.999962] [G loss: 1.000097]\n",
            "1675 [D loss: 0.999996] [G loss: 1.000040]\n",
            "1676 [D loss: 0.999964] [G loss: 1.000025]\n",
            "1677 [D loss: 0.999937] [G loss: 1.000084]\n",
            "1678 [D loss: 0.999970] [G loss: 1.000051]\n",
            "1679 [D loss: 0.999941] [G loss: 1.000118]\n",
            "1680 [D loss: 0.999967] [G loss: 1.000094]\n",
            "1681 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1682 [D loss: 0.999946] [G loss: 1.000073]\n",
            "1683 [D loss: 0.999978] [G loss: 1.000078]\n",
            "1684 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1685 [D loss: 0.999959] [G loss: 1.000110]\n",
            "1686 [D loss: 0.999943] [G loss: 1.000074]\n",
            "1687 [D loss: 0.999945] [G loss: 1.000057]\n",
            "1688 [D loss: 0.999987] [G loss: 1.000101]\n",
            "1689 [D loss: 0.999960] [G loss: 1.000066]\n",
            "1690 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1691 [D loss: 0.999949] [G loss: 1.000069]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1692 [D loss: 0.999940] [G loss: 1.000057]\n",
            "1693 [D loss: 0.999947] [G loss: 1.000056]\n",
            "1694 [D loss: 0.999985] [G loss: 1.000059]\n",
            "1695 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1696 [D loss: 0.999998] [G loss: 1.000100]\n",
            "1697 [D loss: 0.999930] [G loss: 1.000067]\n",
            "1698 [D loss: 0.999980] [G loss: 1.000065]\n",
            "1699 [D loss: 0.999974] [G loss: 1.000041]\n",
            "1700 [D loss: 0.999999] [G loss: 1.000063]\n",
            "1701 [D loss: 0.999960] [G loss: 1.000095]\n",
            "1702 [D loss: 1.000009] [G loss: 1.000094]\n",
            "1703 [D loss: 0.999978] [G loss: 1.000060]\n",
            "1704 [D loss: 0.999962] [G loss: 1.000026]\n",
            "1705 [D loss: 0.999963] [G loss: 1.000074]\n",
            "1706 [D loss: 0.999955] [G loss: 1.000068]\n",
            "1707 [D loss: 0.999979] [G loss: 1.000096]\n",
            "1708 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1709 [D loss: 0.999987] [G loss: 1.000092]\n",
            "1710 [D loss: 0.999984] [G loss: 1.000074]\n",
            "1711 [D loss: 0.999969] [G loss: 1.000087]\n",
            "1712 [D loss: 1.000004] [G loss: 1.000096]\n",
            "1713 [D loss: 0.999974] [G loss: 1.000053]\n",
            "1714 [D loss: 0.999964] [G loss: 1.000043]\n",
            "1715 [D loss: 0.999972] [G loss: 1.000050]\n",
            "1716 [D loss: 0.999963] [G loss: 1.000094]\n",
            "1717 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1718 [D loss: 0.999964] [G loss: 1.000061]\n",
            "1719 [D loss: 0.999979] [G loss: 1.000068]\n",
            "1720 [D loss: 0.999975] [G loss: 1.000056]\n",
            "1721 [D loss: 0.999970] [G loss: 1.000099]\n",
            "1722 [D loss: 0.999957] [G loss: 1.000089]\n",
            "1723 [D loss: 0.999962] [G loss: 1.000040]\n",
            "1724 [D loss: 0.999986] [G loss: 1.000102]\n",
            "1725 [D loss: 0.999950] [G loss: 1.000096]\n",
            "1726 [D loss: 0.999987] [G loss: 1.000070]\n",
            "1727 [D loss: 0.999982] [G loss: 1.000087]\n",
            "1728 [D loss: 0.999952] [G loss: 1.000044]\n",
            "1729 [D loss: 0.999957] [G loss: 1.000068]\n",
            "1730 [D loss: 0.999989] [G loss: 1.000079]\n",
            "1731 [D loss: 0.999969] [G loss: 1.000081]\n",
            "1732 [D loss: 0.999986] [G loss: 1.000103]\n",
            "1733 [D loss: 0.999951] [G loss: 1.000072]\n",
            "1734 [D loss: 0.999960] [G loss: 1.000065]\n",
            "1735 [D loss: 0.999964] [G loss: 1.000070]\n",
            "1736 [D loss: 0.999977] [G loss: 1.000074]\n",
            "1737 [D loss: 0.999955] [G loss: 1.000066]\n",
            "1738 [D loss: 0.999992] [G loss: 1.000099]\n",
            "1739 [D loss: 0.999949] [G loss: 1.000068]\n",
            "1740 [D loss: 0.999984] [G loss: 1.000070]\n",
            "1741 [D loss: 0.999967] [G loss: 1.000080]\n",
            "1742 [D loss: 0.999948] [G loss: 1.000070]\n",
            "1743 [D loss: 0.999988] [G loss: 1.000056]\n",
            "1744 [D loss: 0.999988] [G loss: 1.000056]\n",
            "1745 [D loss: 0.999985] [G loss: 1.000043]\n",
            "1746 [D loss: 0.999956] [G loss: 1.000048]\n",
            "1747 [D loss: 0.999982] [G loss: 1.000081]\n",
            "1748 [D loss: 0.999999] [G loss: 1.000059]\n",
            "1749 [D loss: 0.999966] [G loss: 1.000074]\n",
            "1750 [D loss: 0.999973] [G loss: 1.000060]\n",
            "1751 [D loss: 0.999938] [G loss: 1.000071]\n",
            "1752 [D loss: 0.999958] [G loss: 1.000057]\n",
            "1753 [D loss: 0.999970] [G loss: 1.000092]\n",
            "1754 [D loss: 0.999987] [G loss: 1.000043]\n",
            "1755 [D loss: 0.999985] [G loss: 1.000056]\n",
            "1756 [D loss: 1.000004] [G loss: 1.000087]\n",
            "1757 [D loss: 0.999957] [G loss: 1.000067]\n",
            "1758 [D loss: 0.999953] [G loss: 1.000062]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1759 [D loss: 0.999976] [G loss: 1.000085]\n",
            "1760 [D loss: 0.999951] [G loss: 1.000073]\n",
            "1761 [D loss: 0.999982] [G loss: 1.000054]\n",
            "1762 [D loss: 1.000003] [G loss: 1.000077]\n",
            "1763 [D loss: 0.999975] [G loss: 1.000093]\n",
            "1764 [D loss: 0.999982] [G loss: 1.000068]\n",
            "1765 [D loss: 0.999960] [G loss: 1.000073]\n",
            "1766 [D loss: 0.999991] [G loss: 1.000096]\n",
            "1767 [D loss: 0.999964] [G loss: 1.000062]\n",
            "1768 [D loss: 0.999950] [G loss: 1.000097]\n",
            "1769 [D loss: 0.999986] [G loss: 1.000075]\n",
            "1770 [D loss: 0.999974] [G loss: 1.000084]\n",
            "1771 [D loss: 0.999979] [G loss: 1.000077]\n",
            "1772 [D loss: 0.999955] [G loss: 1.000042]\n",
            "1773 [D loss: 0.999947] [G loss: 1.000061]\n",
            "1774 [D loss: 0.999961] [G loss: 1.000078]\n",
            "1775 [D loss: 0.999962] [G loss: 1.000075]\n",
            "1776 [D loss: 0.999962] [G loss: 1.000070]\n",
            "1777 [D loss: 0.999982] [G loss: 1.000081]\n",
            "1778 [D loss: 0.999942] [G loss: 1.000071]\n",
            "1779 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1780 [D loss: 0.999951] [G loss: 1.000129]\n",
            "1781 [D loss: 0.999918] [G loss: 1.000060]\n",
            "1782 [D loss: 0.999981] [G loss: 1.000060]\n",
            "1783 [D loss: 0.999985] [G loss: 1.000069]\n",
            "1784 [D loss: 0.999958] [G loss: 1.000078]\n",
            "1785 [D loss: 0.999977] [G loss: 1.000083]\n",
            "1786 [D loss: 0.999935] [G loss: 1.000079]\n",
            "1787 [D loss: 0.999987] [G loss: 1.000053]\n",
            "1788 [D loss: 0.999943] [G loss: 1.000082]\n",
            "1789 [D loss: 0.999959] [G loss: 1.000080]\n",
            "1790 [D loss: 0.999928] [G loss: 1.000087]\n",
            "1791 [D loss: 0.999965] [G loss: 1.000057]\n",
            "1792 [D loss: 0.999939] [G loss: 1.000094]\n",
            "1793 [D loss: 0.999971] [G loss: 1.000055]\n",
            "1794 [D loss: 0.999989] [G loss: 1.000058]\n",
            "1795 [D loss: 0.999973] [G loss: 1.000049]\n",
            "1796 [D loss: 0.999966] [G loss: 1.000094]\n",
            "1797 [D loss: 0.999980] [G loss: 1.000090]\n",
            "1798 [D loss: 0.999982] [G loss: 1.000037]\n",
            "1799 [D loss: 0.999938] [G loss: 1.000076]\n",
            "1800 [D loss: 0.999990] [G loss: 1.000079]\n",
            "1801 [D loss: 0.999979] [G loss: 1.000075]\n",
            "1802 [D loss: 0.999959] [G loss: 1.000066]\n",
            "1803 [D loss: 0.999993] [G loss: 1.000051]\n",
            "1804 [D loss: 0.999970] [G loss: 1.000056]\n",
            "1805 [D loss: 0.999957] [G loss: 1.000049]\n",
            "1806 [D loss: 0.999951] [G loss: 1.000050]\n",
            "1807 [D loss: 0.999921] [G loss: 1.000033]\n",
            "1808 [D loss: 0.999953] [G loss: 1.000080]\n",
            "1809 [D loss: 0.999978] [G loss: 1.000072]\n",
            "1810 [D loss: 0.999968] [G loss: 1.000075]\n",
            "1811 [D loss: 0.999983] [G loss: 1.000053]\n",
            "1812 [D loss: 0.999971] [G loss: 1.000072]\n",
            "1813 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1814 [D loss: 0.999959] [G loss: 1.000066]\n",
            "1815 [D loss: 0.999985] [G loss: 1.000051]\n",
            "1816 [D loss: 0.999975] [G loss: 1.000047]\n",
            "1817 [D loss: 0.999958] [G loss: 1.000070]\n",
            "1818 [D loss: 0.999988] [G loss: 1.000038]\n",
            "1819 [D loss: 0.999991] [G loss: 1.000074]\n",
            "1820 [D loss: 0.999932] [G loss: 1.000062]\n",
            "1821 [D loss: 0.999997] [G loss: 1.000097]\n",
            "1822 [D loss: 0.999963] [G loss: 1.000061]\n",
            "1823 [D loss: 1.000001] [G loss: 1.000056]\n",
            "1824 [D loss: 0.999975] [G loss: 1.000063]\n",
            "1825 [D loss: 0.999958] [G loss: 1.000079]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1826 [D loss: 0.999944] [G loss: 1.000094]\n",
            "1827 [D loss: 0.999991] [G loss: 1.000072]\n",
            "1828 [D loss: 0.999987] [G loss: 1.000065]\n",
            "1829 [D loss: 0.999969] [G loss: 1.000077]\n",
            "1830 [D loss: 0.999942] [G loss: 1.000081]\n",
            "1831 [D loss: 0.999965] [G loss: 1.000059]\n",
            "1832 [D loss: 0.999948] [G loss: 1.000091]\n",
            "1833 [D loss: 0.999994] [G loss: 1.000091]\n",
            "1834 [D loss: 0.999954] [G loss: 1.000065]\n",
            "1835 [D loss: 0.999978] [G loss: 1.000075]\n",
            "1836 [D loss: 0.999962] [G loss: 1.000064]\n",
            "1837 [D loss: 0.999961] [G loss: 1.000075]\n",
            "1838 [D loss: 0.999967] [G loss: 1.000088]\n",
            "1839 [D loss: 0.999997] [G loss: 1.000050]\n",
            "1840 [D loss: 0.999955] [G loss: 1.000053]\n",
            "1841 [D loss: 0.999985] [G loss: 1.000057]\n",
            "1842 [D loss: 0.999946] [G loss: 1.000069]\n",
            "1843 [D loss: 0.999984] [G loss: 1.000086]\n",
            "1844 [D loss: 0.999982] [G loss: 1.000082]\n",
            "1845 [D loss: 0.999980] [G loss: 1.000084]\n",
            "1846 [D loss: 0.999974] [G loss: 1.000030]\n",
            "1847 [D loss: 0.999999] [G loss: 1.000077]\n",
            "1848 [D loss: 0.999964] [G loss: 1.000066]\n",
            "1849 [D loss: 0.999928] [G loss: 1.000049]\n",
            "1850 [D loss: 0.999972] [G loss: 1.000072]\n",
            "1851 [D loss: 0.999963] [G loss: 1.000092]\n",
            "1852 [D loss: 0.999988] [G loss: 1.000048]\n",
            "1853 [D loss: 0.999963] [G loss: 1.000100]\n",
            "1854 [D loss: 0.999962] [G loss: 1.000059]\n",
            "1855 [D loss: 0.999997] [G loss: 1.000066]\n",
            "1856 [D loss: 0.999987] [G loss: 1.000064]\n",
            "1857 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1858 [D loss: 1.000006] [G loss: 1.000051]\n",
            "1859 [D loss: 0.999955] [G loss: 1.000062]\n",
            "1860 [D loss: 0.999970] [G loss: 1.000049]\n",
            "1861 [D loss: 0.999954] [G loss: 1.000044]\n",
            "1862 [D loss: 0.999969] [G loss: 1.000074]\n",
            "1863 [D loss: 0.999953] [G loss: 1.000090]\n",
            "1864 [D loss: 0.999973] [G loss: 1.000064]\n",
            "1865 [D loss: 0.999965] [G loss: 1.000043]\n",
            "1866 [D loss: 0.999986] [G loss: 1.000050]\n",
            "1867 [D loss: 0.999960] [G loss: 1.000042]\n",
            "1868 [D loss: 0.999984] [G loss: 1.000086]\n",
            "1869 [D loss: 0.999995] [G loss: 1.000058]\n",
            "1870 [D loss: 0.999946] [G loss: 1.000086]\n",
            "1871 [D loss: 0.999980] [G loss: 1.000046]\n",
            "1872 [D loss: 0.999974] [G loss: 1.000098]\n",
            "1873 [D loss: 0.999944] [G loss: 1.000063]\n",
            "1874 [D loss: 0.999956] [G loss: 1.000082]\n",
            "1875 [D loss: 0.999994] [G loss: 1.000086]\n",
            "1876 [D loss: 0.999991] [G loss: 1.000072]\n",
            "1877 [D loss: 0.999988] [G loss: 1.000081]\n",
            "1878 [D loss: 0.999941] [G loss: 1.000075]\n",
            "1879 [D loss: 0.999949] [G loss: 1.000088]\n",
            "1880 [D loss: 0.999982] [G loss: 1.000070]\n",
            "1881 [D loss: 0.999978] [G loss: 1.000084]\n",
            "1882 [D loss: 0.999987] [G loss: 1.000070]\n",
            "1883 [D loss: 0.999959] [G loss: 1.000074]\n",
            "1884 [D loss: 0.999966] [G loss: 1.000115]\n",
            "1885 [D loss: 0.999979] [G loss: 1.000054]\n",
            "1886 [D loss: 0.999920] [G loss: 1.000069]\n",
            "1887 [D loss: 0.999971] [G loss: 1.000106]\n",
            "1888 [D loss: 0.999958] [G loss: 1.000080]\n",
            "1889 [D loss: 0.999956] [G loss: 1.000117]\n",
            "1890 [D loss: 0.999957] [G loss: 1.000073]\n",
            "1891 [D loss: 0.999986] [G loss: 1.000079]\n",
            "1892 [D loss: 0.999982] [G loss: 1.000079]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1893 [D loss: 0.999994] [G loss: 1.000084]\n",
            "1894 [D loss: 1.000012] [G loss: 1.000068]\n",
            "1895 [D loss: 0.999948] [G loss: 1.000052]\n",
            "1896 [D loss: 0.999958] [G loss: 1.000064]\n",
            "1897 [D loss: 0.999973] [G loss: 1.000070]\n",
            "1898 [D loss: 0.999983] [G loss: 1.000094]\n",
            "1899 [D loss: 0.999954] [G loss: 1.000092]\n",
            "1900 [D loss: 0.999963] [G loss: 1.000090]\n",
            "1901 [D loss: 0.999950] [G loss: 1.000056]\n",
            "1902 [D loss: 0.999952] [G loss: 1.000093]\n",
            "1903 [D loss: 0.999963] [G loss: 1.000099]\n",
            "1904 [D loss: 1.000022] [G loss: 1.000071]\n",
            "1905 [D loss: 0.999984] [G loss: 1.000079]\n",
            "1906 [D loss: 0.999959] [G loss: 1.000069]\n",
            "1907 [D loss: 0.999978] [G loss: 1.000091]\n",
            "1908 [D loss: 0.999947] [G loss: 1.000063]\n",
            "1909 [D loss: 0.999948] [G loss: 1.000060]\n",
            "1910 [D loss: 0.999970] [G loss: 1.000087]\n",
            "1911 [D loss: 0.999985] [G loss: 1.000045]\n",
            "1912 [D loss: 0.999985] [G loss: 1.000033]\n",
            "1913 [D loss: 0.999974] [G loss: 1.000110]\n",
            "1914 [D loss: 0.999941] [G loss: 1.000064]\n",
            "1915 [D loss: 1.000003] [G loss: 1.000089]\n",
            "1916 [D loss: 0.999932] [G loss: 1.000072]\n",
            "1917 [D loss: 0.999956] [G loss: 1.000046]\n",
            "1918 [D loss: 0.999962] [G loss: 1.000080]\n",
            "1919 [D loss: 0.999972] [G loss: 1.000101]\n",
            "1920 [D loss: 0.999965] [G loss: 1.000076]\n",
            "1921 [D loss: 0.999941] [G loss: 1.000067]\n",
            "1922 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1923 [D loss: 0.999982] [G loss: 1.000064]\n",
            "1924 [D loss: 0.999962] [G loss: 1.000065]\n",
            "1925 [D loss: 0.999979] [G loss: 1.000078]\n",
            "1926 [D loss: 0.999963] [G loss: 1.000094]\n",
            "1927 [D loss: 0.999952] [G loss: 1.000053]\n",
            "1928 [D loss: 0.999988] [G loss: 1.000071]\n",
            "1929 [D loss: 0.999969] [G loss: 1.000052]\n",
            "1930 [D loss: 0.999969] [G loss: 1.000082]\n",
            "1931 [D loss: 0.999982] [G loss: 1.000066]\n",
            "1932 [D loss: 0.999969] [G loss: 1.000042]\n",
            "1933 [D loss: 0.999964] [G loss: 1.000095]\n",
            "1934 [D loss: 0.999984] [G loss: 1.000082]\n",
            "1935 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1936 [D loss: 1.000009] [G loss: 1.000065]\n",
            "1937 [D loss: 0.999982] [G loss: 1.000045]\n",
            "1938 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1939 [D loss: 0.999933] [G loss: 1.000089]\n",
            "1940 [D loss: 0.999944] [G loss: 1.000062]\n",
            "1941 [D loss: 0.999965] [G loss: 1.000066]\n",
            "1942 [D loss: 0.999981] [G loss: 1.000099]\n",
            "1943 [D loss: 0.999944] [G loss: 1.000066]\n",
            "1944 [D loss: 0.999956] [G loss: 1.000069]\n",
            "1945 [D loss: 1.000000] [G loss: 1.000063]\n",
            "1946 [D loss: 0.999971] [G loss: 1.000051]\n",
            "1947 [D loss: 0.999976] [G loss: 1.000067]\n",
            "1948 [D loss: 0.999931] [G loss: 1.000070]\n",
            "1949 [D loss: 0.999988] [G loss: 1.000022]\n",
            "1950 [D loss: 0.999949] [G loss: 1.000096]\n",
            "1951 [D loss: 0.999973] [G loss: 1.000076]\n",
            "1952 [D loss: 0.999960] [G loss: 1.000064]\n",
            "1953 [D loss: 0.999956] [G loss: 1.000042]\n",
            "1954 [D loss: 0.999928] [G loss: 1.000041]\n",
            "1955 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1956 [D loss: 0.999975] [G loss: 1.000075]\n",
            "1957 [D loss: 0.999989] [G loss: 1.000096]\n",
            "1958 [D loss: 0.999957] [G loss: 1.000105]\n",
            "1959 [D loss: 0.999977] [G loss: 1.000047]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1960 [D loss: 0.999964] [G loss: 1.000068]\n",
            "1961 [D loss: 0.999997] [G loss: 1.000068]\n",
            "1962 [D loss: 0.999963] [G loss: 1.000035]\n",
            "1963 [D loss: 0.999991] [G loss: 1.000080]\n",
            "1964 [D loss: 0.999966] [G loss: 1.000068]\n",
            "1965 [D loss: 0.999986] [G loss: 1.000049]\n",
            "1966 [D loss: 0.999959] [G loss: 1.000040]\n",
            "1967 [D loss: 1.000008] [G loss: 1.000108]\n",
            "1968 [D loss: 0.999977] [G loss: 1.000074]\n",
            "1969 [D loss: 0.999948] [G loss: 1.000096]\n",
            "1970 [D loss: 0.999980] [G loss: 1.000042]\n",
            "1971 [D loss: 0.999990] [G loss: 1.000052]\n",
            "1972 [D loss: 0.999927] [G loss: 1.000066]\n",
            "1973 [D loss: 0.999958] [G loss: 1.000047]\n",
            "1974 [D loss: 0.999947] [G loss: 1.000070]\n",
            "1975 [D loss: 0.999987] [G loss: 1.000053]\n",
            "1976 [D loss: 0.999993] [G loss: 1.000027]\n",
            "1977 [D loss: 0.999948] [G loss: 1.000036]\n",
            "1978 [D loss: 0.999989] [G loss: 1.000046]\n",
            "1979 [D loss: 0.999977] [G loss: 1.000082]\n",
            "1980 [D loss: 0.999960] [G loss: 1.000112]\n",
            "1981 [D loss: 0.999959] [G loss: 1.000057]\n",
            "1982 [D loss: 0.999989] [G loss: 1.000068]\n",
            "1983 [D loss: 0.999947] [G loss: 1.000050]\n",
            "1984 [D loss: 0.999947] [G loss: 1.000050]\n",
            "1985 [D loss: 0.999998] [G loss: 1.000089]\n",
            "1986 [D loss: 0.999976] [G loss: 1.000084]\n",
            "1987 [D loss: 0.999954] [G loss: 1.000048]\n",
            "1988 [D loss: 0.999950] [G loss: 1.000060]\n",
            "1989 [D loss: 0.999977] [G loss: 1.000078]\n",
            "1990 [D loss: 0.999990] [G loss: 1.000079]\n",
            "1991 [D loss: 0.999973] [G loss: 1.000023]\n",
            "1992 [D loss: 0.999937] [G loss: 1.000103]\n",
            "1993 [D loss: 0.999998] [G loss: 1.000041]\n",
            "1994 [D loss: 0.999980] [G loss: 1.000099]\n",
            "1995 [D loss: 0.999917] [G loss: 1.000078]\n",
            "1996 [D loss: 1.000022] [G loss: 1.000074]\n",
            "1997 [D loss: 0.999958] [G loss: 1.000061]\n",
            "1998 [D loss: 1.000002] [G loss: 1.000054]\n",
            "1999 [D loss: 0.999941] [G loss: 1.000072]\n",
            "2000 [D loss: 0.999957] [G loss: 1.000092]\n",
            "2001 [D loss: 0.999942] [G loss: 1.000065]\n",
            "2002 [D loss: 0.999939] [G loss: 1.000069]\n",
            "2003 [D loss: 0.999950] [G loss: 1.000070]\n",
            "2004 [D loss: 0.999962] [G loss: 1.000106]\n",
            "2005 [D loss: 0.999960] [G loss: 1.000094]\n",
            "2006 [D loss: 0.999954] [G loss: 1.000045]\n",
            "2007 [D loss: 0.999943] [G loss: 1.000070]\n",
            "2008 [D loss: 0.999964] [G loss: 1.000070]\n",
            "2009 [D loss: 0.999954] [G loss: 1.000076]\n",
            "2010 [D loss: 0.999972] [G loss: 1.000035]\n",
            "2011 [D loss: 0.999981] [G loss: 1.000056]\n",
            "2012 [D loss: 0.999989] [G loss: 1.000078]\n",
            "2013 [D loss: 0.999982] [G loss: 1.000094]\n",
            "2014 [D loss: 0.999940] [G loss: 1.000028]\n",
            "2015 [D loss: 0.999988] [G loss: 1.000077]\n",
            "2016 [D loss: 0.999974] [G loss: 1.000056]\n",
            "2017 [D loss: 0.999983] [G loss: 1.000073]\n",
            "2018 [D loss: 0.999963] [G loss: 1.000066]\n",
            "2019 [D loss: 0.999962] [G loss: 1.000066]\n",
            "2020 [D loss: 0.999953] [G loss: 1.000061]\n",
            "2021 [D loss: 0.999988] [G loss: 1.000030]\n",
            "2022 [D loss: 0.999955] [G loss: 1.000070]\n",
            "2023 [D loss: 0.999967] [G loss: 1.000070]\n",
            "2024 [D loss: 0.999961] [G loss: 1.000074]\n",
            "2025 [D loss: 0.999949] [G loss: 1.000079]\n",
            "2026 [D loss: 0.999972] [G loss: 1.000070]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2027 [D loss: 0.999981] [G loss: 1.000031]\n",
            "2028 [D loss: 0.999972] [G loss: 1.000041]\n",
            "2029 [D loss: 0.999990] [G loss: 1.000072]\n",
            "2030 [D loss: 0.999973] [G loss: 1.000077]\n",
            "2031 [D loss: 0.999983] [G loss: 1.000061]\n",
            "2032 [D loss: 0.999984] [G loss: 1.000041]\n",
            "2033 [D loss: 0.999941] [G loss: 1.000079]\n",
            "2034 [D loss: 0.999977] [G loss: 1.000065]\n",
            "2035 [D loss: 0.999966] [G loss: 1.000088]\n",
            "2036 [D loss: 0.999972] [G loss: 1.000055]\n",
            "2037 [D loss: 0.999954] [G loss: 1.000084]\n",
            "2038 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2039 [D loss: 0.999961] [G loss: 1.000068]\n",
            "2040 [D loss: 0.999970] [G loss: 1.000054]\n",
            "2041 [D loss: 0.999978] [G loss: 1.000073]\n",
            "2042 [D loss: 0.999996] [G loss: 1.000066]\n",
            "2043 [D loss: 0.999952] [G loss: 1.000047]\n",
            "2044 [D loss: 0.999991] [G loss: 1.000082]\n",
            "2045 [D loss: 0.999949] [G loss: 1.000072]\n",
            "2046 [D loss: 0.999952] [G loss: 1.000055]\n",
            "2047 [D loss: 0.999964] [G loss: 1.000074]\n",
            "2048 [D loss: 0.999949] [G loss: 1.000054]\n",
            "2049 [D loss: 0.999997] [G loss: 1.000072]\n",
            "2050 [D loss: 0.999965] [G loss: 1.000044]\n",
            "2051 [D loss: 0.999957] [G loss: 1.000070]\n",
            "2052 [D loss: 0.999957] [G loss: 1.000075]\n",
            "2053 [D loss: 0.999993] [G loss: 1.000058]\n",
            "2054 [D loss: 0.999998] [G loss: 1.000059]\n",
            "2055 [D loss: 0.999942] [G loss: 1.000040]\n",
            "2056 [D loss: 0.999967] [G loss: 1.000067]\n",
            "2057 [D loss: 0.999977] [G loss: 1.000070]\n",
            "2058 [D loss: 0.999953] [G loss: 1.000099]\n",
            "2059 [D loss: 0.999932] [G loss: 1.000086]\n",
            "2060 [D loss: 0.999938] [G loss: 1.000079]\n",
            "2061 [D loss: 0.999951] [G loss: 1.000056]\n",
            "2062 [D loss: 0.999948] [G loss: 1.000060]\n",
            "2063 [D loss: 0.999956] [G loss: 1.000050]\n",
            "2064 [D loss: 0.999973] [G loss: 1.000103]\n",
            "2065 [D loss: 0.999934] [G loss: 1.000039]\n",
            "2066 [D loss: 0.999968] [G loss: 1.000070]\n",
            "2067 [D loss: 0.999970] [G loss: 1.000078]\n",
            "2068 [D loss: 0.999972] [G loss: 1.000082]\n",
            "2069 [D loss: 0.999973] [G loss: 1.000084]\n",
            "2070 [D loss: 0.999973] [G loss: 1.000086]\n",
            "2071 [D loss: 0.999944] [G loss: 1.000091]\n",
            "2072 [D loss: 0.999980] [G loss: 1.000062]\n",
            "2073 [D loss: 0.999977] [G loss: 1.000075]\n",
            "2074 [D loss: 0.999950] [G loss: 1.000074]\n",
            "2075 [D loss: 0.999967] [G loss: 1.000049]\n",
            "2076 [D loss: 0.999956] [G loss: 1.000079]\n",
            "2077 [D loss: 0.999962] [G loss: 1.000045]\n",
            "2078 [D loss: 0.999958] [G loss: 1.000090]\n",
            "2079 [D loss: 0.999982] [G loss: 1.000084]\n",
            "2080 [D loss: 0.999951] [G loss: 1.000085]\n",
            "2081 [D loss: 0.999962] [G loss: 1.000043]\n",
            "2082 [D loss: 0.999972] [G loss: 1.000111]\n",
            "2083 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2084 [D loss: 0.999962] [G loss: 1.000076]\n",
            "2085 [D loss: 0.999968] [G loss: 1.000054]\n",
            "2086 [D loss: 0.999977] [G loss: 1.000114]\n",
            "2087 [D loss: 0.999970] [G loss: 1.000076]\n",
            "2088 [D loss: 0.999987] [G loss: 1.000074]\n",
            "2089 [D loss: 0.999989] [G loss: 1.000062]\n",
            "2090 [D loss: 0.999973] [G loss: 1.000081]\n",
            "2091 [D loss: 0.999975] [G loss: 1.000045]\n",
            "2092 [D loss: 0.999975] [G loss: 1.000082]\n",
            "2093 [D loss: 1.000001] [G loss: 1.000049]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2094 [D loss: 0.999949] [G loss: 1.000060]\n",
            "2095 [D loss: 0.999989] [G loss: 1.000039]\n",
            "2096 [D loss: 0.999980] [G loss: 1.000059]\n",
            "2097 [D loss: 0.999969] [G loss: 1.000070]\n",
            "2098 [D loss: 0.999971] [G loss: 1.000107]\n",
            "2099 [D loss: 0.999999] [G loss: 1.000060]\n",
            "2100 [D loss: 0.999992] [G loss: 1.000042]\n",
            "2101 [D loss: 0.999967] [G loss: 1.000080]\n",
            "2102 [D loss: 0.999965] [G loss: 1.000106]\n",
            "2103 [D loss: 0.999980] [G loss: 1.000058]\n",
            "2104 [D loss: 0.999948] [G loss: 1.000071]\n",
            "2105 [D loss: 0.999985] [G loss: 1.000089]\n",
            "2106 [D loss: 1.000007] [G loss: 1.000064]\n",
            "2107 [D loss: 1.000002] [G loss: 1.000035]\n",
            "2108 [D loss: 0.999991] [G loss: 1.000113]\n",
            "2109 [D loss: 0.999981] [G loss: 1.000060]\n",
            "2110 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2111 [D loss: 0.999979] [G loss: 1.000060]\n",
            "2112 [D loss: 0.999955] [G loss: 1.000088]\n",
            "2113 [D loss: 0.999928] [G loss: 1.000108]\n",
            "2114 [D loss: 0.999931] [G loss: 1.000072]\n",
            "2115 [D loss: 0.999927] [G loss: 1.000042]\n",
            "2116 [D loss: 0.999949] [G loss: 1.000097]\n",
            "2117 [D loss: 0.999950] [G loss: 1.000062]\n",
            "2118 [D loss: 0.999986] [G loss: 1.000044]\n",
            "2119 [D loss: 0.999988] [G loss: 1.000071]\n",
            "2120 [D loss: 0.999983] [G loss: 1.000064]\n",
            "2121 [D loss: 0.999973] [G loss: 1.000084]\n",
            "2122 [D loss: 0.999951] [G loss: 1.000067]\n",
            "2123 [D loss: 0.999931] [G loss: 1.000061]\n",
            "2124 [D loss: 0.999958] [G loss: 1.000049]\n",
            "2125 [D loss: 0.999963] [G loss: 1.000078]\n",
            "2126 [D loss: 0.999959] [G loss: 1.000072]\n",
            "2127 [D loss: 0.999949] [G loss: 1.000074]\n",
            "2128 [D loss: 0.999955] [G loss: 1.000057]\n",
            "2129 [D loss: 0.999966] [G loss: 1.000077]\n",
            "2130 [D loss: 0.999965] [G loss: 1.000092]\n",
            "2131 [D loss: 0.999939] [G loss: 1.000078]\n",
            "2132 [D loss: 0.999986] [G loss: 1.000075]\n",
            "2133 [D loss: 0.999963] [G loss: 1.000092]\n",
            "2134 [D loss: 0.999996] [G loss: 1.000070]\n",
            "2135 [D loss: 0.999989] [G loss: 1.000061]\n",
            "2136 [D loss: 0.999950] [G loss: 1.000081]\n",
            "2137 [D loss: 1.000006] [G loss: 1.000082]\n",
            "2138 [D loss: 0.999986] [G loss: 1.000056]\n",
            "2139 [D loss: 0.999944] [G loss: 1.000081]\n",
            "2140 [D loss: 0.999972] [G loss: 1.000049]\n",
            "2141 [D loss: 0.999964] [G loss: 1.000079]\n",
            "2142 [D loss: 0.999949] [G loss: 1.000085]\n",
            "2143 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2144 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2145 [D loss: 0.999976] [G loss: 1.000067]\n",
            "2146 [D loss: 0.999977] [G loss: 1.000087]\n",
            "2147 [D loss: 0.999971] [G loss: 1.000079]\n",
            "2148 [D loss: 0.999977] [G loss: 1.000042]\n",
            "2149 [D loss: 0.999963] [G loss: 1.000081]\n",
            "2150 [D loss: 0.999934] [G loss: 1.000055]\n",
            "2151 [D loss: 0.999975] [G loss: 1.000075]\n",
            "2152 [D loss: 0.999976] [G loss: 1.000059]\n",
            "2153 [D loss: 0.999971] [G loss: 1.000105]\n",
            "2154 [D loss: 0.999941] [G loss: 1.000065]\n",
            "2155 [D loss: 0.999980] [G loss: 1.000113]\n",
            "2156 [D loss: 0.999964] [G loss: 1.000038]\n",
            "2157 [D loss: 0.999949] [G loss: 1.000064]\n",
            "2158 [D loss: 0.999965] [G loss: 1.000064]\n",
            "2159 [D loss: 0.999963] [G loss: 1.000077]\n",
            "2160 [D loss: 0.999972] [G loss: 1.000072]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2161 [D loss: 0.999962] [G loss: 1.000085]\n",
            "2162 [D loss: 0.999971] [G loss: 1.000103]\n",
            "2163 [D loss: 0.999964] [G loss: 1.000089]\n",
            "2164 [D loss: 0.999979] [G loss: 1.000070]\n",
            "2165 [D loss: 0.999962] [G loss: 1.000032]\n",
            "2166 [D loss: 0.999964] [G loss: 1.000082]\n",
            "2167 [D loss: 0.999978] [G loss: 1.000074]\n",
            "2168 [D loss: 0.999910] [G loss: 1.000068]\n",
            "2169 [D loss: 0.999992] [G loss: 1.000097]\n",
            "2170 [D loss: 0.999983] [G loss: 1.000085]\n",
            "2171 [D loss: 0.999992] [G loss: 1.000060]\n",
            "2172 [D loss: 0.999968] [G loss: 1.000097]\n",
            "2173 [D loss: 1.000038] [G loss: 1.000109]\n",
            "2174 [D loss: 0.999969] [G loss: 1.000110]\n",
            "2175 [D loss: 0.999971] [G loss: 1.000090]\n",
            "2176 [D loss: 0.999949] [G loss: 1.000065]\n",
            "2177 [D loss: 0.999999] [G loss: 1.000062]\n",
            "2178 [D loss: 0.999970] [G loss: 1.000083]\n",
            "2179 [D loss: 0.999965] [G loss: 1.000068]\n",
            "2180 [D loss: 0.999966] [G loss: 1.000025]\n",
            "2181 [D loss: 0.999991] [G loss: 1.000053]\n",
            "2182 [D loss: 0.999989] [G loss: 1.000047]\n",
            "2183 [D loss: 0.999972] [G loss: 1.000045]\n",
            "2184 [D loss: 0.999973] [G loss: 1.000070]\n",
            "2185 [D loss: 0.999976] [G loss: 1.000085]\n",
            "2186 [D loss: 0.999984] [G loss: 1.000075]\n",
            "2187 [D loss: 0.999967] [G loss: 1.000060]\n",
            "2188 [D loss: 0.999949] [G loss: 1.000067]\n",
            "2189 [D loss: 0.999956] [G loss: 1.000119]\n",
            "2190 [D loss: 0.999986] [G loss: 1.000069]\n",
            "2191 [D loss: 0.999964] [G loss: 1.000071]\n",
            "2192 [D loss: 0.999995] [G loss: 1.000084]\n",
            "2193 [D loss: 0.999977] [G loss: 1.000043]\n",
            "2194 [D loss: 0.999948] [G loss: 1.000085]\n",
            "2195 [D loss: 0.999965] [G loss: 1.000091]\n",
            "2196 [D loss: 0.999958] [G loss: 1.000072]\n",
            "2197 [D loss: 0.999941] [G loss: 1.000074]\n",
            "2198 [D loss: 0.999992] [G loss: 1.000091]\n",
            "2199 [D loss: 0.999961] [G loss: 1.000051]\n",
            "2200 [D loss: 0.999980] [G loss: 1.000009]\n",
            "2201 [D loss: 0.999967] [G loss: 1.000069]\n",
            "2202 [D loss: 0.999956] [G loss: 1.000129]\n",
            "2203 [D loss: 0.999978] [G loss: 1.000119]\n",
            "2204 [D loss: 0.999957] [G loss: 1.000077]\n",
            "2205 [D loss: 0.999953] [G loss: 1.000084]\n",
            "2206 [D loss: 0.999965] [G loss: 1.000082]\n",
            "2207 [D loss: 0.999967] [G loss: 1.000064]\n",
            "2208 [D loss: 0.999963] [G loss: 1.000073]\n",
            "2209 [D loss: 0.999985] [G loss: 1.000101]\n",
            "2210 [D loss: 0.999974] [G loss: 1.000060]\n",
            "2211 [D loss: 0.999983] [G loss: 1.000063]\n",
            "2212 [D loss: 0.999965] [G loss: 1.000089]\n",
            "2213 [D loss: 0.999974] [G loss: 1.000081]\n",
            "2214 [D loss: 0.999992] [G loss: 1.000077]\n",
            "2215 [D loss: 0.999982] [G loss: 1.000118]\n",
            "2216 [D loss: 0.999973] [G loss: 1.000088]\n",
            "2217 [D loss: 0.999969] [G loss: 1.000110]\n",
            "2218 [D loss: 0.999934] [G loss: 1.000065]\n",
            "2219 [D loss: 0.999911] [G loss: 1.000092]\n",
            "2220 [D loss: 0.999977] [G loss: 1.000038]\n",
            "2221 [D loss: 0.999996] [G loss: 1.000054]\n",
            "2222 [D loss: 1.000007] [G loss: 1.000078]\n",
            "2223 [D loss: 0.999978] [G loss: 1.000087]\n",
            "2224 [D loss: 0.999960] [G loss: 1.000093]\n",
            "2225 [D loss: 0.999993] [G loss: 1.000062]\n",
            "2226 [D loss: 0.999946] [G loss: 1.000064]\n",
            "2227 [D loss: 0.999944] [G loss: 1.000057]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2228 [D loss: 0.999965] [G loss: 1.000062]\n",
            "2229 [D loss: 0.999987] [G loss: 1.000048]\n",
            "2230 [D loss: 0.999995] [G loss: 1.000065]\n",
            "2231 [D loss: 0.999997] [G loss: 1.000069]\n",
            "2232 [D loss: 0.999962] [G loss: 1.000088]\n",
            "2233 [D loss: 0.999989] [G loss: 1.000088]\n",
            "2234 [D loss: 0.999979] [G loss: 1.000105]\n",
            "2235 [D loss: 0.999984] [G loss: 1.000041]\n",
            "2236 [D loss: 0.999979] [G loss: 1.000042]\n",
            "2237 [D loss: 0.999979] [G loss: 1.000079]\n",
            "2238 [D loss: 0.999941] [G loss: 1.000058]\n",
            "2239 [D loss: 0.999990] [G loss: 1.000050]\n",
            "2240 [D loss: 0.999987] [G loss: 1.000072]\n",
            "2241 [D loss: 0.999957] [G loss: 1.000094]\n",
            "2242 [D loss: 0.999937] [G loss: 1.000097]\n",
            "2243 [D loss: 0.999979] [G loss: 1.000079]\n",
            "2244 [D loss: 0.999935] [G loss: 1.000049]\n",
            "2245 [D loss: 0.999979] [G loss: 1.000083]\n",
            "2246 [D loss: 0.999970] [G loss: 1.000088]\n",
            "2247 [D loss: 0.999970] [G loss: 1.000037]\n",
            "2248 [D loss: 0.999961] [G loss: 1.000097]\n",
            "2249 [D loss: 0.999939] [G loss: 1.000052]\n",
            "2250 [D loss: 0.999940] [G loss: 1.000084]\n",
            "2251 [D loss: 1.000011] [G loss: 1.000083]\n",
            "2252 [D loss: 0.999947] [G loss: 1.000073]\n",
            "2253 [D loss: 0.999969] [G loss: 1.000104]\n",
            "2254 [D loss: 0.999934] [G loss: 1.000067]\n",
            "2255 [D loss: 0.999949] [G loss: 1.000074]\n",
            "2256 [D loss: 0.999965] [G loss: 1.000132]\n",
            "2257 [D loss: 0.999950] [G loss: 1.000107]\n",
            "2258 [D loss: 0.999961] [G loss: 1.000069]\n",
            "2259 [D loss: 0.999947] [G loss: 1.000056]\n",
            "2260 [D loss: 0.999981] [G loss: 1.000069]\n",
            "2261 [D loss: 0.999988] [G loss: 1.000071]\n",
            "2262 [D loss: 0.999995] [G loss: 1.000075]\n",
            "2263 [D loss: 0.999954] [G loss: 1.000047]\n",
            "2264 [D loss: 0.999962] [G loss: 1.000070]\n",
            "2265 [D loss: 0.999976] [G loss: 1.000095]\n",
            "2266 [D loss: 1.000005] [G loss: 1.000091]\n",
            "2267 [D loss: 0.999977] [G loss: 1.000061]\n",
            "2268 [D loss: 0.999969] [G loss: 1.000079]\n",
            "2269 [D loss: 0.999982] [G loss: 1.000101]\n",
            "2270 [D loss: 0.999976] [G loss: 1.000071]\n",
            "2271 [D loss: 0.999963] [G loss: 1.000055]\n",
            "2272 [D loss: 0.999950] [G loss: 1.000072]\n",
            "2273 [D loss: 0.999921] [G loss: 1.000072]\n",
            "2274 [D loss: 0.999964] [G loss: 1.000090]\n",
            "2275 [D loss: 0.999955] [G loss: 1.000087]\n",
            "2276 [D loss: 0.999989] [G loss: 1.000064]\n",
            "2277 [D loss: 0.999959] [G loss: 1.000066]\n",
            "2278 [D loss: 0.999951] [G loss: 1.000041]\n",
            "2279 [D loss: 0.999982] [G loss: 1.000074]\n",
            "2280 [D loss: 0.999944] [G loss: 1.000077]\n",
            "2281 [D loss: 0.999969] [G loss: 1.000050]\n",
            "2282 [D loss: 0.999976] [G loss: 1.000086]\n",
            "2283 [D loss: 0.999950] [G loss: 1.000084]\n",
            "2284 [D loss: 0.999934] [G loss: 1.000051]\n",
            "2285 [D loss: 1.000032] [G loss: 1.000060]\n",
            "2286 [D loss: 0.999962] [G loss: 1.000076]\n",
            "2287 [D loss: 0.999971] [G loss: 1.000074]\n",
            "2288 [D loss: 0.999972] [G loss: 1.000076]\n",
            "2289 [D loss: 0.999965] [G loss: 1.000066]\n",
            "2290 [D loss: 0.999968] [G loss: 1.000070]\n",
            "2291 [D loss: 0.999963] [G loss: 1.000070]\n",
            "2292 [D loss: 0.999969] [G loss: 1.000069]\n",
            "2293 [D loss: 0.999947] [G loss: 1.000065]\n",
            "2294 [D loss: 0.999990] [G loss: 1.000074]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2295 [D loss: 0.999952] [G loss: 1.000066]\n",
            "2296 [D loss: 0.999946] [G loss: 1.000043]\n",
            "2297 [D loss: 0.999965] [G loss: 1.000072]\n",
            "2298 [D loss: 1.000001] [G loss: 1.000099]\n",
            "2299 [D loss: 0.999960] [G loss: 1.000073]\n",
            "2300 [D loss: 0.999975] [G loss: 1.000062]\n",
            "2301 [D loss: 0.999963] [G loss: 1.000061]\n",
            "2302 [D loss: 0.999997] [G loss: 1.000037]\n",
            "2303 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2304 [D loss: 0.999957] [G loss: 1.000041]\n",
            "2305 [D loss: 0.999964] [G loss: 1.000066]\n",
            "2306 [D loss: 0.999986] [G loss: 1.000033]\n",
            "2307 [D loss: 0.999977] [G loss: 1.000086]\n",
            "2308 [D loss: 0.999972] [G loss: 1.000083]\n",
            "2309 [D loss: 0.999921] [G loss: 1.000049]\n",
            "2310 [D loss: 1.000009] [G loss: 1.000096]\n",
            "2311 [D loss: 0.999984] [G loss: 1.000081]\n",
            "2312 [D loss: 0.999960] [G loss: 1.000083]\n",
            "2313 [D loss: 0.999972] [G loss: 1.000092]\n",
            "2314 [D loss: 0.999962] [G loss: 1.000101]\n",
            "2315 [D loss: 0.999986] [G loss: 1.000097]\n",
            "2316 [D loss: 0.999923] [G loss: 1.000071]\n",
            "2317 [D loss: 0.999962] [G loss: 1.000030]\n",
            "2318 [D loss: 0.999954] [G loss: 1.000083]\n",
            "2319 [D loss: 1.000022] [G loss: 1.000114]\n",
            "2320 [D loss: 0.999966] [G loss: 1.000081]\n",
            "2321 [D loss: 0.999957] [G loss: 1.000079]\n",
            "2322 [D loss: 0.999953] [G loss: 1.000101]\n",
            "2323 [D loss: 0.999975] [G loss: 1.000073]\n",
            "2324 [D loss: 0.999929] [G loss: 1.000059]\n",
            "2325 [D loss: 0.999986] [G loss: 1.000085]\n",
            "2326 [D loss: 0.999978] [G loss: 1.000074]\n",
            "2327 [D loss: 0.999962] [G loss: 1.000057]\n",
            "2328 [D loss: 0.999943] [G loss: 1.000062]\n",
            "2329 [D loss: 0.999972] [G loss: 1.000051]\n",
            "2330 [D loss: 0.999988] [G loss: 1.000084]\n",
            "2331 [D loss: 0.999985] [G loss: 1.000051]\n",
            "2332 [D loss: 1.000011] [G loss: 1.000074]\n",
            "2333 [D loss: 0.999962] [G loss: 1.000105]\n",
            "2334 [D loss: 0.999912] [G loss: 1.000051]\n",
            "2335 [D loss: 0.999971] [G loss: 1.000081]\n",
            "2336 [D loss: 0.999976] [G loss: 1.000067]\n",
            "2337 [D loss: 0.999976] [G loss: 1.000065]\n",
            "2338 [D loss: 0.999972] [G loss: 1.000043]\n",
            "2339 [D loss: 0.999936] [G loss: 1.000081]\n",
            "2340 [D loss: 0.999985] [G loss: 1.000074]\n",
            "2341 [D loss: 0.999967] [G loss: 1.000081]\n",
            "2342 [D loss: 0.999958] [G loss: 1.000066]\n",
            "2343 [D loss: 0.999991] [G loss: 1.000061]\n",
            "2344 [D loss: 0.999965] [G loss: 1.000066]\n",
            "2345 [D loss: 0.999978] [G loss: 1.000095]\n",
            "2346 [D loss: 0.999984] [G loss: 1.000065]\n",
            "2347 [D loss: 0.999965] [G loss: 1.000070]\n",
            "2348 [D loss: 0.999953] [G loss: 1.000052]\n",
            "2349 [D loss: 0.999942] [G loss: 1.000055]\n",
            "2350 [D loss: 0.999992] [G loss: 1.000066]\n",
            "2351 [D loss: 0.999995] [G loss: 1.000062]\n",
            "2352 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2353 [D loss: 0.999982] [G loss: 1.000095]\n",
            "2354 [D loss: 0.999971] [G loss: 1.000093]\n",
            "2355 [D loss: 0.999921] [G loss: 1.000050]\n",
            "2356 [D loss: 0.999979] [G loss: 1.000060]\n",
            "2357 [D loss: 0.999984] [G loss: 1.000078]\n",
            "2358 [D loss: 0.999958] [G loss: 1.000098]\n",
            "2359 [D loss: 0.999964] [G loss: 1.000043]\n",
            "2360 [D loss: 0.999932] [G loss: 1.000066]\n",
            "2361 [D loss: 0.999994] [G loss: 1.000051]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2362 [D loss: 0.999981] [G loss: 1.000048]\n",
            "2363 [D loss: 0.999981] [G loss: 1.000077]\n",
            "2364 [D loss: 0.999973] [G loss: 1.000073]\n",
            "2365 [D loss: 0.999937] [G loss: 1.000058]\n",
            "2366 [D loss: 0.999962] [G loss: 1.000041]\n",
            "2367 [D loss: 0.999969] [G loss: 1.000085]\n",
            "2368 [D loss: 0.999976] [G loss: 1.000083]\n",
            "2369 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2370 [D loss: 0.999995] [G loss: 1.000067]\n",
            "2371 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2372 [D loss: 0.999955] [G loss: 1.000052]\n",
            "2373 [D loss: 0.999949] [G loss: 1.000091]\n",
            "2374 [D loss: 0.999945] [G loss: 1.000095]\n",
            "2375 [D loss: 0.999955] [G loss: 1.000075]\n",
            "2376 [D loss: 1.000004] [G loss: 1.000055]\n",
            "2377 [D loss: 1.000021] [G loss: 1.000101]\n",
            "2378 [D loss: 0.999929] [G loss: 1.000054]\n",
            "2379 [D loss: 0.999946] [G loss: 1.000048]\n",
            "2380 [D loss: 0.999977] [G loss: 1.000053]\n",
            "2381 [D loss: 0.999969] [G loss: 1.000075]\n",
            "2382 [D loss: 0.999996] [G loss: 1.000075]\n",
            "2383 [D loss: 0.999993] [G loss: 1.000048]\n",
            "2384 [D loss: 0.999959] [G loss: 1.000063]\n",
            "2385 [D loss: 0.999960] [G loss: 1.000061]\n",
            "2386 [D loss: 0.999973] [G loss: 1.000083]\n",
            "2387 [D loss: 0.999980] [G loss: 1.000099]\n",
            "2388 [D loss: 0.999971] [G loss: 1.000059]\n",
            "2389 [D loss: 0.999987] [G loss: 1.000073]\n",
            "2390 [D loss: 0.999945] [G loss: 1.000043]\n",
            "2391 [D loss: 0.999941] [G loss: 1.000057]\n",
            "2392 [D loss: 0.999965] [G loss: 1.000091]\n",
            "2393 [D loss: 0.999966] [G loss: 1.000058]\n",
            "2394 [D loss: 0.999922] [G loss: 1.000068]\n",
            "2395 [D loss: 0.999975] [G loss: 1.000098]\n",
            "2396 [D loss: 0.999953] [G loss: 1.000061]\n",
            "2397 [D loss: 0.999979] [G loss: 1.000074]\n",
            "2398 [D loss: 0.999961] [G loss: 1.000054]\n",
            "2399 [D loss: 0.999959] [G loss: 1.000042]\n",
            "2400 [D loss: 0.999988] [G loss: 1.000067]\n",
            "2401 [D loss: 0.999943] [G loss: 1.000087]\n",
            "2402 [D loss: 0.999947] [G loss: 1.000056]\n",
            "2403 [D loss: 0.999988] [G loss: 1.000066]\n",
            "2404 [D loss: 0.999988] [G loss: 1.000069]\n",
            "2405 [D loss: 0.999928] [G loss: 1.000028]\n",
            "2406 [D loss: 0.999986] [G loss: 1.000081]\n",
            "2407 [D loss: 0.999962] [G loss: 1.000079]\n",
            "2408 [D loss: 0.999954] [G loss: 1.000061]\n",
            "2409 [D loss: 0.999949] [G loss: 1.000059]\n",
            "2410 [D loss: 0.999952] [G loss: 1.000103]\n",
            "2411 [D loss: 1.000006] [G loss: 1.000053]\n",
            "2412 [D loss: 0.999966] [G loss: 1.000060]\n",
            "2413 [D loss: 0.999970] [G loss: 1.000082]\n",
            "2414 [D loss: 0.999968] [G loss: 1.000046]\n",
            "2415 [D loss: 0.999969] [G loss: 1.000092]\n",
            "2416 [D loss: 0.999981] [G loss: 1.000050]\n",
            "2417 [D loss: 0.999956] [G loss: 1.000065]\n",
            "2418 [D loss: 1.000006] [G loss: 1.000074]\n",
            "2419 [D loss: 1.000010] [G loss: 1.000108]\n",
            "2420 [D loss: 0.999988] [G loss: 1.000069]\n",
            "2421 [D loss: 0.999975] [G loss: 1.000054]\n",
            "2422 [D loss: 0.999962] [G loss: 1.000049]\n",
            "2423 [D loss: 1.000003] [G loss: 1.000077]\n",
            "2424 [D loss: 0.999975] [G loss: 1.000066]\n",
            "2425 [D loss: 0.999936] [G loss: 1.000060]\n",
            "2426 [D loss: 0.999988] [G loss: 1.000108]\n",
            "2427 [D loss: 0.999979] [G loss: 1.000075]\n",
            "2428 [D loss: 0.999958] [G loss: 1.000045]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2429 [D loss: 0.999953] [G loss: 1.000073]\n",
            "2430 [D loss: 0.999943] [G loss: 1.000040]\n",
            "2431 [D loss: 0.999987] [G loss: 1.000090]\n",
            "2432 [D loss: 0.999983] [G loss: 1.000061]\n",
            "2433 [D loss: 0.999947] [G loss: 1.000103]\n",
            "2434 [D loss: 0.999989] [G loss: 1.000067]\n",
            "2435 [D loss: 0.999961] [G loss: 1.000082]\n",
            "2436 [D loss: 0.999918] [G loss: 1.000100]\n",
            "2437 [D loss: 0.999965] [G loss: 1.000080]\n",
            "2438 [D loss: 0.999937] [G loss: 1.000082]\n",
            "2439 [D loss: 0.999975] [G loss: 1.000065]\n",
            "2440 [D loss: 0.999959] [G loss: 1.000044]\n",
            "2441 [D loss: 0.999991] [G loss: 1.000052]\n",
            "2442 [D loss: 0.999961] [G loss: 1.000069]\n",
            "2443 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2444 [D loss: 1.000002] [G loss: 1.000059]\n",
            "2445 [D loss: 0.999935] [G loss: 1.000094]\n",
            "2446 [D loss: 0.999965] [G loss: 1.000080]\n",
            "2447 [D loss: 0.999961] [G loss: 1.000080]\n",
            "2448 [D loss: 0.999972] [G loss: 1.000069]\n",
            "2449 [D loss: 0.999952] [G loss: 1.000047]\n",
            "2450 [D loss: 0.999933] [G loss: 1.000052]\n",
            "2451 [D loss: 0.999950] [G loss: 1.000046]\n",
            "2452 [D loss: 0.999990] [G loss: 1.000063]\n",
            "2453 [D loss: 0.999978] [G loss: 1.000055]\n",
            "2454 [D loss: 0.999961] [G loss: 1.000052]\n",
            "2455 [D loss: 0.999982] [G loss: 1.000091]\n",
            "2456 [D loss: 0.999968] [G loss: 1.000071]\n",
            "2457 [D loss: 0.999983] [G loss: 1.000065]\n",
            "2458 [D loss: 0.999941] [G loss: 1.000053]\n",
            "2459 [D loss: 0.999964] [G loss: 1.000068]\n",
            "2460 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2461 [D loss: 0.999975] [G loss: 1.000077]\n",
            "2462 [D loss: 0.999958] [G loss: 1.000066]\n",
            "2463 [D loss: 0.999968] [G loss: 1.000070]\n",
            "2464 [D loss: 0.999994] [G loss: 1.000086]\n",
            "2465 [D loss: 0.999958] [G loss: 1.000067]\n",
            "2466 [D loss: 1.000013] [G loss: 1.000062]\n",
            "2467 [D loss: 0.999942] [G loss: 1.000063]\n",
            "2468 [D loss: 0.999959] [G loss: 1.000054]\n",
            "2469 [D loss: 0.999963] [G loss: 1.000035]\n",
            "2470 [D loss: 0.999986] [G loss: 1.000079]\n",
            "2471 [D loss: 0.999994] [G loss: 1.000030]\n",
            "2472 [D loss: 0.999944] [G loss: 1.000089]\n",
            "2473 [D loss: 0.999977] [G loss: 1.000067]\n",
            "2474 [D loss: 0.999946] [G loss: 1.000081]\n",
            "2475 [D loss: 0.999947] [G loss: 1.000077]\n",
            "2476 [D loss: 0.999924] [G loss: 1.000039]\n",
            "2477 [D loss: 0.999952] [G loss: 1.000063]\n",
            "2478 [D loss: 0.999998] [G loss: 1.000066]\n",
            "2479 [D loss: 0.999968] [G loss: 1.000091]\n",
            "2480 [D loss: 0.999931] [G loss: 1.000070]\n",
            "2481 [D loss: 0.999946] [G loss: 1.000061]\n",
            "2482 [D loss: 0.999983] [G loss: 1.000072]\n",
            "2483 [D loss: 0.999965] [G loss: 1.000064]\n",
            "2484 [D loss: 0.999983] [G loss: 1.000072]\n",
            "2485 [D loss: 0.999992] [G loss: 1.000056]\n",
            "2486 [D loss: 1.000003] [G loss: 1.000066]\n",
            "2487 [D loss: 0.999961] [G loss: 1.000061]\n",
            "2488 [D loss: 0.999986] [G loss: 1.000071]\n",
            "2489 [D loss: 0.999947] [G loss: 1.000076]\n",
            "2490 [D loss: 0.999967] [G loss: 1.000070]\n",
            "2491 [D loss: 0.999962] [G loss: 1.000056]\n",
            "2492 [D loss: 0.999953] [G loss: 1.000056]\n",
            "2493 [D loss: 0.999958] [G loss: 1.000053]\n",
            "2494 [D loss: 0.999956] [G loss: 1.000076]\n",
            "2495 [D loss: 0.999968] [G loss: 1.000043]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2496 [D loss: 0.999978] [G loss: 1.000027]\n",
            "2497 [D loss: 0.999984] [G loss: 1.000092]\n",
            "2498 [D loss: 0.999994] [G loss: 1.000076]\n",
            "2499 [D loss: 0.999966] [G loss: 1.000061]\n",
            "2500 [D loss: 0.999969] [G loss: 1.000082]\n",
            "2501 [D loss: 0.999965] [G loss: 1.000054]\n",
            "2502 [D loss: 0.999919] [G loss: 1.000064]\n",
            "2503 [D loss: 0.999969] [G loss: 1.000075]\n",
            "2504 [D loss: 0.999977] [G loss: 1.000103]\n",
            "2505 [D loss: 0.999956] [G loss: 1.000083]\n",
            "2506 [D loss: 0.999967] [G loss: 1.000070]\n",
            "2507 [D loss: 0.999964] [G loss: 1.000059]\n",
            "2508 [D loss: 0.999993] [G loss: 1.000085]\n",
            "2509 [D loss: 0.999980] [G loss: 1.000088]\n",
            "2510 [D loss: 0.999964] [G loss: 1.000037]\n",
            "2511 [D loss: 0.999948] [G loss: 1.000089]\n",
            "2512 [D loss: 0.999986] [G loss: 1.000066]\n",
            "2513 [D loss: 0.999945] [G loss: 1.000075]\n",
            "2514 [D loss: 0.999987] [G loss: 1.000084]\n",
            "2515 [D loss: 0.999936] [G loss: 1.000054]\n",
            "2516 [D loss: 0.999957] [G loss: 1.000060]\n",
            "2517 [D loss: 0.999974] [G loss: 1.000064]\n",
            "2518 [D loss: 0.999968] [G loss: 1.000080]\n",
            "2519 [D loss: 0.999960] [G loss: 1.000052]\n",
            "2520 [D loss: 0.999953] [G loss: 1.000053]\n",
            "2521 [D loss: 0.999982] [G loss: 1.000067]\n",
            "2522 [D loss: 0.999969] [G loss: 1.000052]\n",
            "2523 [D loss: 0.999961] [G loss: 1.000063]\n",
            "2524 [D loss: 0.999937] [G loss: 1.000059]\n",
            "2525 [D loss: 0.999982] [G loss: 1.000056]\n",
            "2526 [D loss: 0.999971] [G loss: 1.000076]\n",
            "2527 [D loss: 0.999959] [G loss: 1.000060]\n",
            "2528 [D loss: 0.999991] [G loss: 1.000056]\n",
            "2529 [D loss: 0.999971] [G loss: 1.000076]\n",
            "2530 [D loss: 0.999982] [G loss: 1.000098]\n",
            "2531 [D loss: 0.999974] [G loss: 1.000069]\n",
            "2532 [D loss: 0.999952] [G loss: 1.000044]\n",
            "2533 [D loss: 0.999990] [G loss: 1.000057]\n",
            "2534 [D loss: 0.999984] [G loss: 1.000049]\n",
            "2535 [D loss: 0.999995] [G loss: 1.000110]\n",
            "2536 [D loss: 0.999951] [G loss: 1.000047]\n",
            "2537 [D loss: 0.999974] [G loss: 1.000073]\n",
            "2538 [D loss: 1.000014] [G loss: 1.000061]\n",
            "2539 [D loss: 0.999959] [G loss: 1.000065]\n",
            "2540 [D loss: 0.999997] [G loss: 1.000067]\n",
            "2541 [D loss: 0.999954] [G loss: 1.000055]\n",
            "2542 [D loss: 0.999942] [G loss: 1.000074]\n",
            "2543 [D loss: 0.999989] [G loss: 1.000055]\n",
            "2544 [D loss: 0.999960] [G loss: 1.000048]\n",
            "2545 [D loss: 0.999969] [G loss: 1.000051]\n",
            "2546 [D loss: 0.999985] [G loss: 1.000060]\n",
            "2547 [D loss: 0.999962] [G loss: 1.000052]\n",
            "2548 [D loss: 0.999963] [G loss: 1.000046]\n",
            "2549 [D loss: 0.999973] [G loss: 1.000072]\n",
            "2550 [D loss: 1.000010] [G loss: 1.000091]\n",
            "2551 [D loss: 1.000009] [G loss: 1.000030]\n",
            "2552 [D loss: 0.999968] [G loss: 1.000080]\n",
            "2553 [D loss: 0.999981] [G loss: 1.000078]\n",
            "2554 [D loss: 0.999945] [G loss: 1.000055]\n",
            "2555 [D loss: 0.999972] [G loss: 1.000050]\n",
            "2556 [D loss: 0.999982] [G loss: 1.000098]\n",
            "2557 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2558 [D loss: 0.999960] [G loss: 1.000043]\n",
            "2559 [D loss: 0.999937] [G loss: 1.000056]\n",
            "2560 [D loss: 0.999991] [G loss: 1.000078]\n",
            "2561 [D loss: 0.999957] [G loss: 1.000075]\n",
            "2562 [D loss: 0.999978] [G loss: 1.000026]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2563 [D loss: 1.000005] [G loss: 1.000063]\n",
            "2564 [D loss: 0.999989] [G loss: 1.000034]\n",
            "2565 [D loss: 0.999962] [G loss: 1.000050]\n",
            "2566 [D loss: 0.999962] [G loss: 1.000059]\n",
            "2567 [D loss: 1.000002] [G loss: 1.000043]\n",
            "2568 [D loss: 0.999976] [G loss: 1.000031]\n",
            "2569 [D loss: 0.999981] [G loss: 1.000077]\n",
            "2570 [D loss: 0.999995] [G loss: 1.000069]\n",
            "2571 [D loss: 0.999984] [G loss: 1.000048]\n",
            "2572 [D loss: 0.999946] [G loss: 1.000055]\n",
            "2573 [D loss: 0.999953] [G loss: 1.000097]\n",
            "2574 [D loss: 0.999974] [G loss: 1.000049]\n",
            "2575 [D loss: 0.999978] [G loss: 1.000032]\n",
            "2576 [D loss: 0.999937] [G loss: 1.000082]\n",
            "2577 [D loss: 0.999967] [G loss: 1.000079]\n",
            "2578 [D loss: 0.999949] [G loss: 1.000069]\n",
            "2579 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2580 [D loss: 0.999942] [G loss: 1.000073]\n",
            "2581 [D loss: 0.999988] [G loss: 1.000072]\n",
            "2582 [D loss: 0.999977] [G loss: 1.000053]\n",
            "2583 [D loss: 0.999947] [G loss: 1.000067]\n",
            "2584 [D loss: 0.999977] [G loss: 1.000095]\n",
            "2585 [D loss: 0.999969] [G loss: 1.000081]\n",
            "2586 [D loss: 0.999950] [G loss: 1.000050]\n",
            "2587 [D loss: 0.999998] [G loss: 1.000085]\n",
            "2588 [D loss: 0.999951] [G loss: 1.000055]\n",
            "2589 [D loss: 0.999959] [G loss: 1.000063]\n",
            "2590 [D loss: 0.999948] [G loss: 1.000081]\n",
            "2591 [D loss: 0.999980] [G loss: 1.000084]\n",
            "2592 [D loss: 0.999975] [G loss: 1.000035]\n",
            "2593 [D loss: 0.999971] [G loss: 1.000052]\n",
            "2594 [D loss: 0.999963] [G loss: 1.000059]\n",
            "2595 [D loss: 0.999940] [G loss: 1.000063]\n",
            "2596 [D loss: 0.999997] [G loss: 1.000049]\n",
            "2597 [D loss: 0.999981] [G loss: 1.000085]\n",
            "2598 [D loss: 0.999947] [G loss: 1.000077]\n",
            "2599 [D loss: 0.999992] [G loss: 1.000073]\n",
            "2600 [D loss: 0.999968] [G loss: 1.000069]\n",
            "2601 [D loss: 0.999970] [G loss: 1.000071]\n",
            "2602 [D loss: 0.999971] [G loss: 1.000047]\n",
            "2603 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2604 [D loss: 0.999970] [G loss: 1.000071]\n",
            "2605 [D loss: 0.999961] [G loss: 1.000052]\n",
            "2606 [D loss: 0.999967] [G loss: 1.000081]\n",
            "2607 [D loss: 0.999957] [G loss: 1.000050]\n",
            "2608 [D loss: 0.999987] [G loss: 1.000022]\n",
            "2609 [D loss: 0.999965] [G loss: 1.000066]\n",
            "2610 [D loss: 0.999940] [G loss: 1.000077]\n",
            "2611 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2612 [D loss: 0.999994] [G loss: 1.000057]\n",
            "2613 [D loss: 0.999957] [G loss: 1.000027]\n",
            "2614 [D loss: 0.999965] [G loss: 1.000095]\n",
            "2615 [D loss: 0.999986] [G loss: 1.000053]\n",
            "2616 [D loss: 0.999988] [G loss: 1.000055]\n",
            "2617 [D loss: 0.999952] [G loss: 1.000039]\n",
            "2618 [D loss: 0.999968] [G loss: 1.000077]\n",
            "2619 [D loss: 0.999975] [G loss: 1.000038]\n",
            "2620 [D loss: 0.999956] [G loss: 1.000066]\n",
            "2621 [D loss: 0.999960] [G loss: 1.000060]\n",
            "2622 [D loss: 0.999956] [G loss: 1.000034]\n",
            "2623 [D loss: 0.999965] [G loss: 1.000037]\n",
            "2624 [D loss: 0.999952] [G loss: 1.000080]\n",
            "2625 [D loss: 0.999957] [G loss: 1.000064]\n",
            "2626 [D loss: 0.999967] [G loss: 1.000031]\n",
            "2627 [D loss: 0.999941] [G loss: 1.000067]\n",
            "2628 [D loss: 0.999916] [G loss: 1.000065]\n",
            "2629 [D loss: 0.999960] [G loss: 1.000056]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2630 [D loss: 1.000019] [G loss: 1.000031]\n",
            "2631 [D loss: 0.999981] [G loss: 1.000047]\n",
            "2632 [D loss: 0.999984] [G loss: 1.000072]\n",
            "2633 [D loss: 1.000008] [G loss: 1.000036]\n",
            "2634 [D loss: 0.999971] [G loss: 1.000117]\n",
            "2635 [D loss: 0.999927] [G loss: 1.000060]\n",
            "2636 [D loss: 0.999970] [G loss: 1.000041]\n",
            "2637 [D loss: 0.999973] [G loss: 1.000101]\n",
            "2638 [D loss: 0.999983] [G loss: 1.000070]\n",
            "2639 [D loss: 0.999960] [G loss: 1.000094]\n",
            "2640 [D loss: 0.999984] [G loss: 1.000063]\n",
            "2641 [D loss: 0.999956] [G loss: 1.000119]\n",
            "2642 [D loss: 0.999980] [G loss: 1.000062]\n",
            "2643 [D loss: 0.999975] [G loss: 1.000067]\n",
            "2644 [D loss: 0.999977] [G loss: 1.000051]\n",
            "2645 [D loss: 0.999931] [G loss: 1.000055]\n",
            "2646 [D loss: 0.999955] [G loss: 1.000068]\n",
            "2647 [D loss: 0.999962] [G loss: 1.000095]\n",
            "2648 [D loss: 0.999993] [G loss: 1.000099]\n",
            "2649 [D loss: 0.999992] [G loss: 1.000075]\n",
            "2650 [D loss: 0.999947] [G loss: 1.000091]\n",
            "2651 [D loss: 0.999950] [G loss: 1.000065]\n",
            "2652 [D loss: 0.999945] [G loss: 1.000090]\n",
            "2653 [D loss: 0.999946] [G loss: 1.000061]\n",
            "2654 [D loss: 0.999966] [G loss: 1.000061]\n",
            "2655 [D loss: 0.999963] [G loss: 1.000053]\n",
            "2656 [D loss: 0.999957] [G loss: 1.000069]\n",
            "2657 [D loss: 0.999992] [G loss: 1.000078]\n",
            "2658 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2659 [D loss: 0.999953] [G loss: 1.000096]\n",
            "2660 [D loss: 0.999957] [G loss: 1.000102]\n",
            "2661 [D loss: 0.999997] [G loss: 1.000041]\n",
            "2662 [D loss: 0.999943] [G loss: 1.000059]\n",
            "2663 [D loss: 0.999980] [G loss: 1.000073]\n",
            "2664 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2665 [D loss: 0.999955] [G loss: 1.000087]\n",
            "2666 [D loss: 1.000001] [G loss: 1.000066]\n",
            "2667 [D loss: 0.999950] [G loss: 1.000077]\n",
            "2668 [D loss: 0.999981] [G loss: 1.000045]\n",
            "2669 [D loss: 0.999958] [G loss: 1.000069]\n",
            "2670 [D loss: 0.999939] [G loss: 1.000069]\n",
            "2671 [D loss: 0.999949] [G loss: 1.000067]\n",
            "2672 [D loss: 0.999962] [G loss: 1.000040]\n",
            "2673 [D loss: 0.999990] [G loss: 1.000093]\n",
            "2674 [D loss: 0.999925] [G loss: 1.000104]\n",
            "2675 [D loss: 0.999958] [G loss: 1.000057]\n",
            "2676 [D loss: 0.999948] [G loss: 1.000049]\n",
            "2677 [D loss: 1.000004] [G loss: 1.000063]\n",
            "2678 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2679 [D loss: 0.999936] [G loss: 1.000078]\n",
            "2680 [D loss: 0.999969] [G loss: 1.000090]\n",
            "2681 [D loss: 0.999969] [G loss: 1.000069]\n",
            "2682 [D loss: 0.999952] [G loss: 1.000067]\n",
            "2683 [D loss: 0.999972] [G loss: 1.000022]\n",
            "2684 [D loss: 0.999982] [G loss: 1.000068]\n",
            "2685 [D loss: 0.999953] [G loss: 1.000065]\n",
            "2686 [D loss: 0.999978] [G loss: 1.000051]\n",
            "2687 [D loss: 0.999961] [G loss: 1.000099]\n",
            "2688 [D loss: 0.999928] [G loss: 1.000075]\n",
            "2689 [D loss: 0.999984] [G loss: 1.000074]\n",
            "2690 [D loss: 0.999999] [G loss: 1.000041]\n",
            "2691 [D loss: 0.999947] [G loss: 1.000077]\n",
            "2692 [D loss: 0.999972] [G loss: 1.000046]\n",
            "2693 [D loss: 0.999960] [G loss: 1.000055]\n",
            "2694 [D loss: 0.999986] [G loss: 1.000089]\n",
            "2695 [D loss: 0.999957] [G loss: 1.000045]\n",
            "2696 [D loss: 0.999975] [G loss: 1.000072]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2697 [D loss: 0.999969] [G loss: 1.000054]\n",
            "2698 [D loss: 0.999957] [G loss: 1.000051]\n",
            "2699 [D loss: 0.999954] [G loss: 1.000054]\n",
            "2700 [D loss: 0.999950] [G loss: 1.000049]\n",
            "2701 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2702 [D loss: 0.999969] [G loss: 1.000056]\n",
            "2703 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2704 [D loss: 0.999962] [G loss: 1.000047]\n",
            "2705 [D loss: 0.999943] [G loss: 1.000041]\n",
            "2706 [D loss: 0.999981] [G loss: 1.000054]\n",
            "2707 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2708 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2709 [D loss: 0.999992] [G loss: 1.000095]\n",
            "2710 [D loss: 0.999975] [G loss: 1.000059]\n",
            "2711 [D loss: 0.999993] [G loss: 1.000063]\n",
            "2712 [D loss: 0.999950] [G loss: 1.000026]\n",
            "2713 [D loss: 0.999946] [G loss: 1.000070]\n",
            "2714 [D loss: 0.999954] [G loss: 1.000068]\n",
            "2715 [D loss: 0.999962] [G loss: 1.000076]\n",
            "2716 [D loss: 0.999971] [G loss: 1.000094]\n",
            "2717 [D loss: 0.999970] [G loss: 1.000087]\n",
            "2718 [D loss: 0.999965] [G loss: 1.000049]\n",
            "2719 [D loss: 0.999946] [G loss: 1.000057]\n",
            "2720 [D loss: 0.999998] [G loss: 1.000059]\n",
            "2721 [D loss: 0.999968] [G loss: 1.000097]\n",
            "2722 [D loss: 0.999948] [G loss: 1.000053]\n",
            "2723 [D loss: 0.999970] [G loss: 1.000080]\n",
            "2724 [D loss: 0.999965] [G loss: 1.000076]\n",
            "2725 [D loss: 0.999967] [G loss: 1.000072]\n",
            "2726 [D loss: 0.999959] [G loss: 1.000077]\n",
            "2727 [D loss: 0.999954] [G loss: 1.000064]\n",
            "2728 [D loss: 0.999975] [G loss: 1.000070]\n",
            "2729 [D loss: 0.999979] [G loss: 1.000098]\n",
            "2730 [D loss: 0.999952] [G loss: 1.000065]\n",
            "2731 [D loss: 0.999954] [G loss: 1.000082]\n",
            "2732 [D loss: 0.999983] [G loss: 1.000043]\n",
            "2733 [D loss: 0.999963] [G loss: 1.000048]\n",
            "2734 [D loss: 0.999954] [G loss: 1.000068]\n",
            "2735 [D loss: 0.999947] [G loss: 1.000057]\n",
            "2736 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2737 [D loss: 0.999932] [G loss: 1.000076]\n",
            "2738 [D loss: 0.999972] [G loss: 1.000090]\n",
            "2739 [D loss: 0.999989] [G loss: 1.000081]\n",
            "2740 [D loss: 0.999972] [G loss: 1.000082]\n",
            "2741 [D loss: 0.999991] [G loss: 1.000066]\n",
            "2742 [D loss: 0.999970] [G loss: 1.000072]\n",
            "2743 [D loss: 0.999996] [G loss: 1.000091]\n",
            "2744 [D loss: 0.999991] [G loss: 1.000065]\n",
            "2745 [D loss: 1.000010] [G loss: 1.000065]\n",
            "2746 [D loss: 0.999916] [G loss: 1.000064]\n",
            "2747 [D loss: 0.999967] [G loss: 1.000047]\n",
            "2748 [D loss: 0.999975] [G loss: 1.000032]\n",
            "2749 [D loss: 1.000004] [G loss: 1.000090]\n",
            "2750 [D loss: 0.999990] [G loss: 1.000066]\n",
            "2751 [D loss: 0.999940] [G loss: 1.000070]\n",
            "2752 [D loss: 0.999976] [G loss: 1.000063]\n",
            "2753 [D loss: 0.999998] [G loss: 1.000077]\n",
            "2754 [D loss: 0.999985] [G loss: 1.000055]\n",
            "2755 [D loss: 0.999969] [G loss: 1.000082]\n",
            "2756 [D loss: 0.999935] [G loss: 1.000061]\n",
            "2757 [D loss: 0.999952] [G loss: 1.000105]\n",
            "2758 [D loss: 0.999994] [G loss: 1.000033]\n",
            "2759 [D loss: 0.999993] [G loss: 1.000040]\n",
            "2760 [D loss: 0.999950] [G loss: 1.000075]\n",
            "2761 [D loss: 0.999950] [G loss: 1.000079]\n",
            "2762 [D loss: 0.999959] [G loss: 1.000074]\n",
            "2763 [D loss: 0.999950] [G loss: 1.000066]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2764 [D loss: 0.999979] [G loss: 1.000084]\n",
            "2765 [D loss: 0.999952] [G loss: 1.000085]\n",
            "2766 [D loss: 0.999970] [G loss: 1.000059]\n",
            "2767 [D loss: 0.999999] [G loss: 1.000071]\n",
            "2768 [D loss: 0.999956] [G loss: 1.000050]\n",
            "2769 [D loss: 0.999969] [G loss: 1.000078]\n",
            "2770 [D loss: 0.999974] [G loss: 1.000077]\n",
            "2771 [D loss: 0.999968] [G loss: 1.000056]\n",
            "2772 [D loss: 0.999970] [G loss: 1.000048]\n",
            "2773 [D loss: 0.999985] [G loss: 1.000067]\n",
            "2774 [D loss: 0.999972] [G loss: 1.000108]\n",
            "2775 [D loss: 0.999944] [G loss: 1.000109]\n",
            "2776 [D loss: 0.999971] [G loss: 1.000075]\n",
            "2777 [D loss: 0.999949] [G loss: 1.000069]\n",
            "2778 [D loss: 0.999962] [G loss: 1.000053]\n",
            "2779 [D loss: 0.999966] [G loss: 1.000060]\n",
            "2780 [D loss: 0.999942] [G loss: 1.000090]\n",
            "2781 [D loss: 0.999976] [G loss: 1.000045]\n",
            "2782 [D loss: 0.999985] [G loss: 1.000066]\n",
            "2783 [D loss: 0.999977] [G loss: 1.000068]\n",
            "2784 [D loss: 0.999982] [G loss: 1.000056]\n",
            "2785 [D loss: 0.999981] [G loss: 1.000075]\n",
            "2786 [D loss: 0.999944] [G loss: 1.000084]\n",
            "2787 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2788 [D loss: 0.999999] [G loss: 1.000061]\n",
            "2789 [D loss: 0.999990] [G loss: 1.000086]\n",
            "2790 [D loss: 0.999969] [G loss: 1.000086]\n",
            "2791 [D loss: 0.999960] [G loss: 1.000061]\n",
            "2792 [D loss: 0.999967] [G loss: 1.000081]\n",
            "2793 [D loss: 0.999983] [G loss: 1.000052]\n",
            "2794 [D loss: 0.999972] [G loss: 1.000043]\n",
            "2795 [D loss: 0.999984] [G loss: 1.000080]\n",
            "2796 [D loss: 0.999962] [G loss: 1.000080]\n",
            "2797 [D loss: 0.999957] [G loss: 1.000083]\n",
            "2798 [D loss: 0.999933] [G loss: 1.000059]\n",
            "2799 [D loss: 0.999973] [G loss: 1.000082]\n",
            "2800 [D loss: 0.999976] [G loss: 1.000062]\n",
            "2801 [D loss: 0.999959] [G loss: 1.000076]\n",
            "2802 [D loss: 0.999967] [G loss: 1.000046]\n",
            "2803 [D loss: 0.999980] [G loss: 1.000069]\n",
            "2804 [D loss: 0.999969] [G loss: 1.000057]\n",
            "2805 [D loss: 0.999952] [G loss: 1.000046]\n",
            "2806 [D loss: 0.999954] [G loss: 1.000068]\n",
            "2807 [D loss: 0.999959] [G loss: 1.000049]\n",
            "2808 [D loss: 0.999980] [G loss: 1.000099]\n",
            "2809 [D loss: 0.999974] [G loss: 1.000082]\n",
            "2810 [D loss: 0.999962] [G loss: 1.000082]\n",
            "2811 [D loss: 0.999975] [G loss: 1.000084]\n",
            "2812 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2813 [D loss: 0.999982] [G loss: 1.000055]\n",
            "2814 [D loss: 0.999923] [G loss: 1.000087]\n",
            "2815 [D loss: 0.999979] [G loss: 1.000106]\n",
            "2816 [D loss: 0.999954] [G loss: 1.000063]\n",
            "2817 [D loss: 0.999953] [G loss: 1.000062]\n",
            "2818 [D loss: 0.999987] [G loss: 1.000057]\n",
            "2819 [D loss: 0.999938] [G loss: 1.000094]\n",
            "2820 [D loss: 0.999965] [G loss: 1.000086]\n",
            "2821 [D loss: 0.999976] [G loss: 1.000103]\n",
            "2822 [D loss: 0.999981] [G loss: 1.000050]\n",
            "2823 [D loss: 0.999973] [G loss: 1.000068]\n",
            "2824 [D loss: 0.999974] [G loss: 1.000028]\n",
            "2825 [D loss: 0.999971] [G loss: 1.000020]\n",
            "2826 [D loss: 0.999970] [G loss: 1.000069]\n",
            "2827 [D loss: 0.999965] [G loss: 1.000094]\n",
            "2828 [D loss: 0.999992] [G loss: 1.000075]\n",
            "2829 [D loss: 0.999960] [G loss: 1.000088]\n",
            "2830 [D loss: 0.999946] [G loss: 1.000043]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2831 [D loss: 0.999976] [G loss: 1.000057]\n",
            "2832 [D loss: 0.999989] [G loss: 1.000074]\n",
            "2833 [D loss: 0.999994] [G loss: 1.000052]\n",
            "2834 [D loss: 0.999932] [G loss: 1.000086]\n",
            "2835 [D loss: 0.999964] [G loss: 1.000043]\n",
            "2836 [D loss: 0.999937] [G loss: 1.000069]\n",
            "2837 [D loss: 0.999986] [G loss: 1.000062]\n",
            "2838 [D loss: 0.999969] [G loss: 1.000058]\n",
            "2839 [D loss: 0.999981] [G loss: 1.000056]\n",
            "2840 [D loss: 0.999953] [G loss: 1.000086]\n",
            "2841 [D loss: 0.999937] [G loss: 1.000082]\n",
            "2842 [D loss: 0.999979] [G loss: 1.000092]\n",
            "2843 [D loss: 0.999984] [G loss: 1.000031]\n",
            "2844 [D loss: 0.999964] [G loss: 1.000078]\n",
            "2845 [D loss: 0.999975] [G loss: 1.000075]\n",
            "2846 [D loss: 0.999985] [G loss: 1.000076]\n",
            "2847 [D loss: 0.999963] [G loss: 1.000081]\n",
            "2848 [D loss: 0.999982] [G loss: 1.000060]\n",
            "2849 [D loss: 0.999960] [G loss: 1.000025]\n",
            "2850 [D loss: 0.999943] [G loss: 1.000042]\n",
            "2851 [D loss: 0.999963] [G loss: 1.000030]\n",
            "2852 [D loss: 0.999962] [G loss: 1.000056]\n",
            "2853 [D loss: 0.999959] [G loss: 1.000050]\n",
            "2854 [D loss: 0.999967] [G loss: 1.000069]\n",
            "2855 [D loss: 0.999962] [G loss: 1.000057]\n",
            "2856 [D loss: 0.999967] [G loss: 1.000086]\n",
            "2857 [D loss: 0.999956] [G loss: 1.000077]\n",
            "2858 [D loss: 0.999976] [G loss: 1.000064]\n",
            "2859 [D loss: 0.999960] [G loss: 1.000068]\n",
            "2860 [D loss: 0.999952] [G loss: 1.000079]\n",
            "2861 [D loss: 0.999977] [G loss: 1.000073]\n",
            "2862 [D loss: 0.999988] [G loss: 1.000109]\n",
            "2863 [D loss: 0.999960] [G loss: 1.000081]\n",
            "2864 [D loss: 0.999987] [G loss: 1.000090]\n",
            "2865 [D loss: 0.999936] [G loss: 1.000048]\n",
            "2866 [D loss: 0.999977] [G loss: 1.000096]\n",
            "2867 [D loss: 0.999976] [G loss: 1.000055]\n",
            "2868 [D loss: 0.999990] [G loss: 1.000073]\n",
            "2869 [D loss: 0.999958] [G loss: 1.000067]\n",
            "2870 [D loss: 0.999974] [G loss: 1.000046]\n",
            "2871 [D loss: 0.999967] [G loss: 1.000092]\n",
            "2872 [D loss: 0.999971] [G loss: 1.000052]\n",
            "2873 [D loss: 0.999942] [G loss: 1.000030]\n",
            "2874 [D loss: 0.999934] [G loss: 1.000038]\n",
            "2875 [D loss: 0.999960] [G loss: 1.000055]\n",
            "2876 [D loss: 0.999958] [G loss: 1.000099]\n",
            "2877 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2878 [D loss: 0.999986] [G loss: 1.000101]\n",
            "2879 [D loss: 0.999949] [G loss: 1.000060]\n",
            "2880 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2881 [D loss: 0.999962] [G loss: 1.000078]\n",
            "2882 [D loss: 0.999977] [G loss: 1.000048]\n",
            "2883 [D loss: 0.999962] [G loss: 1.000061]\n",
            "2884 [D loss: 0.999981] [G loss: 1.000057]\n",
            "2885 [D loss: 0.999954] [G loss: 1.000047]\n",
            "2886 [D loss: 0.999978] [G loss: 1.000090]\n",
            "2887 [D loss: 0.999984] [G loss: 1.000094]\n",
            "2888 [D loss: 0.999964] [G loss: 1.000056]\n",
            "2889 [D loss: 0.999964] [G loss: 1.000071]\n",
            "2890 [D loss: 0.999982] [G loss: 1.000070]\n",
            "2891 [D loss: 0.999983] [G loss: 1.000073]\n",
            "2892 [D loss: 0.999959] [G loss: 1.000039]\n",
            "2893 [D loss: 0.999950] [G loss: 1.000067]\n",
            "2894 [D loss: 0.999967] [G loss: 1.000051]\n",
            "2895 [D loss: 0.999948] [G loss: 1.000048]\n",
            "2896 [D loss: 0.999959] [G loss: 1.000058]\n",
            "2897 [D loss: 0.999973] [G loss: 1.000119]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2898 [D loss: 0.999988] [G loss: 1.000069]\n",
            "2899 [D loss: 0.999984] [G loss: 1.000079]\n",
            "2900 [D loss: 0.999981] [G loss: 1.000072]\n",
            "2901 [D loss: 0.999997] [G loss: 1.000080]\n",
            "2902 [D loss: 0.999983] [G loss: 1.000056]\n",
            "2903 [D loss: 0.999968] [G loss: 1.000080]\n",
            "2904 [D loss: 0.999966] [G loss: 1.000060]\n",
            "2905 [D loss: 0.999971] [G loss: 1.000078]\n",
            "2906 [D loss: 0.999954] [G loss: 1.000033]\n",
            "2907 [D loss: 0.999979] [G loss: 1.000041]\n",
            "2908 [D loss: 0.999971] [G loss: 1.000072]\n",
            "2909 [D loss: 0.999976] [G loss: 1.000023]\n",
            "2910 [D loss: 0.999992] [G loss: 1.000047]\n",
            "2911 [D loss: 0.999960] [G loss: 1.000007]\n",
            "2912 [D loss: 0.999945] [G loss: 1.000057]\n",
            "2913 [D loss: 0.999979] [G loss: 1.000069]\n",
            "2914 [D loss: 0.999933] [G loss: 1.000065]\n",
            "2915 [D loss: 0.999965] [G loss: 1.000083]\n",
            "2916 [D loss: 1.000003] [G loss: 1.000079]\n",
            "2917 [D loss: 0.999982] [G loss: 1.000052]\n",
            "2918 [D loss: 0.999979] [G loss: 1.000076]\n",
            "2919 [D loss: 0.999981] [G loss: 1.000125]\n",
            "2920 [D loss: 0.999970] [G loss: 1.000074]\n",
            "2921 [D loss: 0.999991] [G loss: 1.000084]\n",
            "2922 [D loss: 0.999939] [G loss: 1.000059]\n",
            "2923 [D loss: 0.999963] [G loss: 1.000057]\n",
            "2924 [D loss: 0.999992] [G loss: 1.000048]\n",
            "2925 [D loss: 0.999972] [G loss: 1.000056]\n",
            "2926 [D loss: 0.999986] [G loss: 1.000087]\n",
            "2927 [D loss: 0.999993] [G loss: 1.000074]\n",
            "2928 [D loss: 0.999937] [G loss: 1.000081]\n",
            "2929 [D loss: 0.999960] [G loss: 1.000074]\n",
            "2930 [D loss: 0.999954] [G loss: 1.000074]\n",
            "2931 [D loss: 0.999985] [G loss: 1.000043]\n",
            "2932 [D loss: 0.999961] [G loss: 1.000046]\n",
            "2933 [D loss: 0.999998] [G loss: 1.000035]\n",
            "2934 [D loss: 0.999961] [G loss: 1.000053]\n",
            "2935 [D loss: 0.999951] [G loss: 1.000087]\n",
            "2936 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2937 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2938 [D loss: 0.999985] [G loss: 1.000036]\n",
            "2939 [D loss: 0.999973] [G loss: 1.000054]\n",
            "2940 [D loss: 0.999960] [G loss: 1.000075]\n",
            "2941 [D loss: 0.999969] [G loss: 1.000079]\n",
            "2942 [D loss: 0.999957] [G loss: 1.000083]\n",
            "2943 [D loss: 0.999968] [G loss: 1.000073]\n",
            "2944 [D loss: 1.000018] [G loss: 1.000079]\n",
            "2945 [D loss: 0.999985] [G loss: 1.000063]\n",
            "2946 [D loss: 0.999989] [G loss: 1.000035]\n",
            "2947 [D loss: 0.999949] [G loss: 1.000046]\n",
            "2948 [D loss: 0.999966] [G loss: 1.000067]\n",
            "2949 [D loss: 0.999970] [G loss: 1.000053]\n",
            "2950 [D loss: 0.999982] [G loss: 1.000033]\n",
            "2951 [D loss: 0.999975] [G loss: 1.000020]\n",
            "2952 [D loss: 0.999975] [G loss: 1.000055]\n",
            "2953 [D loss: 0.999967] [G loss: 1.000051]\n",
            "2954 [D loss: 0.999933] [G loss: 1.000051]\n",
            "2955 [D loss: 0.999977] [G loss: 1.000064]\n",
            "2956 [D loss: 0.999950] [G loss: 1.000046]\n",
            "2957 [D loss: 0.999968] [G loss: 1.000092]\n",
            "2958 [D loss: 0.999984] [G loss: 1.000059]\n",
            "2959 [D loss: 0.999941] [G loss: 1.000066]\n",
            "2960 [D loss: 0.999962] [G loss: 1.000066]\n",
            "2961 [D loss: 0.999983] [G loss: 1.000079]\n",
            "2962 [D loss: 0.999942] [G loss: 1.000067]\n",
            "2963 [D loss: 0.999976] [G loss: 1.000097]\n",
            "2964 [D loss: 0.999959] [G loss: 1.000078]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2965 [D loss: 0.999953] [G loss: 1.000052]\n",
            "2966 [D loss: 1.000001] [G loss: 1.000050]\n",
            "2967 [D loss: 0.999980] [G loss: 1.000062]\n",
            "2968 [D loss: 0.999944] [G loss: 1.000050]\n",
            "2969 [D loss: 0.999966] [G loss: 1.000076]\n",
            "2970 [D loss: 1.000012] [G loss: 1.000043]\n",
            "2971 [D loss: 0.999956] [G loss: 1.000057]\n",
            "2972 [D loss: 0.999997] [G loss: 1.000044]\n",
            "2973 [D loss: 1.000000] [G loss: 1.000065]\n",
            "2974 [D loss: 0.999970] [G loss: 1.000042]\n",
            "2975 [D loss: 0.999990] [G loss: 1.000087]\n",
            "2976 [D loss: 1.000001] [G loss: 1.000035]\n",
            "2977 [D loss: 0.999938] [G loss: 1.000049]\n",
            "2978 [D loss: 0.999931] [G loss: 1.000055]\n",
            "2979 [D loss: 0.999962] [G loss: 1.000060]\n",
            "2980 [D loss: 0.999961] [G loss: 1.000053]\n",
            "2981 [D loss: 0.999962] [G loss: 1.000070]\n",
            "2982 [D loss: 0.999947] [G loss: 1.000070]\n",
            "2983 [D loss: 0.999946] [G loss: 1.000071]\n",
            "2984 [D loss: 0.999990] [G loss: 1.000108]\n",
            "2985 [D loss: 0.999966] [G loss: 1.000087]\n",
            "2986 [D loss: 0.999978] [G loss: 1.000043]\n",
            "2987 [D loss: 0.999966] [G loss: 1.000066]\n",
            "2988 [D loss: 0.999976] [G loss: 1.000058]\n",
            "2989 [D loss: 0.999991] [G loss: 1.000072]\n",
            "2990 [D loss: 0.999936] [G loss: 1.000092]\n",
            "2991 [D loss: 0.999996] [G loss: 1.000073]\n",
            "2992 [D loss: 0.999964] [G loss: 1.000090]\n",
            "2993 [D loss: 0.999967] [G loss: 1.000076]\n",
            "2994 [D loss: 0.999990] [G loss: 1.000055]\n",
            "2995 [D loss: 0.999960] [G loss: 1.000076]\n",
            "2996 [D loss: 1.000003] [G loss: 1.000087]\n",
            "2997 [D loss: 0.999992] [G loss: 1.000098]\n",
            "2998 [D loss: 0.999976] [G loss: 1.000061]\n",
            "2999 [D loss: 1.000020] [G loss: 1.000080]\n",
            "3000 [D loss: 0.999998] [G loss: 1.000071]\n",
            "3001 [D loss: 0.999940] [G loss: 1.000064]\n",
            "3002 [D loss: 0.999940] [G loss: 1.000035]\n",
            "3003 [D loss: 0.999972] [G loss: 1.000095]\n",
            "3004 [D loss: 0.999969] [G loss: 1.000061]\n",
            "3005 [D loss: 0.999959] [G loss: 1.000094]\n",
            "3006 [D loss: 0.999975] [G loss: 1.000074]\n",
            "3007 [D loss: 0.999974] [G loss: 1.000056]\n",
            "3008 [D loss: 0.999953] [G loss: 1.000083]\n",
            "3009 [D loss: 0.999954] [G loss: 1.000077]\n",
            "3010 [D loss: 0.999943] [G loss: 1.000072]\n",
            "3011 [D loss: 0.999965] [G loss: 1.000075]\n",
            "3012 [D loss: 0.999986] [G loss: 1.000072]\n",
            "3013 [D loss: 0.999987] [G loss: 1.000089]\n",
            "3014 [D loss: 0.999959] [G loss: 1.000072]\n",
            "3015 [D loss: 0.999951] [G loss: 1.000054]\n",
            "3016 [D loss: 0.999954] [G loss: 1.000011]\n",
            "3017 [D loss: 1.000001] [G loss: 1.000056]\n",
            "3018 [D loss: 0.999964] [G loss: 1.000059]\n",
            "3019 [D loss: 0.999986] [G loss: 1.000038]\n",
            "3020 [D loss: 0.999995] [G loss: 1.000070]\n",
            "3021 [D loss: 0.999968] [G loss: 1.000092]\n",
            "3022 [D loss: 0.999976] [G loss: 1.000079]\n",
            "3023 [D loss: 0.999978] [G loss: 1.000088]\n",
            "3024 [D loss: 0.999968] [G loss: 1.000056]\n",
            "3025 [D loss: 0.999976] [G loss: 1.000037]\n",
            "3026 [D loss: 0.999934] [G loss: 1.000106]\n",
            "3027 [D loss: 0.999953] [G loss: 1.000075]\n",
            "3028 [D loss: 0.999959] [G loss: 1.000069]\n",
            "3029 [D loss: 0.999991] [G loss: 1.000098]\n",
            "3030 [D loss: 0.999988] [G loss: 1.000092]\n",
            "3031 [D loss: 0.999929] [G loss: 1.000083]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3032 [D loss: 0.999973] [G loss: 1.000033]\n",
            "3033 [D loss: 0.999979] [G loss: 1.000066]\n",
            "3034 [D loss: 0.999982] [G loss: 1.000066]\n",
            "3035 [D loss: 0.999991] [G loss: 1.000092]\n",
            "3036 [D loss: 0.999995] [G loss: 1.000072]\n",
            "3037 [D loss: 0.999935] [G loss: 1.000039]\n",
            "3038 [D loss: 0.999935] [G loss: 1.000096]\n",
            "3039 [D loss: 0.999966] [G loss: 1.000062]\n",
            "3040 [D loss: 0.999967] [G loss: 1.000046]\n",
            "3041 [D loss: 0.999955] [G loss: 1.000051]\n",
            "3042 [D loss: 0.999998] [G loss: 1.000049]\n",
            "3043 [D loss: 0.999979] [G loss: 1.000035]\n",
            "3044 [D loss: 0.999945] [G loss: 1.000070]\n",
            "3045 [D loss: 0.999955] [G loss: 1.000067]\n",
            "3046 [D loss: 0.999940] [G loss: 1.000064]\n",
            "3047 [D loss: 0.999957] [G loss: 1.000070]\n",
            "3048 [D loss: 0.999962] [G loss: 1.000058]\n",
            "3049 [D loss: 0.999936] [G loss: 1.000020]\n",
            "3050 [D loss: 0.999953] [G loss: 1.000065]\n",
            "3051 [D loss: 0.999982] [G loss: 1.000037]\n",
            "3052 [D loss: 0.999976] [G loss: 1.000094]\n",
            "3053 [D loss: 0.999956] [G loss: 1.000019]\n",
            "3054 [D loss: 0.999967] [G loss: 1.000086]\n",
            "3055 [D loss: 0.999964] [G loss: 1.000067]\n",
            "3056 [D loss: 0.999963] [G loss: 1.000047]\n",
            "3057 [D loss: 0.999948] [G loss: 1.000085]\n",
            "3058 [D loss: 0.999947] [G loss: 1.000050]\n",
            "3059 [D loss: 0.999976] [G loss: 1.000057]\n",
            "3060 [D loss: 0.999978] [G loss: 1.000075]\n",
            "3061 [D loss: 0.999996] [G loss: 1.000071]\n",
            "3062 [D loss: 0.999958] [G loss: 1.000043]\n",
            "3063 [D loss: 0.999960] [G loss: 1.000053]\n",
            "3064 [D loss: 0.999964] [G loss: 1.000075]\n",
            "3065 [D loss: 0.999984] [G loss: 1.000055]\n",
            "3066 [D loss: 0.999989] [G loss: 1.000065]\n",
            "3067 [D loss: 0.999964] [G loss: 1.000113]\n",
            "3068 [D loss: 0.999989] [G loss: 1.000066]\n",
            "3069 [D loss: 0.999981] [G loss: 1.000054]\n",
            "3070 [D loss: 0.999966] [G loss: 1.000081]\n",
            "3071 [D loss: 0.999988] [G loss: 1.000071]\n",
            "3072 [D loss: 0.999961] [G loss: 1.000068]\n",
            "3073 [D loss: 0.999961] [G loss: 1.000076]\n",
            "3074 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3075 [D loss: 0.999980] [G loss: 1.000082]\n",
            "3076 [D loss: 0.999956] [G loss: 1.000064]\n",
            "3077 [D loss: 0.999954] [G loss: 1.000076]\n",
            "3078 [D loss: 0.999977] [G loss: 1.000068]\n",
            "3079 [D loss: 0.999976] [G loss: 1.000036]\n",
            "3080 [D loss: 0.999977] [G loss: 1.000026]\n",
            "3081 [D loss: 0.999960] [G loss: 1.000087]\n",
            "3082 [D loss: 0.999964] [G loss: 1.000080]\n",
            "3083 [D loss: 1.000000] [G loss: 1.000049]\n",
            "3084 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3085 [D loss: 0.999960] [G loss: 1.000061]\n",
            "3086 [D loss: 0.999992] [G loss: 1.000048]\n",
            "3087 [D loss: 0.999956] [G loss: 1.000040]\n",
            "3088 [D loss: 0.999961] [G loss: 1.000051]\n",
            "3089 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3090 [D loss: 0.999951] [G loss: 1.000049]\n",
            "3091 [D loss: 0.999984] [G loss: 1.000081]\n",
            "3092 [D loss: 0.999967] [G loss: 1.000078]\n",
            "3093 [D loss: 0.999961] [G loss: 1.000083]\n",
            "3094 [D loss: 0.999974] [G loss: 1.000085]\n",
            "3095 [D loss: 0.999992] [G loss: 1.000124]\n",
            "3096 [D loss: 0.999957] [G loss: 1.000076]\n",
            "3097 [D loss: 0.999942] [G loss: 1.000059]\n",
            "3098 [D loss: 0.999971] [G loss: 1.000063]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3099 [D loss: 0.999953] [G loss: 1.000076]\n",
            "3100 [D loss: 0.999961] [G loss: 1.000053]\n",
            "3101 [D loss: 0.999970] [G loss: 1.000057]\n",
            "3102 [D loss: 0.999997] [G loss: 1.000023]\n",
            "3103 [D loss: 0.999985] [G loss: 1.000052]\n",
            "3104 [D loss: 0.999968] [G loss: 1.000074]\n",
            "3105 [D loss: 0.999933] [G loss: 1.000074]\n",
            "3106 [D loss: 0.999968] [G loss: 1.000072]\n",
            "3107 [D loss: 0.999948] [G loss: 1.000073]\n",
            "3108 [D loss: 0.999964] [G loss: 1.000037]\n",
            "3109 [D loss: 1.000002] [G loss: 1.000067]\n",
            "3110 [D loss: 0.999958] [G loss: 1.000057]\n",
            "3111 [D loss: 0.999940] [G loss: 1.000052]\n",
            "3112 [D loss: 0.999943] [G loss: 1.000094]\n",
            "3113 [D loss: 0.999940] [G loss: 1.000044]\n",
            "3114 [D loss: 0.999955] [G loss: 1.000055]\n",
            "3115 [D loss: 0.999973] [G loss: 1.000070]\n",
            "3116 [D loss: 0.999974] [G loss: 1.000093]\n",
            "3117 [D loss: 0.999987] [G loss: 1.000053]\n",
            "3118 [D loss: 0.999963] [G loss: 1.000055]\n",
            "3119 [D loss: 0.999948] [G loss: 1.000045]\n",
            "3120 [D loss: 0.999983] [G loss: 1.000113]\n",
            "3121 [D loss: 0.999982] [G loss: 1.000036]\n",
            "3122 [D loss: 0.999996] [G loss: 1.000094]\n",
            "3123 [D loss: 0.999961] [G loss: 1.000114]\n",
            "3124 [D loss: 0.999958] [G loss: 1.000039]\n",
            "3125 [D loss: 0.999969] [G loss: 1.000055]\n",
            "3126 [D loss: 0.999962] [G loss: 1.000031]\n",
            "3127 [D loss: 0.999941] [G loss: 1.000059]\n",
            "3128 [D loss: 0.999997] [G loss: 1.000063]\n",
            "3129 [D loss: 0.999954] [G loss: 1.000091]\n",
            "3130 [D loss: 0.999971] [G loss: 1.000092]\n",
            "3131 [D loss: 0.999941] [G loss: 1.000059]\n",
            "3132 [D loss: 0.999995] [G loss: 1.000037]\n",
            "3133 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3134 [D loss: 0.999949] [G loss: 1.000082]\n",
            "3135 [D loss: 0.999979] [G loss: 1.000114]\n",
            "3136 [D loss: 0.999958] [G loss: 1.000049]\n",
            "3137 [D loss: 0.999989] [G loss: 1.000009]\n",
            "3138 [D loss: 0.999982] [G loss: 1.000035]\n",
            "3139 [D loss: 0.999946] [G loss: 1.000083]\n",
            "3140 [D loss: 0.999985] [G loss: 1.000056]\n",
            "3141 [D loss: 0.999955] [G loss: 1.000082]\n",
            "3142 [D loss: 0.999947] [G loss: 1.000021]\n",
            "3143 [D loss: 0.999966] [G loss: 1.000053]\n",
            "3144 [D loss: 0.999936] [G loss: 1.000052]\n",
            "3145 [D loss: 0.999945] [G loss: 1.000097]\n",
            "3146 [D loss: 0.999955] [G loss: 1.000067]\n",
            "3147 [D loss: 0.999981] [G loss: 1.000042]\n",
            "3148 [D loss: 0.999974] [G loss: 1.000061]\n",
            "3149 [D loss: 0.999956] [G loss: 1.000086]\n",
            "3150 [D loss: 0.999927] [G loss: 1.000061]\n",
            "3151 [D loss: 0.999951] [G loss: 1.000053]\n",
            "3152 [D loss: 0.999985] [G loss: 1.000049]\n",
            "3153 [D loss: 0.999973] [G loss: 1.000047]\n",
            "3154 [D loss: 0.999991] [G loss: 1.000026]\n",
            "3155 [D loss: 0.999961] [G loss: 1.000053]\n",
            "3156 [D loss: 0.999979] [G loss: 1.000120]\n",
            "3157 [D loss: 0.999972] [G loss: 1.000077]\n",
            "3158 [D loss: 0.999956] [G loss: 1.000078]\n",
            "3159 [D loss: 0.999950] [G loss: 1.000077]\n",
            "3160 [D loss: 0.999968] [G loss: 1.000051]\n",
            "3161 [D loss: 1.000006] [G loss: 1.000048]\n",
            "3162 [D loss: 0.999955] [G loss: 1.000056]\n",
            "3163 [D loss: 0.999944] [G loss: 1.000036]\n",
            "3164 [D loss: 0.999980] [G loss: 1.000065]\n",
            "3165 [D loss: 0.999982] [G loss: 1.000061]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3166 [D loss: 0.999930] [G loss: 1.000081]\n",
            "3167 [D loss: 0.999952] [G loss: 1.000096]\n",
            "3168 [D loss: 0.999973] [G loss: 1.000073]\n",
            "3169 [D loss: 0.999960] [G loss: 1.000088]\n",
            "3170 [D loss: 0.999952] [G loss: 1.000047]\n",
            "3171 [D loss: 0.999966] [G loss: 1.000064]\n",
            "3172 [D loss: 0.999975] [G loss: 1.000045]\n",
            "3173 [D loss: 0.999965] [G loss: 1.000050]\n",
            "3174 [D loss: 0.999981] [G loss: 1.000036]\n",
            "3175 [D loss: 0.999951] [G loss: 1.000066]\n",
            "3176 [D loss: 0.999966] [G loss: 1.000057]\n",
            "3177 [D loss: 0.999926] [G loss: 1.000067]\n",
            "3178 [D loss: 0.999963] [G loss: 1.000089]\n",
            "3179 [D loss: 0.999961] [G loss: 1.000099]\n",
            "3180 [D loss: 0.999980] [G loss: 1.000069]\n",
            "3181 [D loss: 0.999975] [G loss: 1.000086]\n",
            "3182 [D loss: 0.999953] [G loss: 1.000069]\n",
            "3183 [D loss: 0.999972] [G loss: 1.000078]\n",
            "3184 [D loss: 0.999973] [G loss: 1.000075]\n",
            "3185 [D loss: 0.999987] [G loss: 1.000049]\n",
            "3186 [D loss: 1.000013] [G loss: 1.000085]\n",
            "3187 [D loss: 0.999949] [G loss: 1.000043]\n",
            "3188 [D loss: 0.999976] [G loss: 1.000072]\n",
            "3189 [D loss: 0.999987] [G loss: 1.000047]\n",
            "3190 [D loss: 0.999950] [G loss: 1.000044]\n",
            "3191 [D loss: 0.999967] [G loss: 1.000064]\n",
            "3192 [D loss: 1.000003] [G loss: 1.000068]\n",
            "3193 [D loss: 0.999946] [G loss: 1.000073]\n",
            "3194 [D loss: 0.999959] [G loss: 1.000066]\n",
            "3195 [D loss: 0.999968] [G loss: 1.000054]\n",
            "3196 [D loss: 0.999959] [G loss: 1.000068]\n",
            "3197 [D loss: 0.999949] [G loss: 1.000040]\n",
            "3198 [D loss: 0.999965] [G loss: 1.000044]\n",
            "3199 [D loss: 0.999953] [G loss: 1.000060]\n",
            "3200 [D loss: 1.000001] [G loss: 1.000034]\n",
            "3201 [D loss: 0.999963] [G loss: 1.000037]\n",
            "3202 [D loss: 0.999925] [G loss: 1.000091]\n",
            "3203 [D loss: 0.999979] [G loss: 1.000084]\n",
            "3204 [D loss: 0.999958] [G loss: 1.000040]\n",
            "3205 [D loss: 0.999993] [G loss: 1.000083]\n",
            "3206 [D loss: 0.999991] [G loss: 1.000050]\n",
            "3207 [D loss: 1.000002] [G loss: 1.000067]\n",
            "3208 [D loss: 0.999977] [G loss: 1.000056]\n",
            "3209 [D loss: 0.999961] [G loss: 1.000067]\n",
            "3210 [D loss: 0.999955] [G loss: 1.000036]\n",
            "3211 [D loss: 0.999971] [G loss: 1.000091]\n",
            "3212 [D loss: 0.999972] [G loss: 1.000044]\n",
            "3213 [D loss: 0.999973] [G loss: 1.000086]\n",
            "3214 [D loss: 0.999952] [G loss: 1.000083]\n",
            "3215 [D loss: 0.999977] [G loss: 1.000106]\n",
            "3216 [D loss: 0.999953] [G loss: 1.000091]\n",
            "3217 [D loss: 0.999945] [G loss: 1.000067]\n",
            "3218 [D loss: 0.999935] [G loss: 1.000084]\n",
            "3219 [D loss: 0.999966] [G loss: 1.000088]\n",
            "3220 [D loss: 0.999975] [G loss: 1.000092]\n",
            "3221 [D loss: 0.999966] [G loss: 1.000020]\n",
            "3222 [D loss: 0.999972] [G loss: 1.000047]\n",
            "3223 [D loss: 0.999980] [G loss: 1.000067]\n",
            "3224 [D loss: 0.999961] [G loss: 1.000065]\n",
            "3225 [D loss: 0.999986] [G loss: 1.000052]\n",
            "3226 [D loss: 1.000001] [G loss: 1.000039]\n",
            "3227 [D loss: 0.999909] [G loss: 1.000028]\n",
            "3228 [D loss: 0.999934] [G loss: 1.000056]\n",
            "3229 [D loss: 0.999979] [G loss: 1.000072]\n",
            "3230 [D loss: 0.999971] [G loss: 1.000093]\n",
            "3231 [D loss: 0.999979] [G loss: 1.000082]\n",
            "3232 [D loss: 0.999935] [G loss: 1.000055]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3233 [D loss: 0.999983] [G loss: 1.000096]\n",
            "3234 [D loss: 0.999975] [G loss: 1.000056]\n",
            "3235 [D loss: 0.999978] [G loss: 1.000078]\n",
            "3236 [D loss: 0.999988] [G loss: 1.000062]\n",
            "3237 [D loss: 0.999950] [G loss: 1.000071]\n",
            "3238 [D loss: 0.999986] [G loss: 1.000068]\n",
            "3239 [D loss: 0.999966] [G loss: 1.000048]\n",
            "3240 [D loss: 0.999965] [G loss: 1.000065]\n",
            "3241 [D loss: 0.999999] [G loss: 1.000046]\n",
            "3242 [D loss: 0.999964] [G loss: 1.000058]\n",
            "3243 [D loss: 0.999946] [G loss: 1.000067]\n",
            "3244 [D loss: 0.999979] [G loss: 1.000070]\n",
            "3245 [D loss: 0.999979] [G loss: 1.000070]\n",
            "3246 [D loss: 0.999945] [G loss: 1.000079]\n",
            "3247 [D loss: 0.999945] [G loss: 1.000054]\n",
            "3248 [D loss: 0.999958] [G loss: 1.000050]\n",
            "3249 [D loss: 0.999978] [G loss: 1.000058]\n",
            "3250 [D loss: 0.999990] [G loss: 1.000058]\n",
            "3251 [D loss: 0.999963] [G loss: 1.000049]\n",
            "3252 [D loss: 0.999973] [G loss: 1.000045]\n",
            "3253 [D loss: 0.999965] [G loss: 1.000076]\n",
            "3254 [D loss: 0.999934] [G loss: 1.000075]\n",
            "3255 [D loss: 0.999975] [G loss: 1.000070]\n",
            "3256 [D loss: 0.999973] [G loss: 1.000081]\n",
            "3257 [D loss: 0.999971] [G loss: 1.000054]\n",
            "3258 [D loss: 0.999968] [G loss: 1.000062]\n",
            "3259 [D loss: 0.999983] [G loss: 1.000038]\n",
            "3260 [D loss: 0.999966] [G loss: 1.000056]\n",
            "3261 [D loss: 0.999955] [G loss: 1.000051]\n",
            "3262 [D loss: 0.999959] [G loss: 1.000073]\n",
            "3263 [D loss: 0.999963] [G loss: 1.000067]\n",
            "3264 [D loss: 1.000006] [G loss: 1.000080]\n",
            "3265 [D loss: 0.999991] [G loss: 1.000087]\n",
            "3266 [D loss: 0.999984] [G loss: 1.000069]\n",
            "3267 [D loss: 0.999950] [G loss: 1.000083]\n",
            "3268 [D loss: 1.000012] [G loss: 1.000069]\n",
            "3269 [D loss: 0.999941] [G loss: 1.000088]\n",
            "3270 [D loss: 0.999947] [G loss: 1.000049]\n",
            "3271 [D loss: 0.999941] [G loss: 1.000090]\n",
            "3272 [D loss: 0.999950] [G loss: 1.000091]\n",
            "3273 [D loss: 0.999961] [G loss: 1.000054]\n",
            "3274 [D loss: 0.999983] [G loss: 1.000064]\n",
            "3275 [D loss: 0.999945] [G loss: 1.000063]\n",
            "3276 [D loss: 0.999965] [G loss: 1.000055]\n",
            "3277 [D loss: 0.999948] [G loss: 1.000068]\n",
            "3278 [D loss: 0.999944] [G loss: 1.000076]\n",
            "3279 [D loss: 0.999962] [G loss: 1.000073]\n",
            "3280 [D loss: 0.999953] [G loss: 1.000060]\n",
            "3281 [D loss: 0.999966] [G loss: 1.000074]\n",
            "3282 [D loss: 0.999965] [G loss: 1.000070]\n",
            "3283 [D loss: 0.999983] [G loss: 1.000083]\n",
            "3284 [D loss: 0.999949] [G loss: 1.000058]\n",
            "3285 [D loss: 0.999965] [G loss: 1.000045]\n",
            "3286 [D loss: 0.999969] [G loss: 1.000047]\n",
            "3287 [D loss: 0.999979] [G loss: 1.000080]\n",
            "3288 [D loss: 0.999968] [G loss: 1.000079]\n",
            "3289 [D loss: 0.999975] [G loss: 1.000072]\n",
            "3290 [D loss: 0.999981] [G loss: 1.000055]\n",
            "3291 [D loss: 0.999973] [G loss: 1.000059]\n",
            "3292 [D loss: 0.999972] [G loss: 1.000050]\n",
            "3293 [D loss: 0.999986] [G loss: 1.000098]\n",
            "3294 [D loss: 0.999960] [G loss: 1.000086]\n",
            "3295 [D loss: 0.999950] [G loss: 1.000075]\n",
            "3296 [D loss: 0.999964] [G loss: 1.000043]\n",
            "3297 [D loss: 0.999979] [G loss: 1.000074]\n",
            "3298 [D loss: 0.999950] [G loss: 1.000054]\n",
            "3299 [D loss: 0.999954] [G loss: 1.000083]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3300 [D loss: 0.999959] [G loss: 1.000050]\n",
            "3301 [D loss: 0.999982] [G loss: 1.000060]\n",
            "3302 [D loss: 0.999972] [G loss: 1.000081]\n",
            "3303 [D loss: 0.999945] [G loss: 1.000099]\n",
            "3304 [D loss: 0.999941] [G loss: 1.000087]\n",
            "3305 [D loss: 0.999961] [G loss: 1.000071]\n",
            "3306 [D loss: 0.999972] [G loss: 1.000040]\n",
            "3307 [D loss: 0.999949] [G loss: 1.000036]\n",
            "3308 [D loss: 0.999963] [G loss: 1.000054]\n",
            "3309 [D loss: 0.999983] [G loss: 1.000066]\n",
            "3310 [D loss: 0.999973] [G loss: 1.000052]\n",
            "3311 [D loss: 0.999960] [G loss: 1.000087]\n",
            "3312 [D loss: 1.000005] [G loss: 1.000087]\n",
            "3313 [D loss: 0.999969] [G loss: 1.000082]\n",
            "3314 [D loss: 0.999977] [G loss: 1.000068]\n",
            "3315 [D loss: 0.999956] [G loss: 1.000039]\n",
            "3316 [D loss: 0.999947] [G loss: 1.000078]\n",
            "3317 [D loss: 0.999937] [G loss: 1.000070]\n",
            "3318 [D loss: 0.999960] [G loss: 1.000077]\n",
            "3319 [D loss: 0.999968] [G loss: 1.000027]\n",
            "3320 [D loss: 0.999966] [G loss: 1.000065]\n",
            "3321 [D loss: 0.999965] [G loss: 1.000074]\n",
            "3322 [D loss: 0.999953] [G loss: 1.000068]\n",
            "3323 [D loss: 0.999962] [G loss: 1.000093]\n",
            "3324 [D loss: 0.999967] [G loss: 1.000065]\n",
            "3325 [D loss: 0.999996] [G loss: 1.000025]\n",
            "3326 [D loss: 0.999953] [G loss: 1.000078]\n",
            "3327 [D loss: 0.999967] [G loss: 1.000062]\n",
            "3328 [D loss: 0.999971] [G loss: 1.000086]\n",
            "3329 [D loss: 0.999970] [G loss: 1.000053]\n",
            "3330 [D loss: 0.999946] [G loss: 1.000058]\n",
            "3331 [D loss: 0.999973] [G loss: 1.000075]\n",
            "3332 [D loss: 0.999966] [G loss: 1.000059]\n",
            "3333 [D loss: 0.999952] [G loss: 1.000051]\n",
            "3334 [D loss: 0.999973] [G loss: 1.000061]\n",
            "3335 [D loss: 0.999971] [G loss: 1.000071]\n",
            "3336 [D loss: 0.999973] [G loss: 1.000070]\n",
            "3337 [D loss: 0.999955] [G loss: 1.000084]\n",
            "3338 [D loss: 0.999955] [G loss: 1.000088]\n",
            "3339 [D loss: 0.999972] [G loss: 1.000089]\n",
            "3340 [D loss: 0.999965] [G loss: 1.000067]\n",
            "3341 [D loss: 0.999950] [G loss: 1.000049]\n",
            "3342 [D loss: 0.999972] [G loss: 1.000076]\n",
            "3343 [D loss: 0.999957] [G loss: 1.000069]\n",
            "3344 [D loss: 0.999977] [G loss: 1.000089]\n",
            "3345 [D loss: 0.999973] [G loss: 1.000081]\n",
            "3346 [D loss: 0.999967] [G loss: 1.000059]\n",
            "3347 [D loss: 0.999971] [G loss: 1.000056]\n",
            "3348 [D loss: 0.999966] [G loss: 1.000068]\n",
            "3349 [D loss: 0.999958] [G loss: 1.000056]\n",
            "3350 [D loss: 0.999996] [G loss: 1.000068]\n",
            "3351 [D loss: 0.999956] [G loss: 1.000050]\n",
            "3352 [D loss: 0.999967] [G loss: 1.000060]\n",
            "3353 [D loss: 0.999978] [G loss: 1.000051]\n",
            "3354 [D loss: 0.999955] [G loss: 1.000042]\n",
            "3355 [D loss: 0.999943] [G loss: 1.000060]\n",
            "3356 [D loss: 0.999968] [G loss: 1.000068]\n",
            "3357 [D loss: 0.999964] [G loss: 1.000072]\n",
            "3358 [D loss: 0.999946] [G loss: 1.000057]\n",
            "3359 [D loss: 0.999976] [G loss: 1.000074]\n",
            "3360 [D loss: 0.999968] [G loss: 1.000051]\n",
            "3361 [D loss: 0.999955] [G loss: 1.000070]\n",
            "3362 [D loss: 0.999962] [G loss: 1.000072]\n",
            "3363 [D loss: 0.999955] [G loss: 1.000088]\n",
            "3364 [D loss: 1.000002] [G loss: 1.000076]\n",
            "3365 [D loss: 0.999980] [G loss: 1.000082]\n",
            "3366 [D loss: 0.999967] [G loss: 1.000066]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3367 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3368 [D loss: 0.999964] [G loss: 1.000069]\n",
            "3369 [D loss: 0.999979] [G loss: 1.000076]\n",
            "3370 [D loss: 0.999980] [G loss: 1.000078]\n",
            "3371 [D loss: 0.999946] [G loss: 1.000046]\n",
            "3372 [D loss: 0.999960] [G loss: 1.000063]\n",
            "3373 [D loss: 0.999977] [G loss: 1.000031]\n",
            "3374 [D loss: 1.000002] [G loss: 1.000052]\n",
            "3375 [D loss: 0.999964] [G loss: 1.000050]\n",
            "3376 [D loss: 0.999957] [G loss: 1.000068]\n",
            "3377 [D loss: 0.999953] [G loss: 1.000063]\n",
            "3378 [D loss: 0.999948] [G loss: 1.000085]\n",
            "3379 [D loss: 0.999970] [G loss: 1.000080]\n",
            "3380 [D loss: 0.999967] [G loss: 1.000083]\n",
            "3381 [D loss: 0.999949] [G loss: 1.000049]\n",
            "3382 [D loss: 0.999978] [G loss: 1.000107]\n",
            "3383 [D loss: 0.999968] [G loss: 1.000048]\n",
            "3384 [D loss: 0.999948] [G loss: 1.000051]\n",
            "3385 [D loss: 0.999933] [G loss: 1.000033]\n",
            "3386 [D loss: 0.999981] [G loss: 1.000110]\n",
            "3387 [D loss: 0.999953] [G loss: 1.000039]\n",
            "3388 [D loss: 0.999943] [G loss: 1.000066]\n",
            "3389 [D loss: 0.999982] [G loss: 1.000093]\n",
            "3390 [D loss: 0.999953] [G loss: 1.000091]\n",
            "3391 [D loss: 0.999938] [G loss: 1.000053]\n",
            "3392 [D loss: 0.999995] [G loss: 1.000075]\n",
            "3393 [D loss: 0.999970] [G loss: 1.000085]\n",
            "3394 [D loss: 0.999973] [G loss: 1.000051]\n",
            "3395 [D loss: 0.999940] [G loss: 1.000027]\n",
            "3396 [D loss: 0.999979] [G loss: 1.000101]\n",
            "3397 [D loss: 0.999956] [G loss: 1.000067]\n",
            "3398 [D loss: 0.999955] [G loss: 1.000086]\n",
            "3399 [D loss: 0.999987] [G loss: 1.000090]\n",
            "3400 [D loss: 0.999951] [G loss: 1.000042]\n",
            "3401 [D loss: 0.999984] [G loss: 1.000068]\n",
            "3402 [D loss: 0.999977] [G loss: 1.000019]\n",
            "3403 [D loss: 0.999959] [G loss: 1.000044]\n",
            "3404 [D loss: 1.000000] [G loss: 1.000068]\n",
            "3405 [D loss: 0.999953] [G loss: 1.000060]\n",
            "3406 [D loss: 0.999949] [G loss: 1.000054]\n",
            "3407 [D loss: 0.999944] [G loss: 1.000054]\n",
            "3408 [D loss: 0.999975] [G loss: 1.000065]\n",
            "3409 [D loss: 0.999971] [G loss: 1.000045]\n",
            "3410 [D loss: 0.999968] [G loss: 1.000086]\n",
            "3411 [D loss: 0.999985] [G loss: 1.000049]\n",
            "3412 [D loss: 0.999967] [G loss: 1.000092]\n",
            "3413 [D loss: 0.999967] [G loss: 1.000065]\n",
            "3414 [D loss: 0.999981] [G loss: 1.000082]\n",
            "3415 [D loss: 0.999970] [G loss: 1.000074]\n",
            "3416 [D loss: 0.999959] [G loss: 1.000046]\n",
            "3417 [D loss: 0.999973] [G loss: 1.000067]\n",
            "3418 [D loss: 0.999974] [G loss: 1.000064]\n",
            "3419 [D loss: 0.999972] [G loss: 1.000047]\n",
            "3420 [D loss: 0.999982] [G loss: 1.000059]\n",
            "3421 [D loss: 0.999962] [G loss: 1.000072]\n",
            "3422 [D loss: 0.999967] [G loss: 1.000064]\n",
            "3423 [D loss: 0.999963] [G loss: 1.000065]\n",
            "3424 [D loss: 0.999973] [G loss: 1.000059]\n",
            "3425 [D loss: 0.999965] [G loss: 1.000072]\n",
            "3426 [D loss: 0.999979] [G loss: 1.000091]\n",
            "3427 [D loss: 0.999969] [G loss: 1.000032]\n",
            "3428 [D loss: 0.999978] [G loss: 1.000091]\n",
            "3429 [D loss: 0.999937] [G loss: 1.000070]\n",
            "3430 [D loss: 0.999962] [G loss: 1.000079]\n",
            "3431 [D loss: 0.999961] [G loss: 1.000056]\n",
            "3432 [D loss: 0.999984] [G loss: 1.000066]\n",
            "3433 [D loss: 0.999965] [G loss: 1.000081]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3434 [D loss: 0.999989] [G loss: 1.000077]\n",
            "3435 [D loss: 0.999982] [G loss: 1.000039]\n",
            "3436 [D loss: 0.999949] [G loss: 1.000060]\n",
            "3437 [D loss: 0.999958] [G loss: 1.000075]\n",
            "3438 [D loss: 0.999995] [G loss: 1.000040]\n",
            "3439 [D loss: 0.999981] [G loss: 1.000042]\n",
            "3440 [D loss: 0.999949] [G loss: 1.000059]\n",
            "3441 [D loss: 0.999966] [G loss: 1.000080]\n",
            "3442 [D loss: 0.999974] [G loss: 1.000042]\n",
            "3443 [D loss: 0.999985] [G loss: 1.000062]\n",
            "3444 [D loss: 0.999959] [G loss: 1.000064]\n",
            "3445 [D loss: 0.999983] [G loss: 1.000062]\n",
            "3446 [D loss: 0.999964] [G loss: 1.000048]\n",
            "3447 [D loss: 0.999961] [G loss: 1.000058]\n",
            "3448 [D loss: 0.999949] [G loss: 1.000053]\n",
            "3449 [D loss: 0.999965] [G loss: 1.000067]\n",
            "3450 [D loss: 0.999964] [G loss: 1.000099]\n",
            "3451 [D loss: 0.999928] [G loss: 1.000069]\n",
            "3452 [D loss: 0.999978] [G loss: 1.000072]\n",
            "3453 [D loss: 0.999982] [G loss: 1.000069]\n",
            "3454 [D loss: 0.999958] [G loss: 1.000064]\n",
            "3455 [D loss: 0.999988] [G loss: 1.000078]\n",
            "3456 [D loss: 0.999957] [G loss: 1.000073]\n",
            "3457 [D loss: 0.999991] [G loss: 1.000056]\n",
            "3458 [D loss: 0.999969] [G loss: 1.000059]\n",
            "3459 [D loss: 0.999988] [G loss: 1.000076]\n",
            "3460 [D loss: 0.999980] [G loss: 1.000028]\n",
            "3461 [D loss: 0.999942] [G loss: 1.000070]\n",
            "3462 [D loss: 0.999961] [G loss: 1.000047]\n",
            "3463 [D loss: 0.999940] [G loss: 1.000084]\n",
            "3464 [D loss: 0.999985] [G loss: 1.000063]\n",
            "3465 [D loss: 0.999971] [G loss: 1.000057]\n",
            "3466 [D loss: 0.999973] [G loss: 1.000072]\n",
            "3467 [D loss: 0.999980] [G loss: 1.000050]\n",
            "3468 [D loss: 0.999973] [G loss: 1.000081]\n",
            "3469 [D loss: 0.999976] [G loss: 1.000035]\n",
            "3470 [D loss: 0.999948] [G loss: 1.000067]\n",
            "3471 [D loss: 1.000017] [G loss: 1.000039]\n",
            "3472 [D loss: 0.999967] [G loss: 1.000036]\n",
            "3473 [D loss: 0.999975] [G loss: 1.000023]\n",
            "3474 [D loss: 0.999955] [G loss: 1.000093]\n",
            "3475 [D loss: 0.999979] [G loss: 1.000058]\n",
            "3476 [D loss: 0.999976] [G loss: 1.000074]\n",
            "3477 [D loss: 0.999960] [G loss: 1.000067]\n",
            "3478 [D loss: 0.999951] [G loss: 1.000083]\n",
            "3479 [D loss: 0.999960] [G loss: 1.000049]\n",
            "3480 [D loss: 0.999961] [G loss: 1.000076]\n",
            "3481 [D loss: 0.999930] [G loss: 1.000088]\n",
            "3482 [D loss: 0.999989] [G loss: 1.000072]\n",
            "3483 [D loss: 0.999966] [G loss: 1.000052]\n",
            "3484 [D loss: 0.999962] [G loss: 1.000045]\n",
            "3485 [D loss: 0.999959] [G loss: 1.000088]\n",
            "3486 [D loss: 0.999979] [G loss: 1.000078]\n",
            "3487 [D loss: 0.999956] [G loss: 1.000055]\n",
            "3488 [D loss: 0.999957] [G loss: 1.000050]\n",
            "3489 [D loss: 0.999969] [G loss: 1.000106]\n",
            "3490 [D loss: 0.999937] [G loss: 1.000058]\n",
            "3491 [D loss: 0.999972] [G loss: 1.000084]\n",
            "3492 [D loss: 0.999966] [G loss: 1.000056]\n",
            "3493 [D loss: 0.999984] [G loss: 1.000090]\n",
            "3494 [D loss: 0.999964] [G loss: 1.000068]\n",
            "3495 [D loss: 0.999961] [G loss: 1.000099]\n",
            "3496 [D loss: 0.999976] [G loss: 1.000087]\n",
            "3497 [D loss: 0.999981] [G loss: 1.000041]\n",
            "3498 [D loss: 0.999968] [G loss: 1.000072]\n",
            "3499 [D loss: 0.999967] [G loss: 1.000064]\n",
            "3500 [D loss: 0.999988] [G loss: 1.000049]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3501 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3502 [D loss: 0.999964] [G loss: 1.000076]\n",
            "3503 [D loss: 0.999950] [G loss: 1.000083]\n",
            "3504 [D loss: 0.999939] [G loss: 1.000073]\n",
            "3505 [D loss: 0.999971] [G loss: 1.000087]\n",
            "3506 [D loss: 0.999947] [G loss: 1.000090]\n",
            "3507 [D loss: 1.000006] [G loss: 1.000066]\n",
            "3508 [D loss: 0.999994] [G loss: 1.000078]\n",
            "3509 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3510 [D loss: 0.999968] [G loss: 1.000084]\n",
            "3511 [D loss: 0.999972] [G loss: 1.000061]\n",
            "3512 [D loss: 0.999974] [G loss: 1.000062]\n",
            "3513 [D loss: 0.999974] [G loss: 1.000044]\n",
            "3514 [D loss: 0.999989] [G loss: 1.000082]\n",
            "3515 [D loss: 0.999975] [G loss: 1.000050]\n",
            "3516 [D loss: 0.999963] [G loss: 1.000054]\n",
            "3517 [D loss: 0.999979] [G loss: 1.000072]\n",
            "3518 [D loss: 0.999957] [G loss: 1.000048]\n",
            "3519 [D loss: 0.999945] [G loss: 1.000081]\n",
            "3520 [D loss: 0.999986] [G loss: 1.000038]\n",
            "3521 [D loss: 0.999952] [G loss: 1.000049]\n",
            "3522 [D loss: 0.999943] [G loss: 1.000086]\n",
            "3523 [D loss: 0.999961] [G loss: 1.000073]\n",
            "3524 [D loss: 0.999934] [G loss: 1.000046]\n",
            "3525 [D loss: 0.999972] [G loss: 1.000055]\n",
            "3526 [D loss: 0.999958] [G loss: 1.000089]\n",
            "3527 [D loss: 0.999982] [G loss: 1.000057]\n",
            "3528 [D loss: 0.999974] [G loss: 1.000068]\n",
            "3529 [D loss: 0.999981] [G loss: 1.000061]\n",
            "3530 [D loss: 0.999968] [G loss: 1.000090]\n",
            "3531 [D loss: 0.999974] [G loss: 1.000073]\n",
            "3532 [D loss: 0.999937] [G loss: 1.000086]\n",
            "3533 [D loss: 0.999953] [G loss: 1.000078]\n",
            "3534 [D loss: 0.999993] [G loss: 1.000056]\n",
            "3535 [D loss: 0.999958] [G loss: 1.000071]\n",
            "3536 [D loss: 0.999958] [G loss: 1.000067]\n",
            "3537 [D loss: 0.999979] [G loss: 1.000051]\n",
            "3538 [D loss: 0.999966] [G loss: 1.000058]\n",
            "3539 [D loss: 0.999979] [G loss: 1.000058]\n",
            "3540 [D loss: 0.999976] [G loss: 1.000062]\n",
            "3541 [D loss: 0.999988] [G loss: 1.000058]\n",
            "3542 [D loss: 0.999959] [G loss: 1.000074]\n",
            "3543 [D loss: 0.999964] [G loss: 1.000087]\n",
            "3544 [D loss: 0.999942] [G loss: 1.000057]\n",
            "3545 [D loss: 1.000011] [G loss: 1.000083]\n",
            "3546 [D loss: 0.999981] [G loss: 1.000043]\n",
            "3547 [D loss: 0.999942] [G loss: 1.000075]\n",
            "3548 [D loss: 0.999959] [G loss: 1.000070]\n",
            "3549 [D loss: 0.999992] [G loss: 1.000099]\n",
            "3550 [D loss: 0.999957] [G loss: 1.000085]\n",
            "3551 [D loss: 0.999941] [G loss: 1.000109]\n",
            "3552 [D loss: 1.000009] [G loss: 1.000048]\n",
            "3553 [D loss: 0.999964] [G loss: 1.000049]\n",
            "3554 [D loss: 0.999950] [G loss: 1.000054]\n",
            "3555 [D loss: 0.999979] [G loss: 1.000093]\n",
            "3556 [D loss: 0.999974] [G loss: 1.000047]\n",
            "3557 [D loss: 0.999944] [G loss: 1.000073]\n",
            "3558 [D loss: 0.999976] [G loss: 1.000067]\n",
            "3559 [D loss: 0.999977] [G loss: 1.000045]\n",
            "3560 [D loss: 0.999996] [G loss: 1.000045]\n",
            "3561 [D loss: 0.999972] [G loss: 1.000079]\n",
            "3562 [D loss: 0.999970] [G loss: 1.000072]\n",
            "3563 [D loss: 0.999966] [G loss: 1.000063]\n",
            "3564 [D loss: 1.000001] [G loss: 1.000069]\n",
            "3565 [D loss: 0.999973] [G loss: 1.000059]\n",
            "3566 [D loss: 0.999997] [G loss: 1.000078]\n",
            "3567 [D loss: 0.999977] [G loss: 1.000072]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3568 [D loss: 0.999979] [G loss: 1.000062]\n",
            "3569 [D loss: 0.999974] [G loss: 1.000058]\n",
            "3570 [D loss: 0.999976] [G loss: 1.000066]\n",
            "3571 [D loss: 0.999982] [G loss: 1.000052]\n",
            "3572 [D loss: 0.999999] [G loss: 1.000058]\n",
            "3573 [D loss: 0.999972] [G loss: 1.000047]\n",
            "3574 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3575 [D loss: 0.999976] [G loss: 1.000053]\n",
            "3576 [D loss: 0.999977] [G loss: 1.000047]\n",
            "3577 [D loss: 0.999958] [G loss: 1.000052]\n",
            "3578 [D loss: 0.999989] [G loss: 1.000054]\n",
            "3579 [D loss: 0.999976] [G loss: 1.000084]\n",
            "3580 [D loss: 0.999987] [G loss: 1.000056]\n",
            "3581 [D loss: 0.999971] [G loss: 1.000078]\n",
            "3582 [D loss: 0.999967] [G loss: 1.000050]\n",
            "3583 [D loss: 0.999968] [G loss: 1.000073]\n",
            "3584 [D loss: 0.999956] [G loss: 1.000081]\n",
            "3585 [D loss: 0.999983] [G loss: 1.000072]\n",
            "3586 [D loss: 0.999978] [G loss: 1.000063]\n",
            "3587 [D loss: 0.999974] [G loss: 1.000074]\n",
            "3588 [D loss: 0.999976] [G loss: 1.000066]\n",
            "3589 [D loss: 0.999992] [G loss: 1.000042]\n",
            "3590 [D loss: 0.999963] [G loss: 1.000040]\n",
            "3591 [D loss: 0.999962] [G loss: 1.000077]\n",
            "3592 [D loss: 0.999966] [G loss: 1.000056]\n",
            "3593 [D loss: 0.999962] [G loss: 1.000025]\n",
            "3594 [D loss: 0.999967] [G loss: 1.000063]\n",
            "3595 [D loss: 0.999979] [G loss: 1.000067]\n",
            "3596 [D loss: 1.000002] [G loss: 1.000064]\n",
            "3597 [D loss: 0.999966] [G loss: 1.000094]\n",
            "3598 [D loss: 0.999989] [G loss: 1.000058]\n",
            "3599 [D loss: 0.999974] [G loss: 1.000074]\n",
            "3600 [D loss: 0.999956] [G loss: 1.000060]\n",
            "3601 [D loss: 0.999959] [G loss: 1.000056]\n",
            "3602 [D loss: 0.999942] [G loss: 1.000093]\n",
            "3603 [D loss: 0.999965] [G loss: 1.000070]\n",
            "3604 [D loss: 0.999975] [G loss: 1.000063]\n",
            "3605 [D loss: 0.999934] [G loss: 1.000063]\n",
            "3606 [D loss: 0.999983] [G loss: 1.000061]\n",
            "3607 [D loss: 0.999956] [G loss: 1.000098]\n",
            "3608 [D loss: 0.999960] [G loss: 1.000045]\n",
            "3609 [D loss: 0.999922] [G loss: 1.000056]\n",
            "3610 [D loss: 0.999994] [G loss: 1.000087]\n",
            "3611 [D loss: 0.999982] [G loss: 1.000050]\n",
            "3612 [D loss: 0.999945] [G loss: 1.000082]\n",
            "3613 [D loss: 0.999990] [G loss: 1.000076]\n",
            "3614 [D loss: 0.999964] [G loss: 1.000042]\n",
            "3615 [D loss: 0.999975] [G loss: 1.000073]\n",
            "3616 [D loss: 0.999935] [G loss: 1.000083]\n",
            "3617 [D loss: 0.999972] [G loss: 1.000102]\n",
            "3618 [D loss: 0.999935] [G loss: 1.000075]\n",
            "3619 [D loss: 0.999989] [G loss: 1.000091]\n",
            "3620 [D loss: 0.999912] [G loss: 1.000034]\n",
            "3621 [D loss: 0.999994] [G loss: 1.000106]\n",
            "3622 [D loss: 0.999965] [G loss: 1.000059]\n",
            "3623 [D loss: 0.999951] [G loss: 1.000071]\n",
            "3624 [D loss: 0.999979] [G loss: 1.000058]\n",
            "3625 [D loss: 0.999984] [G loss: 1.000075]\n",
            "3626 [D loss: 0.999980] [G loss: 1.000070]\n",
            "3627 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3628 [D loss: 0.999983] [G loss: 1.000063]\n",
            "3629 [D loss: 0.999955] [G loss: 1.000076]\n",
            "3630 [D loss: 0.999981] [G loss: 1.000043]\n",
            "3631 [D loss: 0.999980] [G loss: 1.000056]\n",
            "3632 [D loss: 0.999979] [G loss: 1.000059]\n",
            "3633 [D loss: 0.999953] [G loss: 1.000061]\n",
            "3634 [D loss: 0.999979] [G loss: 1.000043]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3635 [D loss: 0.999983] [G loss: 1.000076]\n",
            "3636 [D loss: 0.999982] [G loss: 1.000054]\n",
            "3637 [D loss: 0.999984] [G loss: 1.000040]\n",
            "3638 [D loss: 0.999958] [G loss: 1.000052]\n",
            "3639 [D loss: 0.999940] [G loss: 1.000070]\n",
            "3640 [D loss: 0.999956] [G loss: 1.000082]\n",
            "3641 [D loss: 0.999999] [G loss: 1.000054]\n",
            "3642 [D loss: 0.999964] [G loss: 1.000058]\n",
            "3643 [D loss: 0.999963] [G loss: 1.000056]\n",
            "3644 [D loss: 0.999981] [G loss: 1.000050]\n",
            "3645 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3646 [D loss: 0.999972] [G loss: 1.000051]\n",
            "3647 [D loss: 0.999970] [G loss: 1.000037]\n",
            "3648 [D loss: 0.999976] [G loss: 1.000050]\n",
            "3649 [D loss: 0.999959] [G loss: 1.000064]\n",
            "3650 [D loss: 0.999999] [G loss: 1.000044]\n",
            "3651 [D loss: 0.999966] [G loss: 1.000055]\n",
            "3652 [D loss: 0.999960] [G loss: 1.000052]\n",
            "3653 [D loss: 0.999992] [G loss: 1.000083]\n",
            "3654 [D loss: 0.999930] [G loss: 1.000098]\n",
            "3655 [D loss: 0.999974] [G loss: 1.000040]\n",
            "3656 [D loss: 0.999994] [G loss: 1.000087]\n",
            "3657 [D loss: 0.999950] [G loss: 1.000081]\n",
            "3658 [D loss: 0.999951] [G loss: 1.000065]\n",
            "3659 [D loss: 0.999962] [G loss: 1.000051]\n",
            "3660 [D loss: 0.999958] [G loss: 1.000055]\n",
            "3661 [D loss: 0.999979] [G loss: 1.000077]\n",
            "3662 [D loss: 0.999987] [G loss: 1.000076]\n",
            "3663 [D loss: 0.999938] [G loss: 1.000078]\n",
            "3664 [D loss: 0.999953] [G loss: 1.000058]\n",
            "3665 [D loss: 0.999982] [G loss: 1.000081]\n",
            "3666 [D loss: 0.999946] [G loss: 1.000082]\n",
            "3667 [D loss: 0.999968] [G loss: 1.000061]\n",
            "3668 [D loss: 0.999961] [G loss: 1.000091]\n",
            "3669 [D loss: 0.999973] [G loss: 1.000075]\n",
            "3670 [D loss: 1.000002] [G loss: 1.000058]\n",
            "3671 [D loss: 0.999974] [G loss: 1.000099]\n",
            "3672 [D loss: 0.999980] [G loss: 1.000071]\n",
            "3673 [D loss: 0.999968] [G loss: 1.000059]\n",
            "3674 [D loss: 0.999981] [G loss: 1.000065]\n",
            "3675 [D loss: 0.999981] [G loss: 1.000080]\n",
            "3676 [D loss: 0.999985] [G loss: 1.000088]\n",
            "3677 [D loss: 0.999967] [G loss: 1.000084]\n",
            "3678 [D loss: 0.999964] [G loss: 1.000042]\n",
            "3679 [D loss: 0.999940] [G loss: 1.000083]\n",
            "3680 [D loss: 0.999979] [G loss: 1.000052]\n",
            "3681 [D loss: 0.999985] [G loss: 1.000077]\n",
            "3682 [D loss: 0.999967] [G loss: 1.000038]\n",
            "3683 [D loss: 0.999962] [G loss: 1.000073]\n",
            "3684 [D loss: 0.999964] [G loss: 1.000078]\n",
            "3685 [D loss: 0.999958] [G loss: 1.000064]\n",
            "3686 [D loss: 0.999979] [G loss: 1.000090]\n",
            "3687 [D loss: 0.999965] [G loss: 1.000073]\n",
            "3688 [D loss: 0.999998] [G loss: 1.000089]\n",
            "3689 [D loss: 0.999951] [G loss: 1.000053]\n",
            "3690 [D loss: 0.999952] [G loss: 1.000098]\n",
            "3691 [D loss: 0.999959] [G loss: 1.000037]\n",
            "3692 [D loss: 0.999957] [G loss: 1.000078]\n",
            "3693 [D loss: 0.999981] [G loss: 1.000074]\n",
            "3694 [D loss: 0.999983] [G loss: 1.000072]\n",
            "3695 [D loss: 0.999968] [G loss: 1.000056]\n",
            "3696 [D loss: 0.999981] [G loss: 1.000061]\n",
            "3697 [D loss: 0.999955] [G loss: 1.000092]\n",
            "3698 [D loss: 0.999969] [G loss: 1.000095]\n",
            "3699 [D loss: 0.999968] [G loss: 1.000089]\n",
            "3700 [D loss: 0.999966] [G loss: 1.000065]\n",
            "3701 [D loss: 0.999955] [G loss: 1.000078]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3702 [D loss: 0.999975] [G loss: 1.000051]\n",
            "3703 [D loss: 0.999983] [G loss: 1.000047]\n",
            "3704 [D loss: 0.999966] [G loss: 1.000083]\n",
            "3705 [D loss: 0.999977] [G loss: 1.000073]\n",
            "3706 [D loss: 0.999972] [G loss: 1.000047]\n",
            "3707 [D loss: 0.999984] [G loss: 1.000058]\n",
            "3708 [D loss: 0.999969] [G loss: 1.000077]\n",
            "3709 [D loss: 0.999966] [G loss: 1.000083]\n",
            "3710 [D loss: 0.999954] [G loss: 1.000079]\n",
            "3711 [D loss: 0.999964] [G loss: 1.000054]\n",
            "3712 [D loss: 0.999975] [G loss: 1.000057]\n",
            "3713 [D loss: 1.000007] [G loss: 1.000058]\n",
            "3714 [D loss: 0.999955] [G loss: 1.000083]\n",
            "3715 [D loss: 0.999962] [G loss: 1.000049]\n",
            "3716 [D loss: 0.999984] [G loss: 1.000050]\n",
            "3717 [D loss: 0.999998] [G loss: 1.000077]\n",
            "3718 [D loss: 1.000005] [G loss: 1.000072]\n",
            "3719 [D loss: 0.999988] [G loss: 1.000074]\n",
            "3720 [D loss: 0.999945] [G loss: 1.000077]\n",
            "3721 [D loss: 0.999973] [G loss: 1.000056]\n",
            "3722 [D loss: 0.999935] [G loss: 1.000056]\n",
            "3723 [D loss: 0.999962] [G loss: 1.000076]\n",
            "3724 [D loss: 0.999945] [G loss: 1.000046]\n",
            "3725 [D loss: 0.999955] [G loss: 1.000065]\n",
            "3726 [D loss: 0.999981] [G loss: 1.000094]\n",
            "3727 [D loss: 0.999943] [G loss: 1.000046]\n",
            "3728 [D loss: 0.999955] [G loss: 1.000089]\n",
            "3729 [D loss: 0.999974] [G loss: 1.000083]\n",
            "3730 [D loss: 0.999979] [G loss: 1.000071]\n",
            "3731 [D loss: 0.999969] [G loss: 1.000088]\n",
            "3732 [D loss: 0.999972] [G loss: 1.000081]\n",
            "3733 [D loss: 0.999955] [G loss: 1.000053]\n",
            "3734 [D loss: 0.999958] [G loss: 1.000063]\n",
            "3735 [D loss: 1.000002] [G loss: 1.000053]\n",
            "3736 [D loss: 0.999959] [G loss: 1.000073]\n",
            "3737 [D loss: 0.999955] [G loss: 1.000053]\n",
            "3738 [D loss: 0.999929] [G loss: 1.000061]\n",
            "3739 [D loss: 0.999971] [G loss: 1.000085]\n",
            "3740 [D loss: 0.999927] [G loss: 1.000066]\n",
            "3741 [D loss: 0.999955] [G loss: 1.000049]\n",
            "3742 [D loss: 0.999947] [G loss: 1.000053]\n",
            "3743 [D loss: 0.999957] [G loss: 1.000063]\n",
            "3744 [D loss: 0.999970] [G loss: 1.000052]\n",
            "3745 [D loss: 0.999974] [G loss: 1.000075]\n",
            "3746 [D loss: 0.999971] [G loss: 1.000043]\n",
            "3747 [D loss: 0.999978] [G loss: 1.000089]\n",
            "3748 [D loss: 0.999977] [G loss: 1.000055]\n",
            "3749 [D loss: 0.999955] [G loss: 1.000048]\n",
            "3750 [D loss: 0.999970] [G loss: 1.000071]\n",
            "3751 [D loss: 0.999952] [G loss: 1.000050]\n",
            "3752 [D loss: 0.999971] [G loss: 1.000058]\n",
            "3753 [D loss: 0.999982] [G loss: 1.000032]\n",
            "3754 [D loss: 0.999963] [G loss: 1.000050]\n",
            "3755 [D loss: 0.999977] [G loss: 1.000036]\n",
            "3756 [D loss: 0.999967] [G loss: 1.000072]\n",
            "3757 [D loss: 0.999954] [G loss: 1.000057]\n",
            "3758 [D loss: 0.999939] [G loss: 1.000055]\n",
            "3759 [D loss: 0.999967] [G loss: 1.000064]\n",
            "3760 [D loss: 0.999959] [G loss: 1.000090]\n",
            "3761 [D loss: 1.000006] [G loss: 1.000056]\n",
            "3762 [D loss: 0.999981] [G loss: 1.000065]\n",
            "3763 [D loss: 0.999962] [G loss: 1.000071]\n",
            "3764 [D loss: 0.999959] [G loss: 1.000049]\n",
            "3765 [D loss: 0.999975] [G loss: 1.000066]\n",
            "3766 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3767 [D loss: 0.999954] [G loss: 1.000035]\n",
            "3768 [D loss: 0.999959] [G loss: 1.000073]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3769 [D loss: 0.999942] [G loss: 1.000065]\n",
            "3770 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3771 [D loss: 0.999978] [G loss: 1.000082]\n",
            "3772 [D loss: 0.999957] [G loss: 1.000065]\n",
            "3773 [D loss: 0.999982] [G loss: 1.000049]\n",
            "3774 [D loss: 0.999996] [G loss: 1.000031]\n",
            "3775 [D loss: 0.999984] [G loss: 1.000059]\n",
            "3776 [D loss: 0.999961] [G loss: 1.000069]\n",
            "3777 [D loss: 0.999960] [G loss: 1.000047]\n",
            "3778 [D loss: 0.999989] [G loss: 1.000061]\n",
            "3779 [D loss: 0.999984] [G loss: 1.000073]\n",
            "3780 [D loss: 0.999956] [G loss: 1.000041]\n",
            "3781 [D loss: 0.999979] [G loss: 1.000069]\n",
            "3782 [D loss: 0.999961] [G loss: 1.000060]\n",
            "3783 [D loss: 0.999973] [G loss: 1.000052]\n",
            "3784 [D loss: 0.999943] [G loss: 1.000035]\n",
            "3785 [D loss: 0.999984] [G loss: 1.000067]\n",
            "3786 [D loss: 0.999978] [G loss: 1.000070]\n",
            "3787 [D loss: 0.999966] [G loss: 1.000079]\n",
            "3788 [D loss: 0.999973] [G loss: 1.000036]\n",
            "3789 [D loss: 0.999962] [G loss: 1.000069]\n",
            "3790 [D loss: 0.999968] [G loss: 1.000057]\n",
            "3791 [D loss: 0.999960] [G loss: 1.000063]\n",
            "3792 [D loss: 0.999958] [G loss: 1.000042]\n",
            "3793 [D loss: 0.999947] [G loss: 1.000054]\n",
            "3794 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3795 [D loss: 0.999955] [G loss: 1.000065]\n",
            "3796 [D loss: 0.999989] [G loss: 1.000056]\n",
            "3797 [D loss: 0.999987] [G loss: 1.000036]\n",
            "3798 [D loss: 0.999980] [G loss: 1.000091]\n",
            "3799 [D loss: 0.999973] [G loss: 1.000040]\n",
            "3800 [D loss: 0.999990] [G loss: 1.000081]\n",
            "3801 [D loss: 0.999959] [G loss: 1.000057]\n",
            "3802 [D loss: 0.999987] [G loss: 1.000066]\n",
            "3803 [D loss: 0.999969] [G loss: 1.000050]\n",
            "3804 [D loss: 0.999968] [G loss: 1.000019]\n",
            "3805 [D loss: 0.999956] [G loss: 1.000036]\n",
            "3806 [D loss: 0.999971] [G loss: 1.000055]\n",
            "3807 [D loss: 0.999967] [G loss: 1.000066]\n",
            "3808 [D loss: 0.999986] [G loss: 1.000033]\n",
            "3809 [D loss: 0.999966] [G loss: 1.000071]\n",
            "3810 [D loss: 0.999984] [G loss: 1.000067]\n",
            "3811 [D loss: 0.999977] [G loss: 1.000069]\n",
            "3812 [D loss: 0.999960] [G loss: 1.000059]\n",
            "3813 [D loss: 0.999973] [G loss: 1.000073]\n",
            "3814 [D loss: 0.999956] [G loss: 1.000071]\n",
            "3815 [D loss: 0.999950] [G loss: 1.000051]\n",
            "3816 [D loss: 0.999982] [G loss: 1.000077]\n",
            "3817 [D loss: 0.999955] [G loss: 1.000080]\n",
            "3818 [D loss: 0.999993] [G loss: 1.000053]\n",
            "3819 [D loss: 0.999974] [G loss: 1.000063]\n",
            "3820 [D loss: 0.999932] [G loss: 1.000079]\n",
            "3821 [D loss: 0.999975] [G loss: 1.000059]\n",
            "3822 [D loss: 0.999985] [G loss: 1.000043]\n",
            "3823 [D loss: 0.999974] [G loss: 1.000059]\n",
            "3824 [D loss: 0.999952] [G loss: 1.000083]\n",
            "3825 [D loss: 0.999977] [G loss: 1.000057]\n",
            "3826 [D loss: 0.999980] [G loss: 1.000091]\n",
            "3827 [D loss: 0.999990] [G loss: 1.000036]\n",
            "3828 [D loss: 0.999975] [G loss: 1.000043]\n",
            "3829 [D loss: 0.999969] [G loss: 1.000072]\n",
            "3830 [D loss: 0.999965] [G loss: 1.000030]\n",
            "3831 [D loss: 0.999946] [G loss: 1.000094]\n",
            "3832 [D loss: 0.999945] [G loss: 1.000063]\n",
            "3833 [D loss: 0.999959] [G loss: 1.000077]\n",
            "3834 [D loss: 0.999978] [G loss: 1.000063]\n",
            "3835 [D loss: 0.999970] [G loss: 1.000059]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3836 [D loss: 0.999949] [G loss: 1.000068]\n",
            "3837 [D loss: 0.999971] [G loss: 1.000048]\n",
            "3838 [D loss: 0.999979] [G loss: 1.000037]\n",
            "3839 [D loss: 0.999984] [G loss: 1.000057]\n",
            "3840 [D loss: 0.999960] [G loss: 1.000078]\n",
            "3841 [D loss: 0.999957] [G loss: 1.000087]\n",
            "3842 [D loss: 0.999976] [G loss: 1.000076]\n",
            "3843 [D loss: 0.999936] [G loss: 1.000074]\n",
            "3844 [D loss: 0.999976] [G loss: 1.000048]\n",
            "3845 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3846 [D loss: 0.999984] [G loss: 1.000064]\n",
            "3847 [D loss: 1.000008] [G loss: 1.000077]\n",
            "3848 [D loss: 0.999930] [G loss: 1.000108]\n",
            "3849 [D loss: 0.999972] [G loss: 1.000079]\n",
            "3850 [D loss: 0.999984] [G loss: 1.000050]\n",
            "3851 [D loss: 0.999969] [G loss: 1.000090]\n",
            "3852 [D loss: 0.999960] [G loss: 1.000093]\n",
            "3853 [D loss: 1.000003] [G loss: 1.000099]\n",
            "3854 [D loss: 0.999964] [G loss: 1.000072]\n",
            "3855 [D loss: 0.999961] [G loss: 1.000094]\n",
            "3856 [D loss: 0.999970] [G loss: 1.000052]\n",
            "3857 [D loss: 0.999987] [G loss: 1.000068]\n",
            "3858 [D loss: 0.999958] [G loss: 1.000060]\n",
            "3859 [D loss: 0.999969] [G loss: 1.000056]\n",
            "3860 [D loss: 0.999955] [G loss: 1.000067]\n",
            "3861 [D loss: 0.999950] [G loss: 1.000091]\n",
            "3862 [D loss: 0.999972] [G loss: 1.000072]\n",
            "3863 [D loss: 0.999984] [G loss: 1.000043]\n",
            "3864 [D loss: 0.999959] [G loss: 1.000081]\n",
            "3865 [D loss: 0.999962] [G loss: 1.000063]\n",
            "3866 [D loss: 0.999976] [G loss: 1.000047]\n",
            "3867 [D loss: 0.999936] [G loss: 1.000077]\n",
            "3868 [D loss: 0.999954] [G loss: 1.000042]\n",
            "3869 [D loss: 0.999956] [G loss: 1.000063]\n",
            "3870 [D loss: 0.999982] [G loss: 1.000066]\n",
            "3871 [D loss: 0.999975] [G loss: 1.000083]\n",
            "3872 [D loss: 0.999944] [G loss: 1.000077]\n",
            "3873 [D loss: 0.999968] [G loss: 1.000072]\n",
            "3874 [D loss: 0.999969] [G loss: 1.000076]\n",
            "3875 [D loss: 0.999966] [G loss: 1.000050]\n",
            "3876 [D loss: 0.999960] [G loss: 1.000055]\n",
            "3877 [D loss: 0.999964] [G loss: 1.000049]\n",
            "3878 [D loss: 0.999977] [G loss: 1.000063]\n",
            "3879 [D loss: 0.999979] [G loss: 1.000064]\n",
            "3880 [D loss: 0.999962] [G loss: 1.000060]\n",
            "3881 [D loss: 0.999974] [G loss: 1.000056]\n",
            "3882 [D loss: 0.999974] [G loss: 1.000047]\n",
            "3883 [D loss: 0.999964] [G loss: 1.000062]\n",
            "3884 [D loss: 0.999981] [G loss: 1.000049]\n",
            "3885 [D loss: 0.999966] [G loss: 1.000069]\n",
            "3886 [D loss: 0.999984] [G loss: 1.000041]\n",
            "3887 [D loss: 0.999961] [G loss: 1.000087]\n",
            "3888 [D loss: 0.999951] [G loss: 1.000074]\n",
            "3889 [D loss: 1.000011] [G loss: 1.000048]\n",
            "3890 [D loss: 0.999990] [G loss: 1.000097]\n",
            "3891 [D loss: 0.999944] [G loss: 1.000063]\n",
            "3892 [D loss: 0.999956] [G loss: 1.000043]\n",
            "3893 [D loss: 0.999958] [G loss: 1.000072]\n",
            "3894 [D loss: 0.999950] [G loss: 1.000098]\n",
            "3895 [D loss: 0.999942] [G loss: 1.000079]\n",
            "3896 [D loss: 0.999983] [G loss: 1.000073]\n",
            "3897 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3898 [D loss: 0.999961] [G loss: 1.000061]\n",
            "3899 [D loss: 0.999935] [G loss: 1.000058]\n",
            "3900 [D loss: 0.999949] [G loss: 1.000069]\n",
            "3901 [D loss: 0.999978] [G loss: 1.000041]\n",
            "3902 [D loss: 0.999963] [G loss: 1.000057]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3903 [D loss: 1.000010] [G loss: 1.000048]\n",
            "3904 [D loss: 0.999984] [G loss: 1.000074]\n",
            "3905 [D loss: 0.999962] [G loss: 1.000050]\n",
            "3906 [D loss: 0.999977] [G loss: 1.000049]\n",
            "3907 [D loss: 0.999979] [G loss: 1.000065]\n",
            "3908 [D loss: 0.999956] [G loss: 1.000055]\n",
            "3909 [D loss: 0.999988] [G loss: 1.000067]\n",
            "3910 [D loss: 0.999978] [G loss: 1.000103]\n",
            "3911 [D loss: 0.999989] [G loss: 1.000065]\n",
            "3912 [D loss: 0.999971] [G loss: 1.000040]\n",
            "3913 [D loss: 0.999977] [G loss: 1.000062]\n",
            "3914 [D loss: 0.999934] [G loss: 1.000079]\n",
            "3915 [D loss: 0.999955] [G loss: 1.000057]\n",
            "3916 [D loss: 0.999948] [G loss: 1.000080]\n",
            "3917 [D loss: 0.999971] [G loss: 1.000059]\n",
            "3918 [D loss: 0.999964] [G loss: 1.000083]\n",
            "3919 [D loss: 0.999945] [G loss: 1.000066]\n",
            "3920 [D loss: 0.999978] [G loss: 1.000072]\n",
            "3921 [D loss: 0.999945] [G loss: 1.000068]\n",
            "3922 [D loss: 0.999972] [G loss: 1.000056]\n",
            "3923 [D loss: 0.999974] [G loss: 1.000053]\n",
            "3924 [D loss: 0.999975] [G loss: 1.000030]\n",
            "3925 [D loss: 0.999971] [G loss: 1.000058]\n",
            "3926 [D loss: 0.999982] [G loss: 1.000095]\n",
            "3927 [D loss: 0.999951] [G loss: 1.000054]\n",
            "3928 [D loss: 0.999960] [G loss: 1.000040]\n",
            "3929 [D loss: 0.999964] [G loss: 1.000065]\n",
            "3930 [D loss: 0.999962] [G loss: 1.000064]\n",
            "3931 [D loss: 0.999965] [G loss: 1.000080]\n",
            "3932 [D loss: 0.999974] [G loss: 1.000078]\n",
            "3933 [D loss: 0.999965] [G loss: 1.000094]\n",
            "3934 [D loss: 0.999943] [G loss: 1.000087]\n",
            "3935 [D loss: 0.999972] [G loss: 1.000076]\n",
            "3936 [D loss: 0.999977] [G loss: 1.000067]\n",
            "3937 [D loss: 0.999986] [G loss: 1.000065]\n",
            "3938 [D loss: 0.999977] [G loss: 1.000074]\n",
            "3939 [D loss: 0.999950] [G loss: 1.000058]\n",
            "3940 [D loss: 0.999963] [G loss: 1.000062]\n",
            "3941 [D loss: 0.999957] [G loss: 1.000053]\n",
            "3942 [D loss: 0.999977] [G loss: 1.000068]\n",
            "3943 [D loss: 0.999989] [G loss: 1.000091]\n",
            "3944 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3945 [D loss: 0.999961] [G loss: 1.000042]\n",
            "3946 [D loss: 0.999996] [G loss: 1.000076]\n",
            "3947 [D loss: 0.999966] [G loss: 1.000058]\n",
            "3948 [D loss: 0.999972] [G loss: 1.000059]\n",
            "3949 [D loss: 0.999963] [G loss: 1.000073]\n",
            "3950 [D loss: 0.999937] [G loss: 1.000060]\n",
            "3951 [D loss: 0.999973] [G loss: 1.000057]\n",
            "3952 [D loss: 0.999994] [G loss: 1.000116]\n",
            "3953 [D loss: 0.999975] [G loss: 1.000066]\n",
            "3954 [D loss: 0.999948] [G loss: 1.000062]\n",
            "3955 [D loss: 0.999977] [G loss: 1.000077]\n",
            "3956 [D loss: 0.999975] [G loss: 1.000078]\n",
            "3957 [D loss: 0.999950] [G loss: 1.000048]\n",
            "3958 [D loss: 0.999958] [G loss: 1.000083]\n",
            "3959 [D loss: 0.999950] [G loss: 1.000066]\n",
            "3960 [D loss: 0.999970] [G loss: 1.000039]\n",
            "3961 [D loss: 0.999968] [G loss: 1.000090]\n",
            "3962 [D loss: 0.999953] [G loss: 1.000084]\n",
            "3963 [D loss: 0.999975] [G loss: 1.000075]\n",
            "3964 [D loss: 0.999987] [G loss: 1.000088]\n",
            "3965 [D loss: 0.999998] [G loss: 1.000043]\n",
            "3966 [D loss: 0.999966] [G loss: 1.000028]\n",
            "3967 [D loss: 0.999972] [G loss: 1.000048]\n",
            "3968 [D loss: 0.999999] [G loss: 1.000031]\n",
            "3969 [D loss: 0.999969] [G loss: 1.000077]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3970 [D loss: 0.999949] [G loss: 1.000053]\n",
            "3971 [D loss: 0.999973] [G loss: 1.000045]\n",
            "3972 [D loss: 0.999957] [G loss: 1.000059]\n",
            "3973 [D loss: 0.999961] [G loss: 1.000050]\n",
            "3974 [D loss: 0.999986] [G loss: 1.000073]\n",
            "3975 [D loss: 0.999994] [G loss: 1.000066]\n",
            "3976 [D loss: 0.999988] [G loss: 1.000049]\n",
            "3977 [D loss: 0.999969] [G loss: 1.000061]\n",
            "3978 [D loss: 0.999969] [G loss: 1.000073]\n",
            "3979 [D loss: 0.999973] [G loss: 1.000047]\n",
            "3980 [D loss: 0.999978] [G loss: 1.000022]\n",
            "3981 [D loss: 0.999967] [G loss: 1.000063]\n",
            "3982 [D loss: 0.999964] [G loss: 1.000063]\n",
            "3983 [D loss: 0.999973] [G loss: 1.000079]\n",
            "3984 [D loss: 0.999956] [G loss: 1.000074]\n",
            "3985 [D loss: 0.999971] [G loss: 1.000075]\n",
            "3986 [D loss: 0.999939] [G loss: 1.000060]\n",
            "3987 [D loss: 0.999947] [G loss: 1.000082]\n",
            "3988 [D loss: 0.999985] [G loss: 1.000074]\n",
            "3989 [D loss: 0.999983] [G loss: 1.000072]\n",
            "3990 [D loss: 0.999924] [G loss: 1.000069]\n",
            "3991 [D loss: 0.999980] [G loss: 1.000065]\n",
            "3992 [D loss: 0.999939] [G loss: 1.000045]\n",
            "3993 [D loss: 0.999989] [G loss: 1.000067]\n",
            "3994 [D loss: 0.999952] [G loss: 1.000049]\n",
            "3995 [D loss: 0.999985] [G loss: 1.000044]\n",
            "3996 [D loss: 0.999953] [G loss: 1.000054]\n",
            "3997 [D loss: 0.999954] [G loss: 1.000058]\n",
            "3998 [D loss: 0.999988] [G loss: 1.000039]\n",
            "3999 [D loss: 0.999955] [G loss: 1.000081]\n",
            "CPU times: user 22min 55s, sys: 5min 54s, total: 28min 50s\n",
            "Wall time: 22min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cplXquxZutDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfc8b1a6-abe3-406a-d9d4-1c599cd97331"
      },
      "cell_type": "code",
      "source": [
        "cd images\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rn5ORQxfu4kR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "7044f789-7d84-43ac-cc9e-f8d7579f1d9c"
      },
      "cell_type": "code",
      "source": [
        "list=os.listdir()\n",
        "np.sort(list)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mnist_0.png', 'mnist_100.png', 'mnist_1000.png', 'mnist_1050.png',\n",
              "       'mnist_1100.png', 'mnist_1150.png', 'mnist_1200.png',\n",
              "       'mnist_1250.png', 'mnist_1300.png', 'mnist_1350.png',\n",
              "       'mnist_1400.png', 'mnist_1450.png', 'mnist_150.png',\n",
              "       'mnist_1500.png', 'mnist_1550.png', 'mnist_1600.png',\n",
              "       'mnist_1650.png', 'mnist_1700.png', 'mnist_1750.png',\n",
              "       'mnist_1800.png', 'mnist_1850.png', 'mnist_1900.png',\n",
              "       'mnist_1950.png', 'mnist_200.png', 'mnist_2000.png',\n",
              "       'mnist_2050.png', 'mnist_2100.png', 'mnist_2150.png',\n",
              "       'mnist_2200.png', 'mnist_2250.png', 'mnist_2300.png',\n",
              "       'mnist_2350.png', 'mnist_2400.png', 'mnist_2450.png',\n",
              "       'mnist_250.png', 'mnist_2500.png', 'mnist_2550.png',\n",
              "       'mnist_2600.png', 'mnist_2650.png', 'mnist_2700.png',\n",
              "       'mnist_2750.png', 'mnist_2800.png', 'mnist_2850.png',\n",
              "       'mnist_2900.png', 'mnist_2950.png', 'mnist_300.png',\n",
              "       'mnist_3000.png', 'mnist_3050.png', 'mnist_3100.png',\n",
              "       'mnist_3150.png', 'mnist_3200.png', 'mnist_3250.png',\n",
              "       'mnist_3300.png', 'mnist_3350.png', 'mnist_3400.png',\n",
              "       'mnist_3450.png', 'mnist_350.png', 'mnist_3500.png',\n",
              "       'mnist_3550.png', 'mnist_3600.png', 'mnist_3650.png',\n",
              "       'mnist_3700.png', 'mnist_3750.png', 'mnist_3800.png',\n",
              "       'mnist_3850.png', 'mnist_3900.png', 'mnist_3950.png',\n",
              "       'mnist_400.png', 'mnist_450.png', 'mnist_50.png', 'mnist_500.png',\n",
              "       'mnist_550.png', 'mnist_600.png', 'mnist_650.png', 'mnist_700.png',\n",
              "       'mnist_750.png', 'mnist_800.png', 'mnist_850.png', 'mnist_900.png',\n",
              "       'mnist_950.png'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "zsT_fPkljSae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "45cf9959-3b85-430d-8cfc-8ce7dcce14e0"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "sides= 2\n",
        "img_it= 0\n",
        "for i in range(0, sides) :\n",
        "    for j in range(0, sides) :\n",
        "        img = mpimg.imread(list[img_it])\n",
        "        plt.subplot(sides, sides, 1 + img_it)\n",
        "        plt.axis(\"off\")\n",
        "        img_it +=1\n",
        "        plt.imshow(img )\n",
        "        \n",
        "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=.2)\n",
        "plt.show()        \n",
        "        "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkwAAAGvCAYAAACpXIpyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5aefcbbbe0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "p0Dsn-yKjSif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "e251071c-dcdb-488a-dd95-92fb82589a11"
      },
      "cell_type": "code",
      "source": [
        "img=mpimg.imread('mnist_3950.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 575.5, 395.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFOCAYAAADKJGf5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXd4FNXawH+zLT1ASOi9NwWl2PCi\nKBdFUdSLBcQPC+q1C4oNsXcv3CuWC1iwoBcUEVREkS4iRQREsQKRHkoIIZC2u98f85zDbLJJNmUz\nu8n7ex4fye7M7MyZc8573ve8xfD7/X4EQRAEQahyHHbfgCAIgiDUVETICoIgCEKYECErCIIgCGFC\nhKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKY\nECErCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQ\nJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIg\nhAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIg\nCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIg\nCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIg\nCIIQJkTICoIgCEKYcNl9A7UZwzDsvoWgxMTEkJuba9vvh9IuLpfZdb1er/7M7/eH/Z58Pl/YfiOU\n36+q65TWVh6Ph4KCAn18Wc8cznYvi/K2i2EY1fIunU4nAIWFhWH7jdKI1LmlUaNG7N692+7bqFZE\nk40A1MC3Y2AE+938/Pxqv49QUPfqdrtZt24d69atY+/evezdu5cffviB5557LmxtaNf7KS+9evVi\n+vTpTJ8+nXr16uFyuXA6nXrSV5T2LPn5+fp5O3ToUOrv2d0msbGxxMbGhny83+/H5/OFfbEULf2l\nKgnlmQ8ePFhNdxM5iJAVBEEQhDAh5uIIoDzmNqWRWM2kVf3bkboCV/dVt25dGjVqBEBiYiIAd911\nF9u2bcPtdpepiRuGQUxMTIBJtCyznp0m0fKQm5vLN998A8ChQ4dwuVzUqVMHgMOHD+v2aty4Mb/+\n+itgbg8cO3Ys4Bm7dOkClG3utLtd8vLyQjrO2qer457tMhNXBYZh4PF4AGjXrh3p6emAOe4ADhw4\nQG5ubrGthJLmktjYWP2e4uPjw337EYcI2SgiPj6ehIQEAPbt2xe237Fr37EskpKSAFi3bh3169cH\n4OjRowCsX7+e7OzskK5jGAZt2rTh999/B9DCtjTsFiahcvjwYe655x4A/vjjDx588EG9IIuJiaFn\nz54APPvss0yYMAGAnJycYs+n2jXSzXuhvpdoeX+RgN/v1wvVP/74Q/9b9Qk1P4TSpn6/n2PHjum/\n7fT1sAsxF0cBaq+jdevWOBwOHI6qf20Oh4OYmBhiYmK0U5FdBHs+h8PBsGHDGDZsGA0bNgz43OFw\n0LFjRxwOR0haeJs2bXjmmWdo2bIlLVu2DKk9g+1rRiLffPMNTZo0oUmTJgwZMoQtW7aQl5dHXl4e\naWlpeL1evF4vv//+O/n5+eTn5xebLD0eD9deey3XXnttmQuXcPRFwX78fj9+v5+8vDz9b/VfZYhU\nf49wIiMkCqhXrx716tVj8ODBJCYmapNfVWAYBomJiSQkJGiBFYmrfp/Px7Rp05g2bRo7duzQwiIj\nI4OMjAxSU1OB0Mzo77//PhdddBEbN25k48aNDB8+HJfLpRcXVke0aHFgcbvduN1u5syZQ3p6Ounp\n6bz55ptkZmZSv3596tevT4sWLfRz7t69Wy+qFHXq1KFOnTrce++9jBkzhjFjxgR8X9Lv1lReeOEF\nXnjhBVatWsWqVauYNWsWaWlppKWl2b4QDTfB+r0SspUZD+VxUqspiJAVBEEQhDBh+CNRbakllLUi\nVBrrW2+9BUCnTp04fPgwAH/729+A0jW3tLQ0/vvf/wLw9ddfM3HiRIAAs2dubi4+n48XX3yRJ598\nEqBKzEKVwel0lrovPHjwYN5//33guCNFXl4e06dP57777itzHzE7OzvAGuD3+7npppsA0yEoKysL\ngMmTJ+trK+zar1bvLNjvG4ahNas+ffrQqlUrAFasWMFjjz3G4MGDAVi8eDHXXnstYPabE044ATD3\ncdesWRPglKLe/4ABA1i4cGGJ9xWKo1k4UZpRqA5QZfHGG28AcN1115V6nNfrZePGjQD07t27xHFo\n1ziqjLapHAMhcA9V9cHk5GQyMzMrdG27+4sd1GybRxSihGf79u355z//CUCTJk0AU6CowaM6+eTJ\nk3n88ccBU3ioyXbWrFlcdNFF+roXXXSRHiTWAaiEzZgxY3j66aeBqvNcrihlCbLNmzdr7011rzk5\nOaxdu1YvQkqjqLn92LFjLF68GIAdO3boibGqJu6qoLTJ2u/363a46KKLGDZsGAD9+vXjxhtv1J6i\nR44c0dcxDEN7iy5btqyY2XfLli0AAYkDDMModh+R2FdcLldI3r1qHLzyyisAeryFgmEYNG/eHDD3\npe1uh4ricrn0O1VbRfXq1dPfq5hpgLi4OACaNm3KoEGD+OKLL8rtGFeVW13RgpiLBUEQBCFMiCYb\nYSittWfPnrRp0wY4Hp9m1UDVCn7Xrl0cOXJEf67iR5UWq1bYVk3F7XazZMkSffz69eu54YYbomY1\nvm3bNtavXw9A3759AdNs2LFjx3LFJyotrWnTphHp7FUeVH/44IMPGDNmDAAtWrRgy5Yt2jxnfUa/\n38/8+fMBaN26Ndu3bw9Iudi4cWMAVq1axQUXXADAypUri4U72d1uwfpsqCb9H374AYDu3buXem31\nzG63W/87Ozub2267DYi+mFirU1N8fLy2cOXm5pKfn092djadOnUCoGvXrjpc7uabbwZMa5uyjiiS\nk5NL9URXvxctc0xVIkI2wvjf//6n/3/fffcB6Emzd+/euiOr0InXXntNT3RxcXE6cFwRzAuyoKBA\nCyeF3ZNlefB6vVpwqAkiISGBl19+udTz/v73vwf8fckllwDR9exl8cADD+g2adGiBVD28+3cuZOZ\nM2cyZMgQwAzhUZNoTk6OnmSDCRO72y6YQC1NyDocDr3gTE5OBmDPnj2A2Ye+/vprwFzsqrjQZs2a\nAeYCRo3JlJQU+vXrB8BHH31kezuUB6vPRW5urjbh+nw+6tSpw4gRI3QMNZhJTeD49klmZiZpaWkB\n4VvffPMNJ598MhBckCoha42ZrS2IuVgQBEEQwoR4F9tIqB6ASjNp2rSpNm1t3boVMLUQ5YCxZs2a\nADNOp06ddOo86+85nU692izp9UdyZRWHw8Hw4cMBePvttwHTQWPs2LG89NJLQc9p166dzvAU6u8E\nI1K9RdX3hYWFWsNITU3lwIEDIV3f4XBoE+HatWv57bffAJg0aRLvvfceULIjWCT3ldLOUd7Ve/fu\nBUwva2UO9vl82sN25cqVgKmhKW0Njo/Bdu3alag9R2p/UXOKx+PRjk7dunXjlVdeoW3btgHnr1u3\nDjA91xUDBgxg/PjxnHrqqYBp8VBzzZgxY2jQoAFgbsl8++23un2cTmfUmdcri5iLowDVQU844QQd\nkqNMPHFxcQFJFNSg7t27dzEBq/Z4d+zYEdEdPSkpSYcOqPssOlmpZ1afO51O9u3bh8Ph0O0VHx/P\nL7/8AqAXIgoVAlRTUCknrSa8UAUsmH3sr7/+Asy+ovb2P//881JDLqIhUUdRVJ9RITglodpULWyL\nmkFHjx4dcL1oQj1LYWEhHTt2BGDOnDk6JEo907JlyzjrrLOKnT9//ny+/PJLfe5FF13EZZddpr9T\nQnzixIk6lzZEbsrWcCKarI2EOkGpFfWePXu0E1QwMjMzad26NWCuyq2vNi4uLmBfpKzXbme38Hg8\nehJwuVzExsZy5MgRff8ej0cL2WnTpgHmvtHq1au57777tMalNLNgVFQ4RKpmoooAHDp0KEBrKM/1\n1X6ZtXhCamqq9gOIRqtHRa81evRoHdKmrEPWbEdZWVkMGjQIMDXdSGubUNvF7Xazc+dOwIyrB1Pw\nqkV8qGFsDodDa/y9evXSn59wwgn8/PPPAcfWNpEje7KCIAiCECbEXBwFWD1og6E0jVatWpGTkwOY\n+Y7vvvtuvWfSp08fbrnlFgCmT58e7luuFIWFhXq1q5LYW7F6KI4aNQqAKVOmaFN6SahrhrOCkV1Y\n9worot0NHz48IE/x8uXLAdMTXZmgVcHzmkpKSgr3338/AHfccUexBB05OTl8/vnnAIwbN07vyUaj\nZqY8q3///XetwYI5tipSjs7v99OyZUvA7H/KElJUi62NiLnYRkKdDNVxXbt21WEnTzzxRKnnOBwO\nzj//fN555x3AFLrKwaN58+YRXSe0PELi9ttvB2DChAmlJm3/9NNPufHGGwHTBLp582Y9iZan/Fak\nmv/Us+Tl5eljy1PsoehxVsFa3nOrk6owF6tnzc/PDzCxq8XdpZdeCsCSJUt0O6uwlrKI1P5y7733\nAvDcc88FHFvR9mzcuDHbtm0DzPa8+uqrAZgxY0bAcdGcHauiiLlYEARBEMKEaLI2Em7PTJfLpZ03\nxowZo019sbGxZa4mo0U7Uc5NU6dOpW/fvgGB9oWFhXTp0gUwvSlV8oDHHnuMpk2bBoRGWXP0lkak\naiYqr6xKoACmVtu3b1/Wrl0b9Byl+efl5QV4JZfXZBgtfaUo3bp1A8ySdgDnnXee/i4/P1+HrCgv\n5Io8ZyT2l7p16+r+HhMTo4999dVXufXWW0O6tiqLeeKJJwKmlq/6TEZGBu3atQPQ21eK2qjJyp5s\nDaawsFCnx7MWe7/88sv54IMP7Ly1SqMmBiUoJkyYgMfjISkpiU8++QQwJ8rTTz8dgKuuuooBAwYE\nnGMNU1CmZFUoINoI5kkcExPDmjVreOyxxwBzcaH6wCuvvMLIkSOB4oXXK7InF8k4HA5SUlKA41Wn\n4Lg3rUqvaBWyUDnhGsmhTfHx8drs7fP59PONGzeu2LEul0u315lnngmYzzZx4kRcLpfOBpaQkKC9\nlEeOHFlMuCoiuV3ChZiLBUEQBCFMiLnYRpQ3Z1n1FZUn4IoVK3Twtzq3tNfncrkCErqrf8fExER0\nnGyjRo20k1ZJqHjh1157DYCBAweSnJwcoNHt3buX8ePHA6azl9Jcnn32WbZv366LMWzbtk3HAwbT\n4qz1Wu2qhVkeDUCVqVMx0wprnCccTwygNNloix2G0u9ZmdAvvfRSXS/4jDPO0H1LxblOmjQJICCf\nd2JiotbGVPtUxLM6Es3FiYmJ2lxsjVhISkoqUQO14nA4aNOmDd9//71O2AHHM0P169evVE22Jnuo\nB0M0WUEQBEEIE7InayOhpjZUDhhdu3bVK9QOHToABKROhON7c8uXL+e0004L+E7FUgZbXUfSXklZ\nhaANw9DOE2r13KxZMzp16kT9+vX1sxiGoZ950aJFrFq1CjDLA/r9fs4++2zAXJkrrceqCVsdNKwx\npJGOSp8JZqk6lSKxYcOGOnuRtdwZwHfffVe9N1kNqNCsRYsWcccddwDmc6t9RBXzqiwawc6FmpcK\nMCcnh6lTpwJw9913688nTJigNf7S8Pv9HDhwAK/Xq61j+fn5HD58GKidlXZKQ4SsjYQ6eFU+UZ/P\npyfGp556CjCdFVQpvOuvv75EYfnNN9+wadOmEn8jknYNitYsLYrf79eT4OzZswHTNDxgwAAdnwdm\n7t6PPvoIgD/++EO3t9vtJi0tjVdeeUUf+/jjjwPBy3T5/f6onThOOeWUgL8bNmwImIsSFSs5dOhQ\n3nrrrZCvGUkLstJQfXr37t2MGDECgO+//14vmJRToJXevXsDNbvuqd/v1/HzViF7zTXX8O677/Ln\nn38G9bZX793tdtO5c2fy8/PJysoCTBO02spKTk7WAremLVAqgpiLBUEQBCFMiCYbBViLAqg4yDVr\n1gBw6623cv311wPBNYzXX38dgJtvvrnU37A690SSVhsMwzC0w0b79u0BqF+/PgMHDgSOJzV/9913\ntWYyevRoHYKwf/9+GjRooE2nX375JS+++KK+fjBHl9KySUUTyuknKyuLrl27AubzvvDCC0yZMqXM\n8x0OB06nU1sbiob/RCqqdF+9evV0DKfKUKS0Vms6QGtFq5rI+vXrAbj44ouZNWsWYG5fDR8+nCZN\nmnDNNdcApmlZtYPSVC+44AIuueQSMjMz9fUaNmyotd/StNdosYJUJTVj5qjhqAFhndBU/uHMzEy+\n/fZbAC688ELuuecegJCTKyj8fn/ABBPJ+P1+zjjjDABdXuu8887TsZD33XcfYHqNPv/884CZeEAJ\n5vz8fBwOBxs2bADg6aef1qZDl8vFG2+8ARxPpwcUy2Mb7eTl5elKK2A+t3rvRYWL+rx169YkJycH\nbDtEmyAqLCzU5Q9LI9LHQFUxd+7cgCpDLpeLm266Sff9efPm6QXmhx9+CJim4W3btjFjxgxdTvHe\ne+9l6dKlgLknG2n1de0kOpahgiAIghCFSJysjYS6WlYrzT179jBhwgTguOOTlap8lZEa+6hQDj2q\nakqjRo045ZRTMAxDewj7fD7q1asHmOYsFTe6YcMGsrKydMyr3++nR48eAPTs2ZOePXsC6CxQcNyK\nYJdDTDg0K+UApLIgKQ/cVatWaeuJ2+0OKBZQtBax0+kM2Us+HES6xhmJcbJlERMTo/v+9u3btRe+\nqni1fft2fD4f//rXv/QYatmypd5uyMjIKPG57e4vdiBC1kbKW4VnyJAh2iRTVphLZe/LTq9Ah8MR\nEIZjTf1WGmlpaezfv7/cE5vL5SI1NRWAI0eO6EnAGsYRiUK2JPNuqKgkJ4cOHeLYsWN6b3/BggUB\n/au069udizZShay6L7vGUVW2i+r7DRo0AMwF7fnnn8+xY8eYP38+AOnp6SF54LvdbtsSutiFCFkb\nKe9AqC5njEgQsuo5I8UBJRInTZXg/qeffqpQG/Xq1QuAlStXsmvXLi1kFy1aVK7njHSrhx1E4qIs\nHL9R3ndv99xiB7InKwiCIAhhQryLbUTlyS26z1USVakxKI9Br9db7Lp2a47VZYK0mqStmnNRrdVa\nPs8urFVTFCqxxJ9//qlN26Hep9Pp1FVncnJy+Ouvv7TXrcvlqnUmvarG7v5SHURT6T87ESFrIyps\nZN++fTp8JpyZhZQJyxq6kZ2dXczsE6xsWnXi8Xj0vRYUFODz+Yrty1Z2sDqdTp1KsV27djr+2Ov1\naoGlwqCKpiC0A/VOnE6nvpetW7cCgYn/S2sXp9OpnZ2aN2+un3/hwoVMmzZNH1e/fn327dsHBF+E\nWYmWOFkhMrB7HNmBjBBBEARBCBPi+CQIgiAIYUI0WUEQBEEIEyJkBUEQBCFMiJAVBEEQhDAhQlYQ\nBEEQwoQIWUEQBEEIEyJkBUEQBCFMSDIKGykrMFt9by0iXh0RV9Ga9D2ceY4jMXdxWTidzgq9x9jY\nWF084ODBg0Grpqj7SUpKIisrq9y/UVUEaxeVHUsleLGDaOwvVYl1zrISExMTUHijNiCarCAIgiCE\nCdFkIxTDMPRqUNWTzcvLqxZN1u60ihUlnG0TLTlbDMPQ6RJjY2MrVBIxNze3TG1DtYed2mJJFBQU\nEBcXp7U5O/Iw261J2k3R8aLmsNjYWDtux1ZEyEYofr9fm/rU/51OZ7WYn2piUWXrpGe3OTyc+P1+\nLfiCCUpVGMLhcDBy5EjArA/65JNPVqhvRWI7GoZBw4YNycvLA+DAgQPFxlK4iZZFWVEMwwjYZqjo\nc6jx5nK58Hg8tGrVCjDfRW1DzMWCIAiCECYkd7GNRLJJqSYX4na5XAFOUuXR3O1ql6poE7fbTcuW\nLQHo378/t912G2A6L3Xt2lVXIirvfdlZhLukdlEau8Iu60w09pf4+HjuvPNOACZNmsSRI0cqdR+G\nYWhnNKfTSU5OToWvF42IubgGoOrSnnLKKWzYsAGgQntxikgW/lVB3bp1GTx4MNdddx0AZ555ps13\nVD0UFBSwc+dOAFatWsUtt9wCmObiFi1a6Hqy5SFSS93ZveURrWPI7XbzzDPPMGTIEADS0tL4+OOP\nAVizZg2ANsOHgirDqN5HtLZLZRAhG+UYhsH7778PwMCBA9myZQsAffr0qXUrxlDp3bs3jz76KE2a\nNAGgbdu2/PnnnzbfVfgxDENPkIZh0KJFC8CcWG+88UbuueceoHxhJ3ZqsZFCsHCVaBUm+/fv1+Fb\nALfffrve4z/hhBMAuP766+nRowcHDx5k1qxZANxxxx1akBbV3q3tUhsNp5G5DBUEQRCEGoBosjZS\nt25dwDTjqf2wUFd6aqXcrFkzBg0aBJgaSefOnQH47rvvOPHEEyu0cqypq02lcZx++uk0a9ZM//3H\nH3/o0ILymMKihUsuuQSARx99VJv8TjnlFK2xOBwO/va3v9G8eXMA0tPTg14nNjaWUaNGsXz5cr0t\nUVNxOBx6HFjHg9rrjY+PZ+3atQDUq1eP66+/nk8//bTY8dHElClTdJ9QWmmbNm3Yt28fAMuXLweg\ne/fuOJ1O0tLSuOmmmwDo3LkzAwYMAEoP66qNlg9xfLIRNbEXFhaWO7RAhV9MnTq1mJMHmAN948aN\nnHTSSfrvUIlUZxbr56rtHnzwQQC+/fZbFixYUOpenDr/tNNOY8mSJdoZA8x9SYC9e/eWem/R5sjS\nr18/XnvtNQDatWunJ8Ds7GzS0tL0tQ8dOsTs2bMBuPnmm/Xv/d///R9PPPEEAMnJyaSnp3PFFVew\nceNGwHRksXP/M1i7hJr5q1OnTgD8/PPPQa+lrqHG5v79+0lNTQXM57YeP3/+fL3Y9fv9egFnV4hT\nefqLGkvHjh0DYNeuXTRt2rTM8+Lj4xk6dChgPvP06dOBsp+5tokcMRcLgiAIQpgQTdZGKqKdqHO2\nbdsGoJ1XiuL3+9m/fz8XXXQRYK5Ot2/fHvB90WtaP7OzW8TExGiNS3knJicna3Nm+/btef755wH0\nZwcPHuTTTz/l9ttvLzPDj8vlYt++fdpcD2YYC1BmuEI0abKGYTB37lx69eoFQEpKitawrJqY1+tl\nx44dpKSkAPD222+TkJCgv5s8eTJganLp6enF2iDSwr1SUlI4fPhwiRp2SXmF/X4/O3bsAMzn3rx5\nM3Bc4122bBmXXnopcLy/gNn36tevH/S3oqG/qHZSmd5CPdcwDN0e06dP54MPPgDgpptuKnUM1jaR\nI3uyUYZKmRdMuO7YsUNPEo888gjdu3fXWX/27dtHTEwMYJpGr7rqKsD0tL322ms5fPhwddx+SAQb\noF6vl969ewPQunXrgP1sMAVAfHw8sbGxZQpZp9MZIGB3795dqVjASKVz584MGjQoYBGl/l00A1b9\n+vV1/zjzzDN59NFHAZg7d27U7KOpZ6pbt26J77Njx47FQpV69OgBUOY+s9PpZPDgwYDZlko4KRNy\ntKBiVwEyMjIC0qiq+SUUnE4n//73vwFzYXzuuefqz4XjiLlYEARBEMKEaLJRhjJdKZPLwoULGThw\nIBBo/nI4HCxdulQf16FDBx544AEAzjnnHBo2bKiPbdKkCf369YtYz1q/38+RI0d466239Gfjx48v\n93XmzZsHwPnnnw+gTXyVSdwRiXTr1g1AOyfdf//9gGkGzsjIAOCuu+5ixIgRgJn9adKkSZx22mkA\nzJo1i08++aS6b7tCqMTz1jKQPp8Pp9OprT0HDx7U2qrykIXQzaL/+te/ALj77rv1GInUJByh4Pf7\ndVt98803XHjhhQAMHz485DJ0MTExbNq0iWbNmunPRo8eDRx3oBJMorenCIIgCEKEI5pslPHuu+8C\n6LCM2267LagjgcfjoWnTpvzjH/8A4M4776RBgwZA8VX4kiVLAtzuozVbTWkcOnSIOnXq6L93795d\n4zRYMN+tysJTUFBAfn4+c+fO1d+rd/v+++/z/fffA9CyZUvcbrcO4Xn99der+a4rTrD99x07dgTE\njOfl5dGzZ0/9fXnL8911112A2XarV6+uxN1GHmPHjuW3334D4KOPPsLlcoUUkuXz+di3bx/t2rXT\nn0WL9aO6ESEbRXg8Hm0ufvnll4HinnrK6SA2NpY2bdpoU6HV0cfv92uz0KZNmxg/fnzAwKpJ3n8q\nFtYqYPPz83VKxVCJloWHz+fj//7v/wDTGzY/P1+bx1u2bMlPP/0EmEkGrrzySgC++uorDh48yNln\nnw3AggUL2L17tw13XzWoOFmVVGPXrl0cOnQIMN/9+vXrQ74OHF+U3n777Xrc1RR+++03xo4dW+7z\nPB4PJ598sv7bWmJRCETMxYIgCIIQJkSTjSJeffVVvapWIThvvvmm1jwTEhI477zzADh69CjnnHMO\ne/bsAUxHBXXu3LlzmTJlCmA6ghQ1uUWL1lYWDocjqDlRharUVJTjk8vlYs+ePdrEac2I1bx5cx0X\ne9lllwXEeU6fPl1ng4pGHA4HhYWF/P7774CZqUpVlXG5XDpuWPWNrKwswNxSUOX/rrvuOh1jrjTf\n8mix0TqGHA5HSCFbzz77bMA4Ksn6FWr2rZqMJKOwkfIEfYMZC1qvXj3guPk3Oztbm4iTkpJ0OrSh\nQ4fStGlTli5dCsDjjz9OZmYmAM8995xOpTZr1ixycnICBoLD4bAtHRxU3QRVUFAQkHKyotd1Op36\nOqF6X1Y1od5727Zt9R5sx44d2b9/P9deey0AW7ZsYdeuXYBpLl68eDFg9qWi1w81OUekpuAsinp/\nF198sTanqwWpWoguXbpUm07btm3LggULAEhMTAQo0W/B7XaXuFC1q23CKeSnTZvGiBEjAnw7MjMz\nOfXUUwEzF3hpz13bRI6YiwVBEAQhTIi5OApQlTFSUlL0KlBpr36/X6/SCwsLdRxkYWEhhw4d0llY\nkpKSWLRoEWAmM1eZXYpqsTVC3+LrAAAgAElEQVSJYIUTKkJiYqLtbRQs9WVJKGuGw+EgJyeHAwcO\nALBz505dY/jHH3+kf//+AFx55ZWMGTMmoL1+/PFHALp27aorRJV2X5GOMpPPmjVLe1+Xxt69e7U5\nVI01qyablJSkU0+eddZZzJ8/X2dNs9MKFE6UFe3qq6/WZmVlHcvKytLfR0uGsOpChGwUcPfdd+t/\nq0lWeUvCcdPxlVdeSZs2bQDTLLZnzx4dtpOQkMB9990HmANC7UPVFh5//PFyn6PMYZdeeqkOb7EL\nNdH7fL5ik5jT6dTe0w888ICe/HNzc7niiit0Dl4lYMHsRyqN4ObNm8nJyeGhhx4CTM9RtV/br18/\n5s+fr89RFPW8rWlYk7Wo8ZWRkaGfOy0tTS9oDxw4gMvl0u1jTVtYk1DJJpxOJz6fj6lTp+p86EOG\nDOGvv/6y8/YiFhGyUUB2djYQOMlZM92oYgFFHXpSUlL05Dx58mT2799fDXdbedQghqrbv3nkkUfK\nfY4KUbjmmmt08nO7KC12MTExkTPPPBMw79UqkHft2lWiJqraNj8/n6eeeoqvvvoKMEN/1DmPPvqo\ndiDaunUrsbGx+Hw+nfmopmptbdq0CfCFUKg22759u372o0ePcvjw4YD6s3ZbPsLBuHHj9L+9Xi+X\nX365XpgdPHhQW0ZUyTvBpGYuQwVBEAQhAhBNNgpQK8NnnnlGJ1dQpr/SKl7Ex8frPZPy5Pq1exVe\nWe/mqjJhKpPpvffeW2ZlHzvJy8vTHrJ79+6lcePG+rtu3brpPlBSTlmleSmN1el06iL2jRo10iFA\nZ599Nrt27WL//v2295FwoTyJVZ5rCL7HmJ+frzXdxo0bU7duXW0+rmlY8z0rfD4fO3bsYMWKFYC5\nVaX8Pz777LNatx1VGiJkowA1eHft2kXLli2B0MpJHTp0iFNOOQWgXKXs7J5AK5s5pqpiPNVeZp06\ndSK6fFdeXp52joPjiwyfz8eQIUO0uXPdunWlXkcle7c+q9frpWvXroAZ6mJ33wg3KtWmWsyWhmqL\njz76CIfDETVtU57Y1bi4OPr27Rvwmd/v57XXXiMvL48uXboA5lZC69atAUp1lKuNiLlYEARBEMKE\naLJRgDJX9e/fnz/++AMoOXRCHbtz504yMjLYuXNn9dxkBKGyWUHlwgnUucqxLFLx+/06POvrr79m\n2LBh+vN+/fpx0kknAWaCkh07dpR4HZWkwefzaXNf//79S81jXJO8aDds2BCgwfbr1y+k8/x+P263\nW1sQIiWHr3o3TqczQHutW7dumU6QKkNWMK/6devW8cEHH3D48GG9TTFz5kz9vYTwBCIZn2ykIhOU\nqnqhOrfD4dB7bVavRr/fT4cOHfjll18qdG92dovKTtxqknO5XFx99dVAxTweVdyo1+vV92SXN22o\nbVK3bl2WLVsGmBmflBd6UXw+n95nnj9/PqeeeqrOZlVQUMCdd96pvyurL0RzX4Hj5nGrB/eECRMY\nM2ZMue4jWIiTXQLH6pegimEoP46uXbuyatUqwOzjqoDExRdfzOWXX65j6Iuini8mJqZSC4naJnLE\nXCwIgiAIYUI0WRuJZFNbtGonkyZN4p///CdglvHr0aNHpe/D2hZ2tUt58lyrxBv33XcfLperzHN/\n+eUXnn76aWbMmAEEmjtDed5o7SuKFi1aAJCenq7NnldccUWlrwuR0V8aNmyI3+/XWwBut1tbv5KT\nk/nhhx8A0/EtmIOf6g8qsY01EU5FqG0iR4SsjYiQDU5F2+Wss85i4cKFOlFCu3bt9H5iVT1PJEya\noeJ2u2nZsqUOs0hLS2Pv3r0AzJgxgwceeAAoObQnVKKxr1hR4W3jxo3jxBNPBKjwNkvR+4qUAgHW\nLFTWwuyGYTBo0CAALrzwQlJTU+nTp4/OIPb+++/rRWtVvGe7i4/YgQhZGxEhG5yKtEtcXBwHDx6k\nsLBQh6KUFLZUkWdTe1yRvidb3URLFZ7SUNaOMWPGMGLEiEpfz0okLsqszl1erzcg97nf7w/rO7W7\nv9iB7MkKgiAIQpgQTdZGIlU7gcjXZNUxKl/z8OHDGTVqFA899BALFy7Ux6hk+Q0aNNAZnNq3b8/C\nhQv1/pOqUgPmyl49u1pxW01tdmmySpOOtOFqt/mvMmPIaj4F891WtZYViZqs1fPYjjzLkdaHw40I\nWRtp3rw5YKbvU9mFqiPGrqSML2pgGoYR8ROn9V4hMMuRVTgq05g1jjE/P7+YM5MSuAUFBcWKExiG\nEfC9HVgFgd04HI4AAWVXIXsI3lesBSbAnknd7j3ZYBmorGMm2P1VRzuJuVgQBEEQhCpDNFlBEARB\nCBOiyQqCIAhCmBAhKwiCIAhhQoSsIAiCIIQJEbKCIAiCECZEyAqCIAhCmBAhKwiCIAhhQoq220io\n2WqsiRaqGpVYwJrkIJqz+IQLyV18HGsyE5fLZWuR8khqFysqE5ldiToitV3snlvsQDRZQRAEQQgT\noslGAZXRYK05foOtqoNdu7alPQsFydkSHLu1Emu6SWtOXrvvKz8/39bfr0pUu6rqVjt37qxw+9bG\nuUWEbJShcuiGknM0Pj6en376CYCPPvqIe++9N/w3GEG4XC6du7hBgwbs3LkTMNvQWgigcePG7Nmz\nB0DX2SxKbRKyJeW2Dobd7fLMM88A8Morr5Ceng7Yf0/RTkJCgh4HPXv2pGXLlgDccsstALz00kus\nWbOGv/76q1YKzfIi5mJBEARBCBOiyUYRCQkJLF68GICJEycC8MEHHwQ9NikpKaBouZ2VUsKJMocH\n017q1KlD3759AbMg96mnngqYpsTt27fz5ZdfAnDGGWfQq1evarrj6sUwDJKSkgDIzs4OaCdlBnQ6\nnfj9fq2VpKSkaItJRkZGsbaNJE3x6aefBiAzM7Pc56q+07NnTwB++uknjh07ViX3ZTVdRzrWMWQY\nBrm5ufqz1atX8+OPPwJwzjnnADB16lQOHTrEqaeeSkZGBlA7zcChEj09QRAEQRCiDKnCYyPldbO/\n+eabGT9+PABDhgwBzJWmFbWCvuGGG5g8ebL+PDs7m+Tk5JB/K9KLtiuChTepfdjly5fTvXt3ADwe\nT4B2UVBQQHZ2tj6+a9euAGzfvr3U34vEItylnWMYBldccQVgavMzZswA4IILLmDlypWA+Uy7du3i\nlFNOAWDYsGH6984//3wWLFgAlKyt2NlXKlPMXvkoPPjggwAcPnyY0047DYA9e/YUe964uDgmTJgA\nmP3nrLPOAuCdd97hvffeY//+/YC5r293Pdny9BdltfB6vXg8Hn755RdatWpV5nU+++wzrr/+egCt\n0YZCbRM5ImRtJNSB4PF4AFMAqEGrPP2KevmpAfPMM88Uc3Tq3LkzAL/88kuZvxktQlY9r8vlorCw\nEL/fz1NPPQXA2LFjdXvl5uaSkJAAmM+WmZmpFx1+v5/nn38egPHjx5f67NEkZB0OB1dddRVvv/02\ncLytiqKeSf3fuhhZtWqVFjzBnt3uItyVEbILFy4EoF69eoA5vq6++mrA7C9qbDVs2BCAmTNn6u0H\nK0ePHqVnz578/vvvQOCYjIb+cvLJJwPQrVs3Jk+eTGxsbEjnWZ0HY2JiQu4HtU3kiLlYEARBEMKE\nOD5FMGo1qsIU6tWrx6ZNm4CSzVBqFf3oo4/SunVrLrvsMn2tH374ATDNXtFOgwYNABg8eDAAffr0\n4ejRo6xYsUJr8A6HQzvETJs2TWs97du358MPP+TRRx8FoEWLFtx///0AbNmyhbfeeqs6HyVsOBwO\nxo0bF1SDzcvL0+2xceNGPvzwQ91Wr732mv5u/fr1Ea15VObeevfuHfB3+/btadKkCQBbt24lJSUF\nOJ69yaqher1e3n//fQBGjx6tTcWKSM24pFDxxQkJCdx9992AOYY8Hg+FhYVce+21gKnFHzx4EIB/\n/vOfgDkPuVwubVIGeP/997nyyiur8QmiBxGyEUxiYiJgDmLFn3/+CZgDAmDz5s16b9E64Rw7dowr\nrriCefPmATBw4EA9WUQ6brc74FmKxq663W5twlT7jS1btmTdunXExcXp451OJ7feeisAs2fP1s9/\nwgknkJOTw/r16/W5Sqgo7+1IozQv6qKoCfTZZ5+lY8eOAd+pJAlPPfWU3l88evQofr9ft8EjjzxC\n48aNAejfv7+eTLdt21bib0USZcX5qm0D5XWthOe2bds4evQoYG7RtGvXDjjeZt26ddOCdeTIkaWm\nk4zkhYmVli1bMnToUOD4YuKqq65i5syZxY4tuvjcu3evXuwOHTqUm2++GYBDhw4FHGd9H5G++AgH\nYi4WBEEQhDARecvQWkTR+DT1bzC1MJWhyIqKfX3iiScAGDFihF5pW2Nh/X4/fr9fO70MHDhQ/4bb\n7bY1qXtZGIahtdFgK9+CggJycnKA42Zjv99Pz549A7xCc3Nz+fTTTwFTG1HttHr1auLj43UbWJOW\n//XXX2F8sopTHs1IeaDfddddAe136NAhramlpaUFOA1ZtY133nlHm8+TkpK0B+kjjzxSbJvC7vSF\nwSitrQzD0JYhddzPP/8MwD/+8Q927dqlj1u3bh2AdpDr16+fzqAW7aj3dvXVVwdYuNLT0/noo49C\nusaSJUu4/PLLAbO9rHH5VqzvI1o0/KpEhGyEcvnll2tzluK7777j9ttvB8wQDIB9+/aV6tU3ZcqU\nYp9FusnGmve1pEH5zTffAGhv0O3bt5Oamso999yjn8/lcmmT38aNG/W5hYWFZGdn06hRI/2ZMp1G\ne1D9iSeeGLAnffjwYV577TUAHn744RIXV36/X4c+DRo0SH/eoEEDrrrqKgD+85//cODAAX08RF97\nGYahPfPVQu3zzz8HzGdRfadevXraq/jbb78F0CE7NQEVsnbPPffoz/Lz8+nYsWPI71QJWDAXvtHW\nF6oLCeGxEauwUxOcclI5evSo/v7IkSMAxYRuKNcP1vFjY2PJy8sr9dxoCeEpyhlnnMHSpUsBsy33\n7t0LmNlqtm7dqq//7rvvcskllwDms6q9xbImikgPyTj//POZM2cOYC4mZs+ezXXXXQdQ5jtXTj/b\ntm3T/dHKvn376NChA1lZWRGjnZS3rxiGwffffw+gHZuUw86uXbt09qezzjpL71WmpaUB5l5ueRP/\nR2p/UfvsVs38oYceYtKkSfh8vjLve/PmzXTq1En/7Xa7S8z7XZTaJnJkT1YQBEEQwoSYiyMEZcZT\n2qp1JVpeDVaxYsWKgL9VXtayNJpo5rffftPaqNPpDDD5bdiwAYDTTjstwCtWrd4jmWCZrYKxdu1a\n1q5dC5ihWtnZ2SFpX02bNtUaXjAtFmDGjBnk5OREtbeo1Wpxww03AOgkEmlpadStWxeAefPmaS3v\nkUceAWpW+TrlKZ6QkBCwvdK4cWNiY2N15rNgZmCn0xmgxULJ1asEMRfbinWCUpOo2v/p06cPAwYM\nAI5npikvRV9teSbESDMBlqf8mtormjZtWkBMsFUweL1e7eBRnvg+u9pFmTazsrLKFLRKULRt25bm\nzZtr56+SnJRSUlLIyMgIGk/r9XpZvnw5YJrci/623RmfyivkExMTdRjctGnTAHSqyebNm+uMTnPm\nzNEL36ysLCC0TGlFiVRzcbBjTz75ZB5//HGSk5MZPnw4YKaXVIuLfv36AabDU0V+R1HbRI6YiwVB\nEAQhTIi5OEJo3rw5cDyEZMGCBRXWYAEWLVoU8He0mfWslPfeZ8+eDaATByiUA1mdOnWibjWttNBQ\n7luF6ezdu5d69erpMKfdu3frYwzDoEuXLoCpoS5fvlw7PrVt21Yf16VLF21OjbY2C8arr76qHZnG\njh0LHE/2kp+frxMp9OvXjxtvvBEoX87vaMSauzorK4uEhASd5alOnToMHDgQgHfffTfgvGieU6oT\nEbIRQnp6OhDoFl8Zzj77bKBm1JEt7+Rev359wNwnsppAy1OFKNIoKQaxKIZhEB8fD5ghKr///rsW\nFpMmTdICZsGCBTqr04EDB/jzzz91X/nggw/o0aMHYHoUl0a0Cd4vvvhChySpvVmrZ7kSJHv37tXx\noxdddBFwfPFWU/n111+ZOnUql156qX72mTNnBg1dEgEbOmIuFgRBEIQwIY5PNhKO1aBKFqAcZSr6\nG5Hm+FSec1Vd1G+//TbgWlXR3tHgyKJiIO+9917i4+O15mYYhnawc7lc+ln2799PTExMQClAlaih\nfv36ZWZ1ita+ojxkVZan1q1b62dt3rw5t9xyC2AWDgDTbBzqs6p2tisjVkXaxel0kpiYSHZ2tnZm\nW716dbFCCjfeeCNTp06t8L3VNpEjQtZGwiFk1aBWg7y2Cdnk5GT+85//ADB8+HAdjmJNgF8ZIl3I\nxsTEMHfuXMDMWtS1a1dtPi6K6is//vgjXq+Xk046Sf/WsmXLAHO/tqYK2aI4nU69OM3MzNSVaFTG\nrNjY2JBDVdQ2hV2hLVXVLl6vt9i4qey1a5vIkT3ZGoRhGAEDQO3z1hYMw2Dt2rW0bt0aCIwpVZmf\najq33367Dv0qDZ/PpyfPbt26FVuEPPvss/q40qiKhUuk4PV6tSXI5/Pp8aPGVKtWrfjjjz9CvlZN\nwPp+lWZfGWrjXm7NGSGCIAiCEGGIJluDOO+88wJWirNmzbLxbqoXlfi9efPmAaY6pYmpJPBFz4Ga\nYb5SzzJ8+PBi2oJqA6/Xq9vGWvnJ5XIVy2Z0wgknAGYilGBFBVSSj2ipURwqVs190qRJQPEtmNrK\nO++8Y/ctRCUiZGsATZs2BeDxxx8P+Hz//v123E7YsU52ag+tXbt2PPTQQ3g8ngDhoQToiSeeWEyo\n1gThqrA6MRX9/LfffgPMbEcqhMfpdAYI4x9++IEtW7YAZoyoSsmZkpKiTe0q65bD4aB79+4AxZxi\nagqGYWiHJ1X5SrVjTccwDF2lx8p5550HVG7xXpPGXKjU7qWZIAiCIIQR0WSjHI/Ho017RZN2q+w1\nNQ2fz6cTCLRo0QIwa5327t07QDtzuVw6mcKoUaNqxSpaaaoKwzB0SM+xY8e06dfj8ehj8vLyePHF\nF/nss88AM5GJKqZQNJmJ0+kkNTWViy++GDieXaqmUbduXd2X/vvf/5br3PLk2Y5E/H6/3j44cuQI\niYmJAEyfPh0w8zpX1Gu6NprcRchGKWoCaNCggS44ruIcFSpVY3mJhoGgBrkycVoFrNpX8/v9fPzx\nx4CZrlLtR9YUz89gLFq0SJtyFUqgFvUUVgLyueeeY/bs2bpd5s+fH1RI+P1+vF4vBw8eZPXq1cDx\nYgQ1BbV4U6kkofz9JZoFrEKZxpOTk/XCTO2/79+/nwYNGoRclci66KgJbVNeIn82FQRBEIQoRTTZ\nKMfqGWqNfTQMo1bUeLz77ruB45r9/fffz3PPPac/q20r59GjR+uE9n379uXnn39m3LhxAGzYsEEX\nC7jlllt0O+3cuTNAWyurzQoKCpgzZw5w3OmupvD1118DZg3nmlQ/tqL4/X7tXDh//nzALP0nbRM6\nkvHJRqoiMDsxMZGlS5cCpgetNSH+zJkzy1UrVeFyuYKGbVQX5WkXlfT/vPPO48MPPwybUC1qiq5u\nKtJXXC4XPp8vrKY6p9Np62IuWNrMita4TUxMDKg4tHbtWuB4gYCKEOkZwip67co8V20TOSJkbaQq\nBkJcXJwWpCNHjmTnzp2AqZ0899xzFQrjibZC3NVJTZw0K0ukpFVUDjqtWrViz549ulxbqH3Z7XbT\nsWNHwHT4ysvLA2D79u0Vvr9I6S9ut1svhqriniojaGubyBFzsY1UhSOOz+cjIyMDMJ17VMD49u3b\nK+xdXNsGQU2jJiXZKAurk54SItu2bSM/P7/cC8XCwkI2bdoU9NrRTlX2BRV/XpMdCKuSmtOLBEEQ\nBCHCEHOxjViLRVf0NRiGoUM0YmNjtYnL6/VSWFhYoevabS52OBwRp4VFcuky63cOh0P/XR17pXb3\nFWUNst5DJGjydu/hu1yugL4aqnnXuq+t/q8qWaWmpgKm49ORI0eicm6xAxGygiAIghAmxFwsCIIg\nCGFChKwgCIIghAkRsoIgCIIQJkTICoIgCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJiStoo3Y\nlY+2rMB0uwPGJU9vcSK1TRISEjhy5Ihtv18d7VKR9KcqgYNd1Woitb/YXVDCDiQZhY0UzdQD9mWI\nKUokZMux4na7iYuLAyA7Ozvk+1MTpN/vr5K2jXQhW93l/eyeNKtamATLFqXGZkpKiu5D2dnZWugm\nJCSQk5Ojj7O2R6T3l1DOL+kZDMOoUGar2iZyxFwsCIIgCGFCNFkbqV+/PgB33nkn33zzDQALFiwI\n++/GxsbqvMk5OTnFVpaRYC5W5jaPx0NaWhrNmjXjuuuuA+DFF1/UZcz27t0LBF8dG4bBNddcA8DK\nlSvZsmULULGcvnbnog1VM3E6nVp7j42NJS4uTrdVRXNZB0NpbQ6HI2pqD5eE6msul4shQ4YA0LNn\nT2bOnAnA6tWri/2W3+8vtX5ttPSX6sbhcNS66j2yJ2sjaWlpgDkgKlOzMlTUwCsoKNAdXZkXPR6P\nbftHwVATd0FBATk5OWzbto1du3YBZq1cVQihLFq0aAHAqlWrKnU/0bQWVYuI7OzsgEWU2+2usnes\nhEc0tUtJqGeJj4/XSfBvuukmbrrpJgDatWsHmInxc3Nz9Xnq2WtCG1QXkbIdVp2IJmsjagVtGEaV\naQNq37KwsLDYNa2rW/Vvp9OJ1+ulTp06ZGZm6u8jbU+2otcZNWoUYK6gJ0+eDFTs2SK5Co+VRo0a\n6frCAM2bN9eCo3Hjxnz++edA+dqgtKo2kWD1qOg548ePB9Daa5cuXfSzxMbG6uPVOJo3bx7Dhg0D\n4OjRoyH9VrTtySYmJuLz+fRCLRwL79omcmRPVhAEQRDChJiLbaSqvTLvv/9+zjzzTADmzp3LihUr\nOHbsGADp6ekkJycD5upU7du53W48Hg+HDh2q0nuxE6V1nnPOObz88suAqY387W9/A+Dqq68OSftK\nSkoCzH3rSN3jKkqnTp20ltW+fXtWrlypvyssLNR9oKy+p563e/fu+np//vlnwDYDHK+JHC04nU5y\ncnIAiImJKfZ9MEuFesZTTjmFCy64AICPP/64xuwt1qtXD4AVK1aQk5PDBRdcEGANqSwV9UKuKYi5\n2EaqYuKuV6+edmwJhpoIDh06RJ06dfRnygkoKyuLjz/+mIkTJ0ZE6AFUrl0cDoc2camFRDByc3Np\n2bIlAJmZmdokaBiGNq926dIFgOXLl+v2iFRHFvV9WlqaFiI7d+7U7xzMhYbaTihLQJx11lkAXHbZ\nZTzzzDMA7N69W7eD+r34+PioipM966yz+PLLLwPOVQvR+Ph4tm7dCsAXX3xB165dAejRo4c+Xzna\n9e3bN2B7pSQi3VzsdDr54YcfAHNRtmbNGoYNG8aOHTuq5D7cbneAs5PX6611glbMxYIgCIIQJqLL\n1iNotm3bBqC1saIo71v1/5iYGG32crlcdO7cGTBX8Z988klEmb5iYmJC9h4uysMPP1yqBqs4cuSI\n9h798ccfWbt2LWBqrzt37gRg/fr1gKm9Rrq5WGlMGRkZNGnSBECbhhVxcXEhv2fljb106dKg2pj6\nLFQHoEhhyZIl2kys2kdp+1OmTGH37t0APPDAA7qtJk6cCJhj6bfffgMISYuNBrxeL7fddhsAgwcP\nZvv27boNSsPhcARsFbRq1UpHSOTm5pKSkgKYbevxeGjcuDFQeS//aESEbJTSrFmzYp9lZ2fTunVr\nAPr378/YsWP5+uuvAdNsM3r0aMA0JanJcdasWcyYMaOa7jo0xo8fz6RJkwDYs2dPyOd5PB4effTR\ngM+UMMjKytIDvG7duhw4cEAfu2fPHr744gvAnFB//fVXIDIy91QEFWttGAZer7dC+6ZqkVPWc0dT\nuxTl8OHDwHGP/HPPPVcvppYtW6Y9sT/99FMABg0aFOB1XBMwDINzzjkHgCuvvBKPx8Pbb79NdnY2\nEHx7pE6dOnz33XfExcXp8dmqVSuWLVsGmKZ2FZ546qmnsnjxYtasWQME3wev6Yi5WBAEQRDChGiy\nNlJa/GFJKFOoNdex+veaNWt0hqP+/fvzwAMP6FVq165dtQex2+3WGm9mZiYulyuiMrE89NBDFTpP\nmbnUc/7++++cdtppgNnGqp2dTicffvihbjePx6M1559//jmqtbPLLrtMO2xBxbx/ExMTGTp0KADT\npk2L6vYIBRVL7XQ69ZjcsWOHdohSbdiqVSvtcV6ZLY1IwuFwsHnzZgBuv/123nnnHerVq6c1WWsu\nbNUOV1xxBR06dCArK0tb1JxOp443btmypdZqZ8yYwc6dO/XWSyQlvKkuxLvYRsq7z5eQkMCmTZsA\nc8CDKWSVSWf37t3anJWYmEhGRobeO2rVqhV//PEHYE4qqtOXRLR5F7/yyivccsstAHqAn3/++XpQ\nW02/hmFw+umn61SW27Zto02bNkDkmkfLahM1AVoTkCQmJmpP41Do2LEjYO5NqixHzZo1i9g2garx\n0FeLs0aNGumFZps2bXQYy5w5cwDTDP/BBx8A6H37sojU/qJISUmhQYMGgJnmde7cuUyYMEGPl9Gj\nR+vFhppbYmJiiI+PJz8/n4SEhGK/d+TIEc4991wAvv/++4A5CqJ7i6EiiCYbRbjdbj2JqpXmTz/9\nRPv27QFzH011eo/HQ0pKCn/99Zc+XjkmbNy4sbpvPWyowa0ELBwPPylpMI8cOZI333xT//3ee+9F\n9cB3Op18/PHH+m+lYanJMVSUVvL9998zZswYIPInxIpYgxRqrCghA8ctRe+99x6JiYnAcQe4lStX\n6tSeNYUmTZpoy8eIESNITk7mscce01Yeqyb7008/AXDgwAG6devGnDlzuPbaa/W1lCDdtGmT3oOt\nbeE6wZA9WUEQBEEIE7gmPFQAAB8XSURBVKLJRhFNmjTRZk3FSSedpM1dO3fu1K7zYO5/dOrUCTC1\nm6eeegqoWatLpW14vV6cTid5eXm0bdtWf6dMfvv27dOmQKsWC/D0009X4x1XDmv2HGW+a9y4sdbK\n8vLytCkzLi6OnJyckIqO//3vf9dm0SlTpvDjjz+G7RmqEuWtak3cHyrKqziYFty3b1+9V3nzzTcD\nNXM/8ddff9Va66+//sq5557Lrl27+OWXXwA488wzdTKSCRMmAGY7JCcnk5eXx8iRIwGzX6rr9OjR\ngz59+gBmyE6kW0PCjQjZKEBNkueee26AGUd9p/ZnU1NT9UTg8/lISkrSe7KffPJJmfuwikiPCbWi\nzOZDhw5l1qxZOBwO3nnnHQCuuuoq4uPjARg7diwPP/xwwLlK6FRkgrYLwzD0QuuMM84AYMOGDVx5\n5ZX6GGX+bNWqFX6/n3379gGmk5sSLMeOHaNRo0YAvPTSSwwZMkS/965du0aNQOnQoQNQsS2QhQsX\nAsEzg/l8Prp16wZEvsk8GA6HI6TFdEFBgQ7nO/nkk1m1ahUPPfSQXmRlZ2cHff6cnBy8Xq8Og6pb\nt67+LjY2lk8++QRA97HajJiLBUEQBCFMiCYbBahV4YUXXljqccp0Cubqe/fu3drss379+ogJ0QkH\nK1eu5NixY8TGxtK9e3fANHv+/PPPAPTq1SsgnCUrK0trfKFoKpGi3fv9fu3YpJIlHD58OMB7Wpn3\nLrnkkoDMPD6fT2c5shYdL8qzzz4btvuvairjxKeyNynHH5fLpR0L+/TpUykN1u7CCYZhhLRNAGhn\nrqFDh9KqVSu2bNmiNdSSUP1NZY7bvHlzgDb7559/VvjeaxoSwmMjoU7cymy1ZMkS6tevX+qxykSk\n/q/MqVu2bNH7JGWZkaKxRqjL5SInJ0fX6AUzlMC676YmPsMwSExM1GYyq8BxuVz62a2Tk5qwqrpy\nUqiUp03UM7dt25Y33nhDv/eyUGbz119/XYd7vfrqq2XWOo62EB7VR1R4k3q3fr+fBx98EDC98598\n8skK3VNiYqLeL1em+urG5XLh8XiA8nuZl/d3wPQ4tqbxHDRoEIDOpKawe26xAzEXC4IgCEKYEHNx\nFKBMnsOGDeMf//gHYNaOBdPTT2kgysMWzJjHzZs3678XLlwYssYRjcaNwsJCnn/+eUaOHKmTkVvN\n5yqPL8DMmTMDEtsbhqHPWbRoEc2bNwdMj1uA1atXB2jIkY7SXDZt2sT48eN17l23263f7Z49e5g2\nbRpg5q/u1auXNv1ddtlluj3atGnDnXfeWc1PED7q1q3L/v37geIOT36/nyeeeEJ/p0r4/fvf/w7p\n2sqCkJiYqB0U7cLv92uNsTKxxGWhnll5tyuKarCKUIp31DREyEYBarAsWLBAZzMqySNWmTPT09PZ\nvXu3TgPXpk0bnfZM7fFCdArUknjrrbdo1qwZLVq0AODEE0/Ug3/fvn1MmTIFMCdN63MbhkFWVhZg\n7l03bdoUgOuvvx4wvXejdT/7yy+/1GZDj8cT1BTucrkCasUOHDhQT8wffvhhNd9xeFDjYP/+/UGF\nK5jmYxUSZBiGXpyEQmpqKpdeeikAs2fP1oLcLnw+n967t4Z9AVWSQlVdr2HDhkDowrO2mYpBhGxU\n4ff7Qw6taNiwIc2aNdODoWnTpnoSKCws1OW9PvvsM53rN1KwZpkpD+np6Tz88MO62lCbNm107uKt\nW7eWql2o/blhw4bpifbUU08FzAVNpDg+VYaS+k6DBg3o3Lkzl19+OQCtW7fWk+GKFSuq7f7CxahR\no5g8eTIQfA9XfebxeLRgSkhIYMCAAUDZTjwNGjTg9ttv1ykXDxw4EHHCxOp3YE23WTTlodvtJjY2\nVn/2xhtv0KtXLwCd79wqpIMJ1xEjRpR4H6LJChFPqMInJSUFr9erzZyJiYk6lrJ379488sgj5bpe\ndRIXF6cnO5/PF/I9Ko9K5S1ZWFjIZZddBpjau1p1ezyeAPO59fo+n0+bW7du3QqYCQ9qgpAtCYfD\nweTJk/UkCmZqRYjM/hEqyqLx6quvBrw/a41lOK5dbdmyRZvJDcPQZmJl5Zg1a1bAQkWdP3fuXGJi\nYnj55ZcDrhcpFO3nR44c0WNh4sSJ2tkrMTGRNm3a8Pe//11bcUraJilNWAaLx1ftr+LWaxPi+CQI\ngiAIYUI02SjDWq4NisfAKTf6RYsWBRSY7ty5sz72q6++Yvbs2UBkporLz8/X2oAyc5WlUSnHpkOH\nDumQE6/Xy9ixYwG49dZb9T5ZWlqa1nJKY9u2bYCp6UWLmcswDB1Wcccdd/D555/rcJyi4UdKq5g3\nbx6tW7fW2kZ6errOJhXNqMLhVodAwzD0HrVCPXdqaqrW3Px+vx4vH330EUCxUCalEV9xxRXs2rWr\nzFCnSOLFF18E4NJLL6Vnz56AGbo1f/58Tj311ApZMObOnQuY6RlVG6v5RV3P6nBYW5A4WRupjAly\n3rx5AJx++um6AyclJZUoDDIyMmjSpAlQdnA62GsmdDgcAfVyGzZsSGxsLHv37gXMPdKS7s/pdHL2\n2WcDZqk7JUyPHj2q40VbtWpFcnJyuSZF9a7sMgWG2lfcbje9e/cGzBJtTqeT1atXAzB58mRtBk5J\nSeG///0vYKbTc7vdekJs166drtgUCpEeJ/vCCy/offpg+/KqH+zYsUPvQTds2JDHH38cQJdErAiR\nWOrO4/HoBWRCQoLen42Pj6dfv34sWbJEt4laqJTFW2+9xQ033AAUHyPWFI9Op9O2WHO7EHOxIAiC\nIIQJ0WRtpDKarKq0orTTktixYwcAnTp1KlcBbzu7hcfjoV69eoCZserLL78kMzOTq6++GjDjQEtb\nDSuTX5s2bXQc3913363rpbZs2ZIuXbqUy1QeLZosmFVQwIwHbtWqlW6P3NxcXRO0RYsW2qLh9/tZ\nunSpbt/yhndEuiYL6Lqn48aN09YNZfVR4XCHDx/mvPPOA0yTp+oflQl3iRRN1rrl4nA4dHhSx44d\nSU1NBaBOnTocPXqU+Ph43c+zsrK0I6CKO+/atSsHDhyge/fuOjVlfn6+Np+XZiGqjRmfRMjaSFV4\nrJ599tl6zygpKUlPCIZhMHv2bP3drFmzynVdO7tFTEwMJ510EmBWjtm0aRP79u3ToQQZGRkhT3zW\nyjLKZBgfH8/w4cPLfQ2IDiFrPWfw4MG6APuHH36o0/z9+OOPeq+2oKCgUu87GoRsMLp27Qocj6Ft\n1aoVH3/8MVD5NlHY1TZut1svRB0OB3369NE+CZmZmTqn+eHDh/WCtk6dOjoiQSXi2LNnj96zVSla\nK/NMtVHIirlYEARBEMKEaLI2UtWxl1aTUGpqKh6PR8c+rly5slwrSLsdn1S8YpMmTfjoo4/o168f\n6enpFb5mYmIi48ePB2DGjBnaAais+wAzI5LSeqOhQEBRlLdxuO49WjXZcGJ3QYmhQ4eyadMmwHz/\nL730EgsWLABg8eLF3HvvvQC8/PLL2iHw888/Z/PmzWHNblYbHZ8khKcGYZ3sDh48SKdOnbSpZ/36\n9eXak7UTv9+vTVt79uyhXbt2lR6YPXr00FmuNm/eHDA5lyQk1F5mQUFB1ITwBKO2TWp2Ec4cweVl\n0aJFek+5U6dOOJ1OnQN9/fr1DB06FDC3PxYvXlxt9xWt6Ukrg2iyNhLOVXhMTAwDBgzgrrvuAsy4\nuPnz54d8vt2abNEJq6L3Y00lpwb4sWPHcDqdpKSkAKbji3LWKCk8KJocn6ob0WRLxq62sZZsVLHT\n6m+7F121TeTInqwgCIIghAnRZG0knKtwwzCIjY3VWX38fr8uBFCWNlYVVToqg8fjCdBknU4nBQUF\nFbon676qtaC9tQyew+HQmWi8Xm9Atqmi17GrXSJVY7PbWzSS2wXss3y43e6AEK3SUH1b/T85OVmP\nD7/fr72KlflZacIVyRZnd3+xAxGygiAIghAmxFwsCIIgCGFChKwgCIIghAkRsoIgCIIQJkTICoIg\nCEKYECErCIIgCGFChKwgCIIghAkRsoIgCIIQJiR3sY2UFUhvVy5Uu5N4OxyOiEu9ZnfC98okXUhO\nTgYC6/AGSzhSv359wKwhWp7nlLSKxVFFGUqrrRpO7GiX0uYr9V1MTIyuT1tbECEbwdg1edkt4Oz+\n/WBEc2JzVRjC+gzBnueaa64BYPfu3fzvf/8r9r21ypNCZQkSAonm/lIRDMPA4/EAZr1m1edUXV7V\nbyJ1URROZIQIgiAIQpgQTVaIOJKTk3V+05ycnLBptmpVbRiGNgfbZd6rStRzOZ1O/H5/SLlinU4n\n/fv3B+DQoUPMmTMHMDVVZfoE05RspbbloQ2VSLTGhAPVNwzD0GPn0KFDJT5/bTMVgwjZGkdV7OPa\nPUFkZ2dX6z20aNGChg0bAnDgwAH+/PNPwP52qCzKTBfKc9SpU4cTTzwRgKSkJJ588kn97w8++ACA\nTZs2BTUZ20kk1XCtbTidTr04Pfvss1m0aBFQscIBNRkpEGAjkbo/YXelDJfLVS17Wu3atQPg008/\n1bV2n3rqKV0w3ordVVVC7Ssul4uYmBgAjh49WqrwUdds06YNS5YsoWnTpsV+y+v18sUXXwBw8cUX\nB31+O6cQtQ8YaRaIaOkvFSU1NZXU1NQAgbply5Yyz7N7brED2ZMVBEEQhDAhmqyNhLuebFJSEkeO\nHAHKt6K2e7VZHRr+KaecwowZMwDTvHX66acDprm4tBCESNVM3G43AP/617/0furChQtLPUeZ+rZt\n20azZs1KPK5r164A/Pzzz0G/t3MKOeGEEwDTI1rtF3u93pDuKZym5kjvL5WhR48ezJ07lw0bNvDV\nV18BsHz5ctavX1/muXbXqrYD2ZONUlq0aAGYE6S1GLnVEcGK3+9n9erVAFxwwQUcOHCgGu+2fDgc\njrBMTq1atQJg69atAZ8vWLCAgwcPAiVPuJG+FlX398gjj+iFFZj9QAlgq2nP6XTy4YcfApQqYH0+\nHwMHDgRKFrJ2UrduXQB++eWXkIuUK+bNmwfAsmXLAHjzzTfZu3dvyL+txlhaWhpZWVnk5eWFfG40\nouaWH374ATBDvlTb1TYTcHkQc7EgCIIghAnRZKOU9PR0/W+1op44cSIbN24E4PXXXyc2NlYf4/f7\nSUlJAWDx4sX07t0bQK++rdqj3QkGqnJVrJyYVDajYDRo0CDqV+IqQ1NmZmax76xOQerdrl69mpNO\nOkl/7vV69TWcTqfuU0ePHtUOYpHId999B1QsE5caD61btwZMTU21j9/v1/9OTEwEzPCTOnXqAASM\nrbi4OLKysgLMz8p6EO3ExcVx9OjRoN99+umnDBgwAID169eTm5tbnbcWNYiQjVKU6e+6665j+vTp\nxb5///33cTqdjBo1CoA+ffowbtw4AHJzc4uZtqxCJpr3TJxOp342tedYFrNnz9aTYqR5qVYGZd6z\nvk+1zdCjRw/9zkeNGsW7775LkyZNAFOAnHHGGQCMHz8+QKBEGpXpqz179gSgQ4cOAMyaNYvmzZsD\ncNJJJ7F9+3YApk6dCv/f3v2HVlWGcQD/nnvv7u6uMbblRBm5aukk7YewilVmsRaVVETokijXLDe1\nRYSIFRWSUkRJiMWKRMIkieiPyiDciJG2QN2MNdaYkV2dlmYL827N7o/+ODzPzt12tzvnu3uP+37+\n0m3317nvOc95n/d53xfAF198oefa999/P2pgd3s7knMnWYCNx+Po7+/Xqvy7774bBw8eTPp8mTqT\nYjIwXUxERGQIq4vT6ELu7uQxH330EQCgrq4u6d0mYKdCAbs4o7OzE0Bq6dhMX/TduVoTYFfXtra2\n4tprr036GOldeL1e/Pnnn+jp6QEA9PT0YMuWLQDs4qivvvoKAHD69Olhz5Gu43IhbUU2WnA+Vlbc\n8fv9uuDEyy+/POy15DHTp09HRUUFAOiiFENleltJRs6D9vZ2AMCBAwd0QY6cnBzk5OQAAEpKSvTv\npbefaoGUm9qLGJoils+wYMECAEBnZye8Xi8KCwu153/q1CkdwnJ+5qHnqcfjcX0vf7yYLnYRy7J0\nfOiNN94AYJ8Qkh4duoSe3+/H2bNnAZhdnnCySAorFovpzcOjjz4KAFi/fr1eAEVNTQ127Ngx7Hlk\n0YUXX3wRALBs2TI0NjYCsC8CMrYkx9qt6XNpC3KBW7dunS7ecPDgwWHB1UnaSm9vLw4dOmT4nU4u\nSWvKcZHgsG3bNtxyyy0AgNLSUixduhTA4LFoaGgYV/WxWzkD7G+//aZV+U5FRUXYuXOnnpM1NTUJ\nQy5yfi5cuBBNTU0aWKdi2pg92TSaSIOTVX1qa2vR3NwMwC5iklVX/vvvP9x22206XeWPP/5w9fZl\nQ3+2bds2AMCqVav095ZlobGxUaecjPUaMv/z8OHDI47fykXDedzc2DORtrJnzx7cdddd+rOh7UGm\n8tx00004deqU/kyyA5s2bRqxF5JpbWU0zm0c5SZEAkJeXh7uu+8+AHaQfeSRRwDYS0sC9pra4/2s\nbmsvc+fORVdXl9Z8SNtxmjVrFubOnYvdu3frzk233norfvzxRwB2RkBu4Nra2rBr1y6dVpbubTTT\ngT1Zl5KTIDc3Fx9//DEAuxcmxU1NTU0JFcjprhieKOfFyuPx4KmnngIwWNzT29uLJ554QlO9qfjg\ngw8ADC+Qkp7spXIxkP1kf/rpJ50f7ayejcVisCwLv//+OwC7wEeqaI8cOYKrr74aALBhwwZs374d\nJ06cmOyPcNE4F0yQtiNt6++//9aMxqJFizS4iqnQH2loaABgH4uhpE3U1dUhEAjA7/fjnnvuAQDM\nmzdPe8DZ2dmYNm0aAKC5uXnU4aypwN1XXiIiogzGnqxLyV11S0sL1qxZA8CeC7p27VoA9vzB3Nxc\nHa+NRqMJPRc3y8/PTxifBexF7ke6+07m8ssv18IWp9OnT6OsrOzivNGLaCJLAMpxaWhowPPPPw/A\nToU7p2nE4/GEnrusgGVZFt577z0AwKFDh5CXl4cNGza4boxN3qcU7wDDj2U0GsXPP/+s/5be2Lvv\nvjvu13Nr5uj2228HMLiSlmVZ2uN/7bXXANhDVH6/H7FYTM+/7du3awHZr7/+qqtp/fPPPwnPPxWy\nAUMxyLqUXCBvvPFGnVTv8/mwePFiAHaaJhaLYd++fQDslKGMz77wwguuDLRyody6dauerHITEQ6H\nU34en8+Hs2fP4vHHHwcAfPPNN7o4x8mTJy+5C4F810ePHtUK4eLiYr0Ajnbs4vG4jqcdO3YMN998\nM6666iqEQiEA7kmpS1rTSdqT3Gx1dXVpcKiqqtLf33nnneN6LcuyMnpucTLZ2dkaUKVNBINBvP76\n6wCAlStXArCvPbFYDOXl5VpEVlBQoDdzsVgsYWlPJzdedybKnbdbRERELsCerEvNnDkTgL3/qbMC\nUNJUJSUlsCwLpaWl+ju5u9y8ebNO7XGLrKwsTYUvXbpUe5tSyOPz+RCJRFLqhUajUQQCAS0ea29v\n1/mRf/31l95tZ9Lm0xPpXTs/T3d3N4CR5wCn8h6mT5+O1atX46233gIwPB2YiYLBILZu3Trs5yP1\nqqRa3Wn9+vXjfk039tiee+45WJaFeDyuPX+fz4eqqioAgz3+WCyGUCikG44AyXevIgbZjDZ0vMvr\n9Wqa+MiRIwDsFI807ra2NrzzzjsAgGeffRalpaU6DcXv9+tj3Tjvs7i4WNfa7evrw5dffglgMMiO\nNyDG43E88MADAOyU+4oVKwDYG7kPnYsMuGfscTTxeDxhrE3I8oHJyNzaoqIiRCIRdHV16eMz6UYk\nmb6+Ptxxxx0AoBXUyUj6OxKJ6DSmX375BYB9o5fqQgpuOC6iuLgYgF01LMMDc+bMAQCUl5dr6ls+\neygUGrZB+9AAO/T/UuE+FSuNmS4mIiIyhD3ZNBqt2jcrKwv3338/AGD16tUA7LtKeYz0Svv7+3VV\no7Vr12oPY9euXQgGg1ohuWXLFixfvhyAvUNPfX09gMFdajKZZVk4c+YMXnnlFQDAjh07dG9T6dFm\nZWWNaxeQiooKPR75+fnaS66vrx+xStnNqTB575ZlaVsrKirSJRZ7enqSpjcty8KSJUsA2EtODgwM\nwLIsbTduWSJPVmoKBAL6Xcv8cumpXnfddZoZCYVCOh9YUuxjtQE3ZjuuvPJKtLa2ArC/y0gkgmXL\nlulm7F6vVwsrn3nmGQD2hhqyalqqZFjBjcdoohhk00jSMP39/XoCS2quoKAAixYtAjBY3ejz+dDU\n1AQA2L17NwCgsbERJ0+e1OeU55G0j1QJVlRUJFRLuimdFY/H0dvbq9u4hUIh3UFFxqPz8vLGTAU6\nn2/mzJm44YYbANjHVdLqzjHZS4XcmM2YMUOrZy+77DJ8/vnnAOwdm/bt2zdipfC0adN0LD8WiyEQ\nCGD//v2u3dZsYGBAt8eTLfycW9q9+uqrAOylNmWcPpUbLOd6z7FYLONvyuQzf/3113ojfv78eYTD\nYbS1tSWsXS1VxXK9am5u1s3aU+W8Lk01DLJpJGvtHjt2TC/sElBzc3N13pkUqRQWFmqgaWlpAQAc\nP3581IYrJ0thYaH+LDc3V+cLtrS0ZFzDl+KL0UgxinyO2tpabNy4MaXnz8rKwvz58xNWepIeWaYd\ni4lyXvyLior0pqSvrw8HDhwAAFx//fV48MEH8eabbwKwF3uXi3BxcbG2Q2mjXV1dk/oZTHN+57Lv\ncGFhod7wpvocbmo78l2+9NJLugHC3r17sWrVKliWpTf4VVVVuhmJtI9z585dcI/UrfOHJ2LqfWIi\nIqJJwg0C0khSlJZl6b/Ly8sB2FNwJKUr64lmZ2drZfC6desAAJ999pmOmbS3tyeMv3m9Xnz66acA\ngIcffjjhtd9++20Ado9wpPRoOpuFTHYfzUMPPQTATnUC9mevrKxMaUrJjBkzcPToUZ2SEI/HEQwG\nAWDMNKjbFnwHBnsPCxcu1LV5c3JydEGJa665BpZl6RjtDz/8gL179wIAvv32W3R0dACwx/LPnDmD\n+vp6PQ7p3rrsYo7xBQKBhIXsnZ/xQmV6e3Gu6OTxeNDd3Y38/Hw9HyKRiP77YsxKcNYFTBVMF6eR\njIE5TwgJtt3d3XjyyScTfhaJRHD8+HEAgwudL168GB9++CEAe5WW/Px8/dvz58+PuHRgY2Ojplsz\nscGP9Z6CwaAu9SfjRGVlZfjuu+9QV1enx8bj8egNiHPcu6mpKeG4VFZWunaMcSw+n08vomvWrNEp\nPAB0moaQdvjvv/+ira0NANDa2qpBtLa2Fv39/YhGo5dkAUtJSUnCEMKePXvS+G4mRzweT1gis6Oj\nAwUFBbopRElJyUWd8jcV+3RMFxMRERnCnmwaSQ81Go1q702qhwGgs7MTwGCPY+XKlZq6kaKVWCym\n6Szneqk+nw8dHR1aBTh79mxNP1dXV2dkDzZVTz/9tBZyOVN5CxYswP79+xPS7NKLa29v16IWKZaS\ndY+dxzwZN/bcfD4fGhoaUF1dDWD4ln4yVemTTz5Bd3e3bv03MDCgvRdnz2OkqU3jKQ7KVLL4yKZN\nm/TzhsNhPPbYY+l8W5PuiiuuwOzZs1FZWWlsO0M3nkcTxSCbRhIUz507p0HPmZqRuX0VFRUA7IpQ\nSffJ3/f29upY2pIlS3QlFknrSRCaNWsWNm/eDMDesUbmmbrR+++/r3tbypQLj8ej6feamhoA9jQn\n+fzxeFyP98aNG+Hz+bBz586UX9NNaS65eVu+fDmqq6tH3JA+Eolg3rx5AOx2NnQKyljkeLhlg4DR\n3HvvvQCAEydO6HS4srIy1y09OlHz58/HihUrjO4X7Kbz6GJhupiIiMgQVhen0WSmTjwej0467+vr\nG7OYIZ3NYjzHZSL7rF6IdB0XZ498LJIiLygowJw5czQtfvjwYSM9T7e0lWRM7LM8nqyACZmclp1q\nIYc9WSIiIkPYk02jybzbdI7LjdWLTfdcNt6FDye7mITD4TG/G+nJBgIBhMNh4+/Z7T1Zk7tTZfo8\n2XSYaiGHhU+XuAtJW8lFOl08Hk/GrXWa7ouWs4Jc5jUmWwRCvmtZWIFGZyK4pnv5wFSWJnX+LWCf\nax6Px9UzDzIR08VERESGMF1MRERkCHuyREREhjDIEhERGcIgS0REZAiDLBERkSEMskRERIYwyBIR\nERnCIEtERGQIgywREZEhDLJERESGMMgSEREZwiBLRERkCIMsERGRIQyyREREhjDIEhERGcIgS0RE\nZAiDLBERkSEMskRERIYwyBIRERnCIEtERGQIgywREZEhDLJERESGMMgSEREZwiBLRERkCIMsERGR\nIQyyREREhjDIEhERGcIgS0REZAiDLBERkSEMskRERIYwyBIRERnCIEtERGQIgywREZEhDLJERESG\nMMgSEREZwiBLRERkCIMsERGRIQyyREREhjDIEhERGcIgS0REZAiDLBERkSEMskRERIYwyBIRERnC\nIEtERGQIgywREZEhDLJERESGMMgSEREZwiBLRERkCIMsERGRIQyyREREhjDIEhERGcIgS0REZAiD\nLBERkSEMskRERIYwyBIRERnCIEtERGQIgywREZEhDLJERESGMMgSEREZwiBLRERkCIMsERGRIf8D\nhFXAuepEPPIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5aefce3278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "lqvH6OdfjSle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlAEyCfWjSoQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GwVzFQaXjSq-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DdmF3d0jStO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0EO2_6ziKwg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}